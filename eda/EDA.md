# Разведочный анализ данных

Ноутбук для EDA лежит на [Kaggle](https://www.kaggle.com/taiypeo/spectral-clustering) и в Google Drive.

## Loading and cleaning the graph
В данной части я загрузил граф из файла graph.json с помощью библиотеки NetworkX и убрал из него те вершины, которых нет в файле papers.json.
Также я загрузил файл unique_original_ids.json, содержащий список всех вершин статей 2023 года из выборки ArXiv.

Граф был загружен как неориентированный, чтобы потом можно было применить к нему спектральную кластеризацию.

## Minimal graph analysis
Полученный граф содержит 411988 вершины и 271 компоненту связности. Причем, есть одна большая компонента связности размера 411706 и остальные, которые
в сумме содержат 282 вершины. Исходя из этого, для простоты дальнейшей работы я отбросил маленькие компоненты связности как шум
и сфокусировался на одной большой компоненте.

Посчитав плотность для большой компоненты связности (см. [доки NetworkX](https://networkx.org/documentation/stable/reference/generated/networkx.classes.function.density.html)),
можно удостовериться, что компонента очень слабо связана: ее плотность 0.00011196709822370937 сильно меньше 1 (плотность полносвязного графа).

## Clustering and plotting
К данной компоненте связности я попробовал применить спектральную кластеризацию. Я посчитал Лапласиан компоненты связности и взял из него 30
собственных векторов, соответствующих 30 наименьшим собственным значениям. Над матрицей размера `n_nodes x 30` я применил k-means кластеризацию
с 8 кластерами. Кластеры получились сильно неравномерными по размеру:
```
[654, 1, 1901, 3614, 1213, 3498, 1119, 23220]
```

Видно, что большая часть вершин компоненты связности попала в последний кластер.

Для визуализации я применил метод [UMAP](https://umap-learn.readthedocs.io/en/latest/). Матрицу размера `n_nodes x 30` я преобразовал в матрицу
размера `n_nodes x 2` и нарисовал с помощью matplotlib.

На визуализации сложно выделить конкретные кластеры (кроме нескольки достаточно четких маленьких и одного большого), причем после k-means
кластеры довольно сильно могут пересекаться.

## Cluster content analysis
Вначале я попробовал взять по несколько статей из каждого кластера и вручную посмотреть на их тематики. Однако, к сожалению, ни в одном кластере я не смог выделить
одну очевидную тему.

Далее я склеил поля title и abstract для статей и с помощью библиотеки nltk убрал стоп-слова и токены, содержащие символы, не являющиеся буквами, а также применил
лемматизацию WordNet. Над полученными текстами с помощью библиотеки [wordcloud](https://pypi.org/project/wordcloud/) я построил облака слов с целью выделить ключевые слова
каждого полученного кластера. Но и здесь ключевые слова довольно сильно пересекаются, поэтому сложно выделить конкретные тематики отдельных кластеров.

## Дополнительные эксперименты
Я также пробовал применить методы DBSCAN и HDBSCAN для кластеризации, а также t-SNE для визуализации, но они либо показывали очень плохие результаты, либо работали слишком долго
(может быть часы, а может быть и дни), либо у меня не хватало RAM для вычислений.

## Выводы
Информации о связи вершин в графе недостаточно для адекватной оценки между статьями -- полученные кластеры сильно пересекаются, и выделить отдельные тематики между кластерами
не представляется возможным. Далее, стоит попробовать добавить текстовую информацию о статьях в метод кластеризации.
