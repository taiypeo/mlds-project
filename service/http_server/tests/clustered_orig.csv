paperId,title,abstract,year,raw_text,text,cluster
c3aaeab99fa85bcd6b9d8ee370c35af52eecec48,"High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient","High dimensional structured data enriched model describes groups of observations by shared and per-group individual parameters, each with its own structure such as sparsity or group sparsity. In this paper, we consider the general form of data enrichment where data comes in a fixed but arbitrary number of groups G. Any convex function, e.g., norms, can characterize the structure of both shared and individual parameters. We propose an estimator for high dimensional data enriched model and provide conditions under which it consistently estimates both shared and individual parameters. We also delineate sample complexity of the estimator and present high probability non-asymptotic bound on estimation error of all parameters. Interestingly the sample complexity of our estimator translates to conditions on both per-group sample sizes and the total number of samples. We propose an iterative estimation algorithm with linear convergence rate and supplement our theoretical analysis with synthetic and real experimental results. Particularly, we show the predictive power of data-enriched model along with its interpretable results in anticancer drug sensitivity analysis.",2018,"High Dimensional Data Enrichment: Interpretable, Fast, and Data-EfficientHigh dimensional structured data enriched model describes groups of observations by shared and per-group individual parameters, each with its own structure such as sparsity or group sparsity. In this paper, we consider the general form of data enrichment where data comes in a fixed but arbitrary number of groups G. Any convex function, e.g., norms, can characterize the structure of both shared and individual parameters. We propose an estimator for high dimensional data enriched model and provide conditions under which it consistently estimates both shared and individual parameters. We also delineate sample complexity of the estimator and present high probability non-asymptotic bound on estimation error of all parameters. Interestingly the sample complexity of our estimator translates to conditions on both per-group sample sizes and the total number of samples. We propose an iterative estimation algorithm with linear convergence rate and supplement our theoretical analysis with synthetic and real experimental results. Particularly, we show the predictive power of data-enriched model along with its interpretable results in anticancer drug sensitivity analysis.",high dimensional data enrichment interpretable fast dimensional structured data enriched model describes group observation shared individual parameter structure sparsity group sparsity paper consider general form data enrichment data come fixed arbitrary number group convex function norm characterize structure shared individual parameter propose estimator high dimensional data enriched model provide condition consistently estimate shared individual parameter also delineate sample complexity estimator present high probability bound estimation error parameter interestingly sample complexity estimator translates condition sample size total number sample propose iterative estimation algorithm linear convergence rate supplement theoretical analysis synthetic real experimental result particularly show predictive power model along interpretable result anticancer drug sensitivity analysis,8
a3534e9deb51e6fbaebddb34dd344fb60d86645c,Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation,"Transformer-based detection and segmentation methods use a list of learned detection queries to retrieve information from the transformer network and learn to predict the location and category of one specific object from each query. We empirically find that random convex combinations of the learned queries are still good for the corresponding models. We then propose to learn a convex combination with dynamic coefficients based on the high-level semantics of the image. The generated dynamic queries, named modulated queries, better capture the prior of object locations and categories in the different images. Equipped with our modulated queries, a wide range of DETR-based models achieve consistent and superior performance across multiple tasks including object detection, instance segmentation, panoptic segmentation, and video instance segmentation.",2023,"Learning Dynamic Query Combinations for Transformer-based Object Detection and SegmentationTransformer-based detection and segmentation methods use a list of learned detection queries to retrieve information from the transformer network and learn to predict the location and category of one specific object from each query. We empirically find that random convex combinations of the learned queries are still good for the corresponding models. We then propose to learn a convex combination with dynamic coefficients based on the high-level semantics of the image. The generated dynamic queries, named modulated queries, better capture the prior of object locations and categories in the different images. Equipped with our modulated queries, a wide range of DETR-based models achieve consistent and superior performance across multiple tasks including object detection, instance segmentation, panoptic segmentation, and video instance segmentation.",learning dynamic query combination object detection detection segmentation method use list learned detection query retrieve information transformer network learn predict location category one specific object query empirically find random convex combination learned query still good corresponding model propose learn convex combination dynamic coefficient based semantics image generated dynamic query named modulated query better capture prior object location category different image equipped modulated query wide range model achieve consistent superior performance across multiple task including object detection instance segmentation panoptic segmentation video instance segmentation,1
6684f3ec0f2dec73defbe580489d8a8224074315,Mask-Free Video Instance Segmentation,"The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. In this work, we aim to remove the mask-annotation requirement. We propose MaskFreeVIS, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. We leverage the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. Our TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. Our mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. We validate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of our method by drastically narrowing the gap between fully and weakly-supervised VIS performance. Our code and trained models are available at http://vis.xyz/pub/maskfreevis.",2023,"Mask-Free Video Instance SegmentationThe recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. In this work, we aim to remove the mask-annotation requirement. We propose MaskFreeVIS, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. We leverage the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. Our TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. Our mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. We validate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of our method by drastically narrowing the gap between fully and weakly-supervised VIS performance. Our code and trained models are available at http://vis.xyz/pub/maskfreevis.",video instance segmentationthe recent advancement video instance segmentation vi ha largely driven use deeper increasingly model however video mask tedious expensive annotate limiting scale diversity existing vi datasets work aim remove requirement propose maskfreevis achieving highly competitive vi performance using bounding box annotation object state leverage rich temporal mask consistency constraint video introducing temporal loss providing strong mask supervision without label find match across frame efficient step followed neighbor selection consistency loss enforced found match objective simple implement ha trainable parameter computationally efficient yet outperforms baseline employing optical flow enforce temporal mask consistency validate maskfreevis ovis mot benchmark result clearly demonstrate efficacy method drastically narrowing gap fully vi performance code trained model available http,6
0fb293d7fbcd413b3c29df36614c12f5f0777ffa,Cloud-RAIN: Point Cloud Analysis with Reflectional Invariance,"The networks for point cloud tasks are expected to be invariant when the point clouds are affinely transformed such as rotation and reflection. So far, relative to the rotational invariance that has been attracting major research attention in the past years, the reflection invariance is little addressed. Notwithstanding, reflection symmetry can find itself in very common and important scenarios, e.g., static reflection symmetry of structured streets, dynamic reflection symmetry from bidirectional motion of moving objects (such as pedestrians), and left- and right-hand traffic practices in different countries. To the best of our knowledge, unfortunately, no reflection-invariant network has been reported in point cloud analysis till now. To fill this gap, we propose a framework by using quadratic neurons and PCA canonical representation, referred to as Cloud-RAIN, to endow point \underline{Cloud} models with \underline{R}eflection\underline{A}l \underline{IN}variance. We prove a theorem to explain why Cloud-RAIN can enjoy reflection symmetry. Furthermore, extensive experiments also corroborate the reflection property of the proposed Cloud-RAIN and show that Cloud-RAIN is superior to data augmentation. Our code is available at https://github.com/YimingCuiCuiCui/Cloud-RAIN.",2023,"Cloud-RAIN: Point Cloud Analysis with Reflectional InvarianceThe networks for point cloud tasks are expected to be invariant when the point clouds are affinely transformed such as rotation and reflection. So far, relative to the rotational invariance that has been attracting major research attention in the past years, the reflection invariance is little addressed. Notwithstanding, reflection symmetry can find itself in very common and important scenarios, e.g., static reflection symmetry of structured streets, dynamic reflection symmetry from bidirectional motion of moving objects (such as pedestrians), and left- and right-hand traffic practices in different countries. To the best of our knowledge, unfortunately, no reflection-invariant network has been reported in point cloud analysis till now. To fill this gap, we propose a framework by using quadratic neurons and PCA canonical representation, referred to as Cloud-RAIN, to endow point \underline{Cloud} models with \underline{R}eflection\underline{A}l \underline{IN}variance. We prove a theorem to explain why Cloud-RAIN can enjoy reflection symmetry. Furthermore, extensive experiments also corroborate the reflection property of the proposed Cloud-RAIN and show that Cloud-RAIN is superior to data augmentation. Our code is available at https://github.com/YimingCuiCuiCui/Cloud-RAIN.",point cloud analysis reflectional invariancethe network point cloud task expected invariant point cloud affinely transformed rotation reflection far relative rotational invariance ha attracting major research attention past year reflection invariance little addressed notwithstanding reflection symmetry find common important scenario static reflection symmetry structured street dynamic reflection symmetry bidirectional motion moving object pedestrian traffic practice different country best knowledge unfortunately network ha reported point cloud analysis till fill gap propose framework using quadratic neuron pca canonical representation referred endow point cloud model r l variance prove theorem explain enjoy reflection symmetry furthermore extensive experiment also corroborate reflection property proposed show superior data augmentation code available http,1
0675882b0f861e7d0c26be7fddbf8057374042eb,DR2: Diffusion-Based Robust Degradation Remover for Blind Face Restoration,"Blind face restoration usually synthesizes degraded low-quality data with a pre-defined degradation model for training, while more complex cases could happen in the real world. This gap between the assumed and actual degradation hurts the restoration performance where artifacts are often observed in the output. However, it is expensive and infeasible to include every type of degradation to cover real-world cases in the training data. To tackle this robustness issue, we propose Diffusion-based Robust Degradation Remover (DR2) to first transform the degraded image to a coarse but degradation-invariant prediction, then employ an enhancement module to restore the coarse prediction to a high-quality image. By leveraging a well-performing denoising diffusion probabilistic model, our DR2 diffuses input images to a noisy status where various types of degradation give way to Gaussian noise, and then captures semantic information through iterative denoising steps. As a result, DR2 is robust against common degradation (e.g. blur, resize, noise and compression) and compatible with different designs of enhancement modules. Experiments in various settings show that our framework outperforms state-of-the-art methods on heavily degraded synthetic and real-world datasets.",2023,"DR2: Diffusion-Based Robust Degradation Remover for Blind Face RestorationBlind face restoration usually synthesizes degraded low-quality data with a pre-defined degradation model for training, while more complex cases could happen in the real world. This gap between the assumed and actual degradation hurts the restoration performance where artifacts are often observed in the output. However, it is expensive and infeasible to include every type of degradation to cover real-world cases in the training data. To tackle this robustness issue, we propose Diffusion-based Robust Degradation Remover (DR2) to first transform the degraded image to a coarse but degradation-invariant prediction, then employ an enhancement module to restore the coarse prediction to a high-quality image. By leveraging a well-performing denoising diffusion probabilistic model, our DR2 diffuses input images to a noisy status where various types of degradation give way to Gaussian noise, and then captures semantic information through iterative denoising steps. As a result, DR2 is robust against common degradation (e.g. blur, resize, noise and compression) and compatible with different designs of enhancement modules. Experiments in various settings show that our framework outperforms state-of-the-art methods on heavily degraded synthetic and real-world datasets.",robust degradation remover blind face restorationblind face restoration usually synthesizes degraded data degradation model training complex case could happen real world gap assumed actual degradation hurt restoration performance artifact often observed output however expensive infeasible include every type degradation cover case training data tackle robustness issue propose robust degradation remover first transform degraded image coarse prediction employ enhancement module restore coarse prediction image leveraging denoising diffusion probabilistic model diffuses input image noisy status various type degradation give way gaussian noise capture semantic information iterative denoising step result robust common degradation blur resize noise compression compatible different design enhancement module experiment various setting show framework outperforms method heavily degraded synthetic datasets,3
da82c45a90b269e6ee4d1406ab5b23a8edb49104,Attention-based Graph Convolution Fusing Latent Structures and Multiple Features for Graph Neural Networks,"We present an attention-based spatial graph convolution (AGC) for graph neural networks (GNNs). Existing AGCs focus on only using node-wise features and utilizing one type of attention function when calculating attention weights. Instead, we propose two methods to improve the representational power of AGCs by utilizing 1) structural information in a high-dimensional space and 2) multiple attention functions when calculating their weights. The first method computes a local structure representation of a graph in a high-dimensional space. The second method utilizes multiple attention functions simultaneously in one AGC. Both approaches can be combined. We also propose a GNN for the classification of point clouds and that for the prediction of point labels in a point cloud based on the proposed AGC. According to experiments, the proposed GNNs perform better than existing methods. Our codes open at https://github.com/liyang-tuat/SFAGC.",2023,"Attention-based Graph Convolution Fusing Latent Structures and Multiple Features for Graph Neural NetworksWe present an attention-based spatial graph convolution (AGC) for graph neural networks (GNNs). Existing AGCs focus on only using node-wise features and utilizing one type of attention function when calculating attention weights. Instead, we propose two methods to improve the representational power of AGCs by utilizing 1) structural information in a high-dimensional space and 2) multiple attention functions when calculating their weights. The first method computes a local structure representation of a graph in a high-dimensional space. The second method utilizes multiple attention functions simultaneously in one AGC. Both approaches can be combined. We also propose a GNN for the classification of point clouds and that for the prediction of point labels in a point cloud based on the proposed AGC. According to experiments, the proposed GNNs perform better than existing methods. Our codes open at https://github.com/liyang-tuat/SFAGC.",graph convolution fusing latent structure multiple feature graph neural networkswe present spatial graph convolution agc graph neural network gnns existing agcs focus using feature utilizing one type attention function calculating attention weight instead propose two method improve representational power agcs utilizing structural information space multiple attention function calculating weight first method computes local structure representation graph space second method utilizes multiple attention function simultaneously one agc approach combined also propose gnn classification point cloud prediction point label point cloud based proposed agc according experiment proposed gnns perform better existing method code open http,5
55ae0c1060d7248615b85d53c95f987966643cba,Correcting Semantic Parses with Natural Language through Dynamic Schema Encoding,"In addressing the task of converting natural language to SQL queries, there are several semantic and syntactic challenges. It becomes increasingly important to understand and remedy the points of failure as the performance of semantic parsing systems improve. We explore semantic parse correction with natural language feedback, proposing a new solution built on the success of autoregressive decoders in text-to-SQL tasks. By separating the semantic and syntactic difficulties of the task, we show that the accuracy of text-to-SQL parsers can be boosted by up to 26% with only one turn of correction with natural language. Additionally, we show that a T5-base model is capable of correcting the errors of a T5-large model in a zero-shot, cross-parser setting.",2023,"Correcting Semantic Parses with Natural Language through Dynamic Schema EncodingIn addressing the task of converting natural language to SQL queries, there are several semantic and syntactic challenges. It becomes increasingly important to understand and remedy the points of failure as the performance of semantic parsing systems improve. We explore semantic parse correction with natural language feedback, proposing a new solution built on the success of autoregressive decoders in text-to-SQL tasks. By separating the semantic and syntactic difficulties of the task, we show that the accuracy of text-to-SQL parsers can be boosted by up to 26% with only one turn of correction with natural language. Additionally, we show that a T5-base model is capable of correcting the errors of a T5-large model in a zero-shot, cross-parser setting.",correcting semantic par natural language dynamic schema encodingin addressing task converting natural language sql query several semantic syntactic challenge becomes increasingly important understand remedy point failure performance semantic parsing system improve explore semantic parse correction natural language feedback proposing new solution built success autoregressive decoder task separating semantic syntactic difficulty task show accuracy parser boosted one turn correction natural language additionally show model capable correcting error model setting,7
a8098f710f75b505d4c7c15e9d58c340da05ea6f,Scene Separation & Data Selection: Temporal Segmentation Algorithm for Real-time Video Stream Analysis,"We present 2SDS (Scene Separation and Data Selection algorithm), a temporal segmentation algorithm used in real-time video stream interpretation. It complements CNN-based models to make use of temporal information in videos. 2SDS can detect the change between scenes in a video stream by com-paring the image difference between two frames. It separates a video into segments (scenes), and by combining itself with a CNN model, 2SDS can select the optimal result for each scene. In this paper, we will be discussing some basic methods and concepts behind 2SDS, as well as presenting some preliminary experiment results regarding 2SDS. During these experiments, 2SDS has achieved an overall accuracy of over 90%.",2023,"Scene Separation & Data Selection: Temporal Segmentation Algorithm for Real-time Video Stream AnalysisWe present 2SDS (Scene Separation and Data Selection algorithm), a temporal segmentation algorithm used in real-time video stream interpretation. It complements CNN-based models to make use of temporal information in videos. 2SDS can detect the change between scenes in a video stream by com-paring the image difference between two frames. It separates a video into segments (scenes), and by combining itself with a CNN model, 2SDS can select the optimal result for each scene. In this paper, we will be discussing some basic methods and concepts behind 2SDS, as well as presenting some preliminary experiment results regarding 2SDS. During these experiments, 2SDS has achieved an overall accuracy of over 90%.",scene separation data selection temporal segmentation algorithm video stream analysiswe present scene separation data selection algorithm temporal segmentation algorithm used video stream interpretation complement model make use temporal information video detect change scene video stream image difference two frame separate video segment scene combining cnn model select optimal result scene paper discussing basic method concept behind well presenting preliminary experiment result regarding experiment ha achieved overall accuracy,1
68e7e7c52393fbd7d4d6fb34174df917ef3e8b1e,Modeling Complex Event Scenarios via Simple Entity-focused Questions,"Event scenarios are often complex and involve multiple event sequences connected through different entity participants. Exploring such complex scenarios requires an ability to branch through different sequences, something that is difficult to achieve with standard event language modeling. To address this, we propose a question-guided generation framework that models events in complex scenarios as answers to questions about participants. At any step in the generation process, the framework uses the previously-generated events as context, but generates the next event as an answer to one of three questions: what else a participant did, what else happened to a participant, or what else happened. The participants and the questions themselves can be sampled or be provided as input from a user, allowing for controllable exploration. Our empirical evaluation shows that this question-guided generation provides better coverage of participants, diverse events within a domain, comparable perplexities for modeling event sequences, and more effective control for interactive schema generation.",2023,"Modeling Complex Event Scenarios via Simple Entity-focused QuestionsEvent scenarios are often complex and involve multiple event sequences connected through different entity participants. Exploring such complex scenarios requires an ability to branch through different sequences, something that is difficult to achieve with standard event language modeling. To address this, we propose a question-guided generation framework that models events in complex scenarios as answers to questions about participants. At any step in the generation process, the framework uses the previously-generated events as context, but generates the next event as an answer to one of three questions: what else a participant did, what else happened to a participant, or what else happened. The participants and the questions themselves can be sampled or be provided as input from a user, allowing for controllable exploration. Our empirical evaluation shows that this question-guided generation provides better coverage of participants, diverse events within a domain, comparable perplexities for modeling event sequences, and more effective control for interactive schema generation.",modeling complex event scenario via simple questionsevent scenario often complex involve multiple event sequence connected different entity participant exploring complex scenario requires ability branch different sequence something difficult achieve standard event language modeling address propose generation framework model event complex scenario answer question participant step generation process framework us event context generates next event answer one three question else participant else happened participant else happened participant question sampled provided input user allowing controllable exploration empirical evaluation show generation provides better coverage participant diverse event within domain comparable perplexity modeling event sequence effective control interactive schema generation,7
c4a4d5248c34eff2c09d9ad1752c2780bd22504d,BoxSnake: Polygonal Instance Segmentation with Box Supervision,"Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset. The code has been available publicly.",2023,"BoxSnake: Polygonal Instance Segmentation with Box SupervisionBox-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset. The code has been available publicly.",boxsnake polygonal instance segmentation box instance segmentation ha gained much attention requires simple box annotation instead costly mask polygon annotation however existing instance segmentation model mainly focus framework propose new training technique termed boxsnake achieve effective polygonal instance segmentation using box annotation first time method consists two loss function unary loss constrains bounding box predicted polygon achieve segmentation pairwise loss encourages predicted polygon fit object boundary compared method boxsnake reduces performance gap predicted segmentation bounding box show significant superiority cityscape dataset code ha available publicly,1
064db5889f7cd12d8322490ed52f78dceef0c3f5,ICICLE: Interpretable Class Incremental Continual Learning,"Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free methods of common class-incremental learning when applied to concept-based models.",2023,"ICICLE: Interpretable Class Incremental Continual LearningContinual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free methods of common class-incremental learning when applied to concept-based models.",icicle interpretable class incremental continual learningcontinual learning enables incremental learning new task without forgetting previously learned resulting positive knowledge transfer enhance performance new old task however continual learning pose new challenge interpretability rationale behind model prediction may change time leading interpretability concept drift address problem proposing interpretable learning icicle approach adopts prototypical approach consists three crucial novelty interpretability regularization distills previously learned concept preserving positive reasoning prototype initialization strategy dedicated setting bias compensation devoted prototypical part experimental result demonstrate icicle reduces interpretability concept drift outperforms existing method common learning applied model,5
4309d572a37d655779f9dce6a2c98c66334132de,SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension,"Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.",2023,"SEED-Bench: Benchmarking Multimodal LLMs with Generative ComprehensionBased on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.",benchmarking multimodal llm generative comprehensionbased powerful large language model llm recent generative multimodal large language model mllms gained prominence pivotal research area exhibiting remarkable capability comprehension generation work address evaluation generative comprehension mllms preliminary step towards comprehensive assessment generative model introducing benchmark named consists multiple choice question accurate human annotation x larger existing benchmark span evaluation dimension including comprehension image video modality develop advanced pipeline generating question target specific evaluation dimension integrating automatic filtering manual verification process question groundtruth option derived human annotation enables objective efficient assessment model performance eliminating need human gpt intervention evaluation evaluate performance model across dimension covering spatial temporal understanding revealing limitation existing mllms evaluation result aim provide insight motivating future research launch consistently maintain leaderboard provide platform community ass investigate model capability,7
ddccb43cf3753c8ec2e84a8a5152b50df041130a,Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography,"The diagnostic quality of computed tomography (CT) scans is usually restricted by the induced patient dose, scan speed, and image quality. Sparse-angle tomographic scans reduce radiation exposure and accelerate data acquisition, but suffer from image artifacts and noise. Existing image processing algorithms can restore CT reconstruction quality but often require large training data sets or can not be used for truncated objects. This work presents a self-supervised projection inpainting method that allows optimizing missing projective views via gradient-based optimization. By reconstructing independent stacks of projection data, a self-supervised loss is calculated in the CT image domain and used to directly optimize projection image intensities to match the missing tomographic views constrained by the projection geometry. Our experiments on real X-ray microscope (XRM) tomographic mouse tibia bone scans show that our method improves reconstructions by 3.1-7.4%/7.7-17.6% in terms of PSNR/SSIM with respect to the interpolation baseline. Our approach is applicable as a flexible self-supervised projection inpainting tool for tomographic applications.",2023,"Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View TomographyThe diagnostic quality of computed tomography (CT) scans is usually restricted by the induced patient dose, scan speed, and image quality. Sparse-angle tomographic scans reduce radiation exposure and accelerate data acquisition, but suffer from image artifacts and noise. Existing image processing algorithms can restore CT reconstruction quality but often require large training data sets or can not be used for truncated objects. This work presents a self-supervised projection inpainting method that allows optimizing missing projective views via gradient-based optimization. By reconstructing independent stacks of projection data, a self-supervised loss is calculated in the CT image domain and used to directly optimize projection image intensities to match the missing tomographic views constrained by the projection geometry. Our experiments on real X-ray microscope (XRM) tomographic mouse tibia bone scans show that our method improves reconstructions by 3.1-7.4%/7.7-17.6% in terms of PSNR/SSIM with respect to the interpolation baseline. Our approach is applicable as a flexible self-supervised projection inpainting tool for tomographic applications.",geometric constraint enable sinogram inpainting tomographythe diagnostic quality computed tomography ct scan usually restricted induced patient dose scan speed image quality tomographic scan reduce radiation exposure accelerate data acquisition suffer image artifact noise existing image processing algorithm restore ct reconstruction quality often require large training data set used truncated object work present projection inpainting method allows optimizing missing projective view via optimization reconstructing independent stack projection data loss calculated ct image domain used directly optimize projection image intensity match missing tomographic view constrained projection geometry experiment real microscope xrm tomographic mouse tibia bone scan show method improves reconstruction term respect interpolation baseline approach applicable flexible projection inpainting tool tomographic application,3
00b28f2037475c0cf32ec01f17cb7b98f6254b64,Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer,"Event camera, as an emerging biologically-inspired vision sensor for capturing motion dynamics, presents new potential for 3D human pose tracking, or video-based 3D human pose estimation. However, existing works in pose tracking either require the presence of additional gray-scale images to establish a solid starting pose, or ignore the temporal dependencies all together by collapsing segments of event streams to form static event frames. Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs, a.k.a. dense deep learning) has been showcased in many event-based tasks, the use of ANNs tends to neglect the fact that compared to the dense frame-based image sequences, the occurrence of events from an event camera is spatiotemporally much sparser. Motivated by the above mentioned issues, we present in this paper a dedicated end-to-end sparse deep learning approach for event-based pose tracking: 1) to our knowledge this is the first time that 3D human pose tracking is obtained from events only, thus eliminating the need of accessing to any frame-based images as part of input; 2) our approach is based entirely upon the framework of Spiking Neural Networks (SNNs), which consists of Spike-Element-Wise (SEW) ResNet and a novel Spiking Spatiotemporal Transformer; 3) a large-scale synthetic dataset is constructed that features a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data, named SynEventHPD. Empirical experiments demonstrate that, with superior performance over the state-of-the-art (SOTA) ANNs counterparts, our approach also achieves a significant computation reduction of 80% in FLOPS. Furthermore, our proposed method also outperforms SOTA SNNs in the regression task of human pose tracking. Our implementation is available at https://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be released upon paper acceptance.",2023,"Event-based Human Pose Tracking by Spiking Spatiotemporal TransformerEvent camera, as an emerging biologically-inspired vision sensor for capturing motion dynamics, presents new potential for 3D human pose tracking, or video-based 3D human pose estimation. However, existing works in pose tracking either require the presence of additional gray-scale images to establish a solid starting pose, or ignore the temporal dependencies all together by collapsing segments of event streams to form static event frames. Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs, a.k.a. dense deep learning) has been showcased in many event-based tasks, the use of ANNs tends to neglect the fact that compared to the dense frame-based image sequences, the occurrence of events from an event camera is spatiotemporally much sparser. Motivated by the above mentioned issues, we present in this paper a dedicated end-to-end sparse deep learning approach for event-based pose tracking: 1) to our knowledge this is the first time that 3D human pose tracking is obtained from events only, thus eliminating the need of accessing to any frame-based images as part of input; 2) our approach is based entirely upon the framework of Spiking Neural Networks (SNNs), which consists of Spike-Element-Wise (SEW) ResNet and a novel Spiking Spatiotemporal Transformer; 3) a large-scale synthetic dataset is constructed that features a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data, named SynEventHPD. Empirical experiments demonstrate that, with superior performance over the state-of-the-art (SOTA) ANNs counterparts, our approach also achieves a significant computation reduction of 80% in FLOPS. Furthermore, our proposed method also outperforms SOTA SNNs in the regression task of human pose tracking. Our implementation is available at https://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be released upon paper acceptance.",human pose tracking spiking spatiotemporal transformerevent camera emerging vision sensor capturing motion dynamic present new potential human pose tracking human pose estimation however existing work pose tracking either require presence additional image establish solid starting pose ignore temporal dependency together collapsing segment event stream form static event frame meanwhile although effectiveness artificial neural network anns dense deep learning ha showcased many task use anns tends neglect fact compared dense image sequence occurrence event event camera spatiotemporally much sparser motivated mentioned issue present paper dedicated sparse deep learning approach pose tracking knowledge first time human pose tracking obtained event thus eliminating need accessing image part input approach based entirely upon framework spiking neural network snns consists sew resnet novel spiking spatiotemporal transformer synthetic dataset constructed feature broad diverse set annotated human motion well longer hour event stream data named syneventhpd empirical experiment demonstrate superior performance sota anns counterpart approach also achieves significant computation reduction flop furthermore proposed method also outperforms sota snns regression task human pose tracking implementation available http dataset released upon paper acceptance,1
0696987e2140502f397e21b16e729bb2fa3f5c09,Robust Safe Reinforcement Learning under Adversarial Disturbances,"Safety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the maximal robust invariant set. Second, this paper integrates the proposed policy iteration scheme into a constrained reinforcement learning algorithm that simultaneously synthesizes the robust invariant set and uses it for constrained policy optimization. This algorithm tackles both optimality and safety, i.e., learning a policy that attains high rewards while maintaining safety under worst-case disturbances. Experiments on classic control tasks show that the proposed method achieves zero constraint violation with learned worst-case adversarial disturbances, while other baseline algorithms violate the safety constraints substantially. Our proposed method also attains comparable performance as the baselines even in the absence of the adversary.",2023,"Robust Safe Reinforcement Learning under Adversarial DisturbancesSafety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the maximal robust invariant set. Second, this paper integrates the proposed policy iteration scheme into a constrained reinforcement learning algorithm that simultaneously synthesizes the robust invariant set and uses it for constrained policy optimization. This algorithm tackles both optimality and safety, i.e., learning a policy that attains high rewards while maintaining safety under worst-case disturbances. Experiments on classic control tasks show that the proposed method achieves zero constraint violation with learned worst-case adversarial disturbances, while other baseline algorithms violate the safety constraints substantially. Our proposed method also attains comparable performance as the baselines even in the absence of the adversary.",robust safe reinforcement learning adversarial disturbancessafety primary concern applying reinforcement learning control task especially presence external disturbance however existing safe reinforcement learning algorithm rarely account external disturbance limiting applicability robustness practice address challenge paper proposes robust safe reinforcement learning framework tackle disturbance first paper present policy iteration scheme solve robust invariant set subset safe set persistent safety possible state within key idea establish game leveraging safety value function reachability analysis protagonist control input aim maintain safety adversary external disturbance try break safety paper prof proposed policy iteration algorithm converges monotonically maximal robust invariant set second paper integrates proposed policy iteration scheme constrained reinforcement learning algorithm simultaneously synthesizes robust invariant set us constrained policy optimization algorithm tackle optimality safety learning policy attains high reward maintaining safety disturbance experiment classic control task show proposed method achieves zero constraint violation learned adversarial disturbance baseline algorithm violate safety constraint substantially proposed method also attains comparable performance baseline even absence adversary,8
9ae6a6843b9a875e24fff423a3d339636511756a,ASIC: Aligning Sparse in-the-wild Image Collections,"We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compared to existing self-supervised methods. Code and other material will be made available at \url{https://kampta.github.io/asic}.",2023,"ASIC: Aligning Sparse in-the-wild Image CollectionsWe present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compared to existing self-supervised methods. Code and other material will be made available at \url{https://kampta.github.io/asic}.",asic aligning sparse image collectionswe present method joint alignment sparse image collection object category prior work assume either keypoint annotation large dataset image single object category however neither assumption hold true object present world present technique directly optimizes sparse collection image particular category obtain consistent dense correspondence across collection use pairwise nearest neighbor obtained deep feature vision transformer vit model noisy sparse keypoint match make dense accurate match optimizing neural network jointly map image collection learned canonical grid experiment cub benchmark demonstrate method produce globally consistent higher quality correspondence across image collection compared existing method code material made available http,1
64c2a323470f9ea719c07f010aea97759cd2ead4,Word sense extension,"Humans often make creative use of words to expressnovel senses. A long-standing effort in natural language processing hasbeen focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) thatenables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitivemodels of chaining with a learning scheme that transforms a language model embedding space to supportvarious types of word sense extension. We evaluate our frameworkagainst several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words. Furthermore, we show that our WSE framework improves performance over a range of transformer-based WSD models in predicting rare word senses with few or zero mentions in the training data.",2023,"Word sense extensionHumans often make creative use of words to expressnovel senses. A long-standing effort in natural language processing hasbeen focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) thatenables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitivemodels of chaining with a learning scheme that transforms a language model embedding space to supportvarious types of word sense extension. We evaluate our frameworkagainst several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words. Furthermore, we show that our WSE framework improves performance over a range of transformer-based WSD models in predicting rare word senses with few or zero mentions in the training data.",word sense extensionhumans often make creative use word expressnovel sens effort natural language processing hasbeen focusing word sense disambiguation wsd little ha explored sense inventory word may extended toward novel meaning present paradigm word sense extension wse thatenables word spawn new sens toward novel context develop framework simulates novel word sense extension first partitioning polysemous word type two mark different sens inferring whether meaning extended convey sense denoted token partitioned word type framework combine cognitivemodels chaining learning scheme transforms language model embedding space supportvarious type word sense extension evaluate frameworkagainst several competitive baseline show superior predicting plausible novel sens english word furthermore show wse framework improves performance range wsd model predicting rare word sens zero mention training data,7
6cc9a082e849b1ad63cc5c54dd6999d6584419c6,Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation,"Proxy causal learning (PCL) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal effect, subject to identifiability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be applied to off-policy evaluation for the confounded bandit problem, in which DFPV also exhibits competitive performance.",2021,"Deep Proxy Causal Learning and its Application to Confounded Bandit Policy EvaluationProxy causal learning (PCL) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal effect, subject to identifiability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be applied to off-policy evaluation for the confounded bandit problem, in which DFPV also exhibits competitive performance.",deep proxy causal learning application confounded bandit policy evaluationproxy causal learning pcl method estimating causal effect treatment outcome presence unobserved confounding using proxy structured side information confounder achieved via regression first stage model relation among treatment proxy second stage use model learn effect treatment outcome given context provided proxy pcl guarantee recovery true causal effect subject identifiability condition propose novel method pcl deep feature proxy variable method dfpv address case proxy treatment outcome nonlinear complex relationship represented deep neural network feature show dfpv outperforms recent pcl method challenging synthetic benchmark including setting involving high dimensional image data furthermore show pcl applied evaluation confounded bandit problem dfpv also exhibit competitive performance,8
06fd0879206e9fece83003b953312c7f35f1b229,DEYO: DETR with YOLO for Step-by-Step Object Detection,"Object detection is an important topic in computer vision, with post-processing, an essential part of the typical object detection pipeline, posing a significant bottleneck affecting the performance of traditional object detection models. The detection transformer (DETR), as the first end-to-end target detection model, discards the requirement of manual components like the anchor and non-maximum suppression (NMS), significantly simplifying the target detection process. However, compared with most traditional object detection models, DETR converges very slowly, and a query's meaning is obscure. Thus, inspired by the Step-by-Step concept, this paper proposes a new two-stage object detection model, named DETR with YOLO (DEYO), which relies on a progressive inference to solve the above problems. DEYO is a two-stage architecture comprising a classic target detection model and a DETR-like model as the first and second stages, respectively. Specifically, the first stage provides high-quality query and anchor feeding into the second stage, improving the performance and efficiency of the second stage compared to the original DETR model. Meanwhile, the second stage compensates for the performance degradation caused by the first stage detector's limitations. Extensive experiments demonstrate that DEYO attains 50.6 AP and 52.1 AP in 12 and 36 epochs, respectively, while utilizing ResNet-50 as the backbone and multi-scale features on the COCO dataset. Compared with DINO, an optimal DETR-like model, the developed DEYO model affords a significant performance improvement of 1.6 AP and 1.2 AP in two epoch settings.",2022,"DEYO: DETR with YOLO for Step-by-Step Object DetectionObject detection is an important topic in computer vision, with post-processing, an essential part of the typical object detection pipeline, posing a significant bottleneck affecting the performance of traditional object detection models. The detection transformer (DETR), as the first end-to-end target detection model, discards the requirement of manual components like the anchor and non-maximum suppression (NMS), significantly simplifying the target detection process. However, compared with most traditional object detection models, DETR converges very slowly, and a query's meaning is obscure. Thus, inspired by the Step-by-Step concept, this paper proposes a new two-stage object detection model, named DETR with YOLO (DEYO), which relies on a progressive inference to solve the above problems. DEYO is a two-stage architecture comprising a classic target detection model and a DETR-like model as the first and second stages, respectively. Specifically, the first stage provides high-quality query and anchor feeding into the second stage, improving the performance and efficiency of the second stage compared to the original DETR model. Meanwhile, the second stage compensates for the performance degradation caused by the first stage detector's limitations. Extensive experiments demonstrate that DEYO attains 50.6 AP and 52.1 AP in 12 and 36 epochs, respectively, while utilizing ResNet-50 as the backbone and multi-scale features on the COCO dataset. Compared with DINO, an optimal DETR-like model, the developed DEYO model affords a significant performance improvement of 1.6 AP and 1.2 AP in two epoch settings.",deyo detr yolo object detectionobject detection important topic computer vision essential part typical object detection pipeline posing significant bottleneck affecting performance traditional object detection model detection transformer detr first target detection model discard requirement manual component like anchor suppression nm significantly simplifying target detection process however compared traditional object detection model detr converges slowly query meaning obscure thus inspired concept paper proposes new object detection model named detr yolo deyo relies progressive inference solve problem deyo architecture comprising classic target detection model model first second stage respectively specifically first stage provides query anchor feeding second stage improving performance efficiency second stage compared original detr model meanwhile second stage compensates performance degradation caused first stage detector limitation extensive experiment demonstrate deyo attains ap ap epoch respectively utilizing backbone feature coco dataset compared dino optimal model developed deyo model affords significant performance improvement ap ap two epoch setting,1
4553411d38e9db24a1bd6ac32a9bc67a56a6ea7b,Evolutionary Augmentation Policy Optimization for Self-supervised Learning,"Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and explaining the performance of optimized SSL algorithms. Our results indicate that our proposed method can find solutions that outperform the accuracy of classification of SSL algorithms which confirms the influence of augmentation policy choice on the overall performance of SSL algorithms. We also compare optimal SSL solutions found by our evolutionary search mechanism and show the effect of batch size in the pretext task on two visual datasets.",2023,"Evolutionary Augmentation Policy Optimization for Self-supervised LearningSelf-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and explaining the performance of optimized SSL algorithms. Our results indicate that our proposed method can find solutions that outperform the accuracy of classification of SSL algorithms which confirms the influence of augmentation policy choice on the overall performance of SSL algorithms. We also compare optimal SSL solutions found by our evolutionary search mechanism and show the effect of batch size in the pretext task on two visual datasets.",evolutionary augmentation policy optimization learning ssl machine learning algorithm pretraining deep neural network dnns without requiring manually labeled data central idea learning technique based auxiliary stage aka pretext task labeled data created automatically data augmentation exploited pretraining dnn however effect pretext task well studied compared literature paper study contribution augmentation operator performance self supervised learning algorithm constrained setting propose evolutionary search method optimization data augmentation pipeline pretext task measure impact augmentation operator several sota ssl algorithm encoding different combination augmentation operator chromosome seek optimal augmentation policy evolutionary optimization mechanism introduce method analyzing explaining performance optimized ssl algorithm result indicate proposed method find solution outperform accuracy classification ssl algorithm confirms influence augmentation policy choice overall performance ssl algorithm also compare optimal ssl solution found evolutionary search mechanism show effect batch size pretext task two visual datasets,2
eb61e61c485e9d8f6d0cd7fd4490839092e6c6d4,Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map,"While impressive progress has recently been made in image-oriented facial attribute translation, shape-oriented 3D facial attribute translation remains an unsolved issue. This is primarily limited by the lack of 3D generative models and ineffective usage of 3D facial data. We propose a learning framework for 3D facial attribute translation to relieve these limitations. Firstly, we customize a novel geometric map for 3D shape representation and embed it in an end-to-end generative adversarial network. The geometric map represents 3D shapes symmetrically on a square image grid, while preserving the neighboring relationship of 3D vertices in a local least-square sense. This enables effective learning for the latent representation of data with different attributes. Secondly, we employ a unified and unpaired learning framework for multi-domain attribute translation. It not only makes effective usage of data correlation from multiple domains, but also mitigates the constraint for hardly accessible paired data. Finally, we propose a hierarchical architecture for the discriminator to guarantee robust results against both global and local artifacts. We conduct extensive experiments to demonstrate the advantage of the proposed framework over the state-of-the-art in generating high-fidelity facial shapes. Given an input 3D facial shape, the proposed framework is able to synthesize novel shapes of different attributes, which covers some downstream applications, such as expression transfer, gender translation, and aging. Code at https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap.",2023,"Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric MapWhile impressive progress has recently been made in image-oriented facial attribute translation, shape-oriented 3D facial attribute translation remains an unsolved issue. This is primarily limited by the lack of 3D generative models and ineffective usage of 3D facial data. We propose a learning framework for 3D facial attribute translation to relieve these limitations. Firstly, we customize a novel geometric map for 3D shape representation and embed it in an end-to-end generative adversarial network. The geometric map represents 3D shapes symmetrically on a square image grid, while preserving the neighboring relationship of 3D vertices in a local least-square sense. This enables effective learning for the latent representation of data with different attributes. Secondly, we employ a unified and unpaired learning framework for multi-domain attribute translation. It not only makes effective usage of data correlation from multiple domains, but also mitigates the constraint for hardly accessible paired data. Finally, we propose a hierarchical architecture for the discriminator to guarantee robust results against both global and local artifacts. We conduct extensive experiments to demonstrate the advantage of the proposed framework over the state-of-the-art in generating high-fidelity facial shapes. Given an input 3D facial shape, the proposed framework is able to synthesize novel shapes of different attributes, which covers some downstream applications, such as expression transfer, gender translation, and aging. Code at https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap.",unpaired attribute translation facial shape square symmetric geometric mapwhile impressive progress ha recently made facial attribute translation facial attribute translation remains unsolved issue primarily limited lack generative model ineffective usage facial data propose learning framework facial attribute translation relieve limitation firstly customize novel geometric map shape representation embed generative adversarial network geometric map represents shape symmetrically square image grid preserving neighboring relationship vertex local sense enables effective learning latent representation data different attribute secondly employ unified unpaired learning framework attribute translation make effective usage data correlation multiple domain also mitigates constraint hardly accessible paired data finally propose hierarchical architecture discriminator guarantee robust result global local artifact conduct extensive experiment demonstrate advantage proposed framework generating facial shape given input facial shape proposed framework able synthesize novel shape different attribute cover downstream application expression transfer gender translation aging code http,3
7d43006e97bb30bf22a71a63629592ed177c52c8,Towards Adversarially Robust Continual Learning,"Recent studies show that models trained by continual learning can achieve the comparable performances as the standard supervised learning and the learning flexibility of continual learning models enables their wide applications in the real world. Deep learning models, however, are shown to be vulnerable to adversarial attacks. Though there are many studies on the model robustness in the context of standard supervised learning, protecting continual learning from adversarial attacks has not yet been investigated. To fill in this research gap, we are the first to study adversarial robustness in continual learning and propose a novel method called \textbf{T}ask-\textbf{A}ware \textbf{B}oundary \textbf{A}ugmentation (TABA) to boost the robustness of continual learning models. With extensive experiments on CIFAR-10 and CIFAR-100, we show the efficacy of adversarial training and TABA in defending adversarial attacks.",2023,"Towards Adversarially Robust Continual LearningRecent studies show that models trained by continual learning can achieve the comparable performances as the standard supervised learning and the learning flexibility of continual learning models enables their wide applications in the real world. Deep learning models, however, are shown to be vulnerable to adversarial attacks. Though there are many studies on the model robustness in the context of standard supervised learning, protecting continual learning from adversarial attacks has not yet been investigated. To fill in this research gap, we are the first to study adversarial robustness in continual learning and propose a novel method called \textbf{T}ask-\textbf{A}ware \textbf{B}oundary \textbf{A}ugmentation (TABA) to boost the robustness of continual learning models. With extensive experiments on CIFAR-10 and CIFAR-100, we show the efficacy of adversarial training and TABA in defending adversarial attacks.",towards adversarially robust continual learningrecent study show model trained continual learning achieve comparable performance standard supervised learning learning flexibility continual learning model enables wide application real world deep learning model however shown vulnerable adversarial attack though many study model robustness context standard supervised learning protecting continual learning adversarial attack ha yet investigated fill research gap first study adversarial robustness continual learning propose novel method called ware b oundary ugmentation taba boost robustness continual learning model extensive experiment show efficacy adversarial training taba defending adversarial attack,0
105ad1d2a2c14acb1a0af951e4bed577b17211b6,Preventing Object-centric Discovery of Unsound Process Models for Object Interactions with Loops in Collaborative Systems: Extended Version,"Object-centric process discovery (OCPD) constitutes a paradigm shift in process mining. Instead of assuming a single case notion present in the event log, OCPD can handle events without a single case notion, but that are instead related to a collection of objects each having a certain type. The object types constitute multiple, interacting case notions. The output of OCPD is an object-centric Petri net, i.e. a Petri net with object-typed places, that represents the parallel execution of multiple execution flows corresponding to object types. Similar to classical process discovery, where we aim for behaviorally sound process models as a result, in OCPD, we aim for soundness of the resulting object-centric Petri nets. However, the existing OCPD approach can result in violations of soundness. As we will show, one violation arises for multiple interacting object types with loops that arise in collaborative systems. This paper proposes an extended OCPD approach and proves that it does not suffer from this violation of soundness of the resulting object-centric Petri nets. We also show how we prevent the OCPD approach from introducing spurious interactions in the discovered object-centric Petri net. The proposed framework is prototypically implemented.",2023,"Preventing Object-centric Discovery of Unsound Process Models for Object Interactions with Loops in Collaborative Systems: Extended VersionObject-centric process discovery (OCPD) constitutes a paradigm shift in process mining. Instead of assuming a single case notion present in the event log, OCPD can handle events without a single case notion, but that are instead related to a collection of objects each having a certain type. The object types constitute multiple, interacting case notions. The output of OCPD is an object-centric Petri net, i.e. a Petri net with object-typed places, that represents the parallel execution of multiple execution flows corresponding to object types. Similar to classical process discovery, where we aim for behaviorally sound process models as a result, in OCPD, we aim for soundness of the resulting object-centric Petri nets. However, the existing OCPD approach can result in violations of soundness. As we will show, one violation arises for multiple interacting object types with loops that arise in collaborative systems. This paper proposes an extended OCPD approach and proves that it does not suffer from this violation of soundness of the resulting object-centric Petri nets. We also show how we prevent the OCPD approach from introducing spurious interactions in the discovered object-centric Petri net. The proposed framework is prototypically implemented.",preventing discovery unsound process model object interaction loop collaborative system extended process discovery ocpd constitutes paradigm shift process mining instead assuming single case notion present event log ocpd handle event without single case notion instead related collection object certain type object type constitute multiple interacting case notion output ocpd petri net petri net place represents parallel execution multiple execution flow corresponding object type similar classical process discovery aim behaviorally sound process model result ocpd aim soundness resulting petri net however existing ocpd approach result violation soundness show one violation arises multiple interacting object type loop arise collaborative system paper proposes extended ocpd approach prof doe suffer violation soundness resulting petri net also show prevent ocpd approach introducing spurious interaction discovered petri net proposed framework prototypically implemented,8
9492db2a25009c214bfe8496d915d382046d9019,Scaling Vision-based End-to-End Driving with Multi-View Attention Learning,"On end-to-end driving, human driving demonstrations are used to train perception-based driving models by imitation learning. This process is supervised on vehicle signals (e.g., steering angle, acceleration) but does not require extra costly supervision (human labeling of sensor data). As a representative of such vision-based end-to-end driving models, CILRS is commonly used as a baseline to compare with new driving models. So far, some latest models achieve better performance than CILRS by using expensive sensor suites and/or by using large amounts of human-labeled data for training. Given the difference in performance, one may think that it is not worth pursuing vision-based pure end-to-end driving. However, we argue that this approach still has great value and potential considering cost and maintenance. In this paper, we present CIL++, which improves on CILRS by both processing higher-resolution images using a human-inspired HFOV as an inductive bias and incorporating a proper attention mechanism. CIL++ achieves competitive performance compared to models which are more costly to develop. We propose to replace CILRS with CIL++ as a strong vision-based pure end-to-end driving baseline supervised by only vehicle signals and trained by conditional imitation learning.",2023,"Scaling Vision-based End-to-End Driving with Multi-View Attention LearningOn end-to-end driving, human driving demonstrations are used to train perception-based driving models by imitation learning. This process is supervised on vehicle signals (e.g., steering angle, acceleration) but does not require extra costly supervision (human labeling of sensor data). As a representative of such vision-based end-to-end driving models, CILRS is commonly used as a baseline to compare with new driving models. So far, some latest models achieve better performance than CILRS by using expensive sensor suites and/or by using large amounts of human-labeled data for training. Given the difference in performance, one may think that it is not worth pursuing vision-based pure end-to-end driving. However, we argue that this approach still has great value and potential considering cost and maintenance. In this paper, we present CIL++, which improves on CILRS by both processing higher-resolution images using a human-inspired HFOV as an inductive bias and incorporating a proper attention mechanism. CIL++ achieves competitive performance compared to models which are more costly to develop. We propose to replace CILRS with CIL++ as a strong vision-based pure end-to-end driving baseline supervised by only vehicle signals and trained by conditional imitation learning.",scaling driving attention learningon driving human driving demonstration used train driving model imitation learning process supervised vehicle signal steering angle acceleration doe require extra costly supervision human labeling sensor data representative driving model cilrs commonly used baseline compare new driving model far latest model achieve better performance cilrs using expensive sensor suite using large amount data training given difference performance one may think worth pursuing pure driving however argue approach still ha great value potential considering cost maintenance paper present improves cilrs processing image using hfov inductive bias incorporating proper attention mechanism achieves competitive performance compared model costly develop propose replace cilrs strong pure driving baseline supervised vehicle signal trained conditional imitation learning,1
c54bc5babcfc678be26f063b71ae19338762ec1a,Reinforcement Learning for Syntax-Guided Synthesis,"Program synthesis is the task of automatically generating code based on a specification. In Syntax-Guided Synthesis(SyGuS) this specification is a combination of a syntactic template and a logical formula, and any generated code is proven to satisfy both. Techniques like SyGuS are critical to guaranteeing correct synthesis results. Despite the proliferation of machine learning in other types of program synthesis, state-of-the-art techniques in SyGuS are still driven by automated reasoning tools and simple enumeration. We hypothesize this is for two reasons: first the complexity of the search problem, and second the relatively small data sets available. In this work, we tackle these challenges by framing general SyGuS problems as a tree-search, and present a reinforcement learning guided synthesis algorithm for SyGuS based on Monte-Carlo Tree Search (MCTS). Our algorithm incorporates learned policy and value functions combined with the upper confidence bound for trees to balance exploration and exploitation. We incorporate this search procedure in a reinforcement learning setup in order to iteratively improve our policy and value estimators which are based on boosted tree models. To address the scarcity of training data, we present a method for automatically generating training data for SyGuS based on \emph{anti-unification} of existing first-order satisfiability problems, which we use to train our MCTS policy. We implement and evaluate this setup and demonstrate that learned policy and value improve the synthesis performance over a baseline enumerator by over $26$ percentage points in the training and testing sets. With these results our tool outperforms state-of-the-art-tools such as CVC5 on the training set and performs comparably on the testing set. We make our data set publicly available, enabling further application of machine learning methods to the SyGuS problem.",2023,"Reinforcement Learning for Syntax-Guided SynthesisProgram synthesis is the task of automatically generating code based on a specification. In Syntax-Guided Synthesis(SyGuS) this specification is a combination of a syntactic template and a logical formula, and any generated code is proven to satisfy both. Techniques like SyGuS are critical to guaranteeing correct synthesis results. Despite the proliferation of machine learning in other types of program synthesis, state-of-the-art techniques in SyGuS are still driven by automated reasoning tools and simple enumeration. We hypothesize this is for two reasons: first the complexity of the search problem, and second the relatively small data sets available. In this work, we tackle these challenges by framing general SyGuS problems as a tree-search, and present a reinforcement learning guided synthesis algorithm for SyGuS based on Monte-Carlo Tree Search (MCTS). Our algorithm incorporates learned policy and value functions combined with the upper confidence bound for trees to balance exploration and exploitation. We incorporate this search procedure in a reinforcement learning setup in order to iteratively improve our policy and value estimators which are based on boosted tree models. To address the scarcity of training data, we present a method for automatically generating training data for SyGuS based on \emph{anti-unification} of existing first-order satisfiability problems, which we use to train our MCTS policy. We implement and evaluate this setup and demonstrate that learned policy and value improve the synthesis performance over a baseline enumerator by over $26$ percentage points in the training and testing sets. With these results our tool outperforms state-of-the-art-tools such as CVC5 on the training set and performs comparably on the testing set. We make our data set publicly available, enabling further application of machine learning methods to the SyGuS problem.",reinforcement learning synthesisprogram synthesis task automatically generating code based specification synthesis sygus specification combination syntactic template logical formula generated code proven satisfy technique like sygus critical guaranteeing correct synthesis result despite proliferation machine learning type program synthesis technique sygus still driven automated reasoning tool simple enumeration hypothesize two reason first complexity search problem second relatively small data set available work tackle challenge framing general sygus problem present reinforcement learning guided synthesis algorithm sygus based tree search mcts algorithm incorporates learned policy value function combined upper confidence bound tree balance exploration exploitation incorporate search procedure reinforcement learning setup order iteratively improve policy value estimator based boosted tree model address scarcity training data present method automatically generating training data sygus based existing satisfiability problem use train mcts policy implement evaluate setup demonstrate learned policy value improve synthesis performance baseline enumerator percentage point training testing set result tool outperforms training set performs comparably testing set make data set publicly available enabling application machine learning method sygus problem,8
2861f040a434f6fd64b68f8b9f02affaa991b3fc,Towards High-Quality Specular Highlight Removal by Leveraging Large-Scale Synthetic Data,"This paper aims to remove specular highlights from a single object-level image. Although previous methods have made some progresses, their performance remains somewhat limited, particularly for real images with complex specular highlights. To this end, we propose a three-stage network to address them. Specifically, given an input image, we first decompose it into the albedo, shading, and specular residue components to estimate a coarse specular-free image. Then, we further refine the coarse result to alleviate its visual artifacts such as color distortion. Finally, we adjust the tone of the refined result to match that of the input as closely as possible. In addition, to facilitate network training and quantitative evaluation, we present a large-scale synthetic dataset of object-level images, covering diverse objects and illumination conditions. Extensive experiments illustrate that our network is able to generalize well to unseen real object-level images, and even produce good results for scene-level images with multiple background objects and complex lighting.",2023,"Towards High-Quality Specular Highlight Removal by Leveraging Large-Scale Synthetic DataThis paper aims to remove specular highlights from a single object-level image. Although previous methods have made some progresses, their performance remains somewhat limited, particularly for real images with complex specular highlights. To this end, we propose a three-stage network to address them. Specifically, given an input image, we first decompose it into the albedo, shading, and specular residue components to estimate a coarse specular-free image. Then, we further refine the coarse result to alleviate its visual artifacts such as color distortion. Finally, we adjust the tone of the refined result to match that of the input as closely as possible. In addition, to facilitate network training and quantitative evaluation, we present a large-scale synthetic dataset of object-level images, covering diverse objects and illumination conditions. Extensive experiments illustrate that our network is able to generalize well to unseen real object-level images, and even produce good results for scene-level images with multiple background objects and complex lighting.",towards specular highlight removal leveraging synthetic datathis paper aim remove specular highlight single image although previous method made progress performance remains somewhat limited particularly real image complex specular highlight end propose network address specifically given input image first decompose albedo shading specular residue component estimate coarse image refine coarse result alleviate visual artifact color distortion finally adjust tone refined result match input closely possible addition facilitate network training quantitative evaluation present synthetic dataset image covering diverse object illumination condition extensive experiment illustrate network able generalize well unseen real image even produce good result image multiple background object complex lighting,3
08121a2aa4dedc647a6b519b97a2e58879292107,Coverage-based Example Selection for In-Context Learning,"In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires demonstrations that are informative about the test instance. The standard approach of independently selecting the most similar examples selects redundant demonstrations while overlooking important information. This work proposes a framework for assessing the informativeness of demonstrations based on their coverage of salient aspects (e.g., reasoning patterns) of the test input. Using this framework, we show that contextual token embeddings effectively capture these salient aspects, and their recall measured using BERTScore-Recall (BSR) yields a reliable measure of informativeness. Further, we extend recall metrics like BSR to propose their set versions to find maximally informative sets of demonstrations. On 6 complex compositional generation tasks and 7 diverse LLMs, we show that Set-BSR outperforms the standard similarity-based approach by up to 16% on average and, despite being learning-free, often surpasses methods that leverage task or LLM-specific training.",2023,"Coverage-based Example Selection for In-Context LearningIn-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires demonstrations that are informative about the test instance. The standard approach of independently selecting the most similar examples selects redundant demonstrations while overlooking important information. This work proposes a framework for assessing the informativeness of demonstrations based on their coverage of salient aspects (e.g., reasoning patterns) of the test input. Using this framework, we show that contextual token embeddings effectively capture these salient aspects, and their recall measured using BERTScore-Recall (BSR) yields a reliable measure of informativeness. Further, we extend recall metrics like BSR to propose their set versions to find maximally informative sets of demonstrations. On 6 complex compositional generation tasks and 7 diverse LLMs, we show that Set-BSR outperforms the standard similarity-based approach by up to 16% on average and, despite being learning-free, often surpasses methods that leverage task or LLM-specific training.",example selection learning icl ability large language model perform novel task conditioning prompt task example requires demonstration informative test instance standard approach independently selecting similar example selects redundant demonstration overlooking important information work proposes framework assessing informativeness demonstration based coverage salient aspect reasoning pattern test input using framework show contextual token embeddings effectively capture salient aspect recall measured using bsr yield reliable measure informativeness extend recall metric like bsr propose set version find maximally informative set demonstration complex compositional generation task diverse llm show outperforms standard approach average despite often surpasses method leverage task training,7
062c8a9a8cca3fa0c0c829ca28a00d34cb7f3fe1,"Why Adversarial Reprogramming Works, When It Fails, and How to Tell the Difference",,2021,"Why Adversarial Reprogramming Works, When It Fails, and How to Tell the Difference",adversarial reprogramming work fails tell difference,0
ea7e3911edc5b93765e99dc99244525f481142a8,Conformal Loss-Controlling Prediction,"—Conformal prediction is a learning framework con- trolling prediction coverage of prediction sets, which can be built on any learning algorithm for point prediction. This work proposes a learning framework named conformal loss-controlling prediction, which extends conformal prediction to the situation where the value of a loss function needs to be controlled. Different from existing works about risk-controlling prediction sets and conformal risk control with the purpose of controlling the expected values of loss functions, the proposed approach in this paper focuses on the loss for any test object, which is an extension of conformal prediction from miscoverage loss to some general loss. The controlling guarantee is proved under the assumption of exchangeability of data in ﬁnite-sample cases and the framework is tested empirically for classiﬁcation with a class-varying loss and statistical postprocessing of numerical weather forecasting applications, which are introduced as point-wise classiﬁcation and point-wise regression problems. All theoretical analysis and experimental results conﬁrm the effectiveness of our loss-controlling approach.",2023,"Conformal Loss-Controlling Prediction—Conformal prediction is a learning framework con- trolling prediction coverage of prediction sets, which can be built on any learning algorithm for point prediction. This work proposes a learning framework named conformal loss-controlling prediction, which extends conformal prediction to the situation where the value of a loss function needs to be controlled. Different from existing works about risk-controlling prediction sets and conformal risk control with the purpose of controlling the expected values of loss functions, the proposed approach in this paper focuses on the loss for any test object, which is an extension of conformal prediction from miscoverage loss to some general loss. The controlling guarantee is proved under the assumption of exchangeability of data in ﬁnite-sample cases and the framework is tested empirically for classiﬁcation with a class-varying loss and statistical postprocessing of numerical weather forecasting applications, which are introduced as point-wise classiﬁcation and point-wise regression problems. All theoretical analysis and experimental results conﬁrm the effectiveness of our loss-controlling approach.",conformal prediction learning framework trolling prediction coverage prediction set built learning algorithm point prediction work proposes learning framework named conformal prediction extends conformal prediction situation value loss function need controlled different existing work prediction set conformal risk control purpose controlling expected value loss function proposed approach paper focus loss test object extension conformal prediction miscoverage loss general loss controlling guarantee proved assumption exchangeability data case framework tested empirically classiﬁcation loss statistical postprocessing numerical weather forecasting application introduced classiﬁcation regression problem theoretical analysis experimental result conﬁrm effectiveness approach,8
bbbf98ee91b4ab2dce7e32205a8b9d12aefcf799,ETO Meets Scheduling: Learning Key Knowledge from Single-Objective Problems to Multi-Objective Problem,"Evolutionary transfer optimization(ETO) serves as ""a new frontier in evolutionary computation research"", which will avoid zero reuse of experience and knowledge from solved problems in traditional evolutionary computation. In schedule-ing applications via ETO, a highly competitive ""meeting"" framework between them could be constituted towards both intelligent scheduling and green scheduling, especially for carbon neutrality within the context of China.To the best of our knowledge, our study on scheduling here, is the 1st work of ETO for complex optimization when multi-objective problem ""meets"" single-objective problems in combinatorial case (not multitasking optimization). More specifically, key knowledge like positional building blocks clustered, could be learned and transferred for permutation flow shop scheduling problem (PFSP). Empirical studies on well-studied benchmarks validate relatively firm effectiveness and great potential of our proposed ETO-PFSP framework.",2021,"ETO Meets Scheduling: Learning Key Knowledge from Single-Objective Problems to Multi-Objective ProblemEvolutionary transfer optimization(ETO) serves as ""a new frontier in evolutionary computation research"", which will avoid zero reuse of experience and knowledge from solved problems in traditional evolutionary computation. In schedule-ing applications via ETO, a highly competitive ""meeting"" framework between them could be constituted towards both intelligent scheduling and green scheduling, especially for carbon neutrality within the context of China.To the best of our knowledge, our study on scheduling here, is the 1st work of ETO for complex optimization when multi-objective problem ""meets"" single-objective problems in combinatorial case (not multitasking optimization). More specifically, key knowledge like positional building blocks clustered, could be learned and transferred for permutation flow shop scheduling problem (PFSP). Empirical studies on well-studied benchmarks validate relatively firm effectiveness and great potential of our proposed ETO-PFSP framework.",eto meet scheduling learning key knowledge problem problemevolutionary transfer optimization eto serf new frontier evolutionary computation research avoid zero reuse experience knowledge solved problem traditional evolutionary computation application via eto highly competitive meeting framework could constituted towards intelligent scheduling green scheduling especially carbon neutrality within context best knowledge study scheduling work eto complex optimization problem meet problem combinatorial case multitasking optimization specifically key knowledge like positional building block clustered could learned transferred permutation flow shop scheduling problem pfsp empirical study benchmark validate relatively firm effectiveness great potential proposed framework,9
debee48621164a721b5928729897a89682222654,Contextual Prompt Learning for Vision-Language Understanding,"Recent advances in multimodal learning has resulted in powerful vision-language models, whose representations are generalizable across a variety of downstream tasks. Recently, their generalizability has been further extended by incorporating trainable prompts, borrowed from the natural language processing literature. While such prompt learning techniques have shown impressive results, we identify that these prompts are trained based on global image features which limits itself in two aspects: First, by using global features, these prompts could be focusing less on the discriminative foreground image, resulting in poor generalization to various out-of-distribution test cases. Second, existing work weights all prompts equally whereas our intuition is that these prompts are more specific to the type of the image. We address these issues with as part of our proposed Contextual Prompt Learning (CoPL) framework, capable of aligning the prompts to the localized features of the image. Our key innovations over earlier works include using local image features as part of the prompt learning process, and more crucially, learning to weight these prompts based on local features that are appropriate for the task at hand. This gives us dynamic prompts that are both aligned to local image features as well as aware of local contextual relationships. Our extensive set of experiments on a variety of standard and few-shot datasets show that our method produces substantially improved performance when compared to the current state of the art methods. We also demonstrate both few-shot and out-of-distribution performance to establish the utility of learning dynamic prompts that are aligned to local image features.",2023,"Contextual Prompt Learning for Vision-Language UnderstandingRecent advances in multimodal learning has resulted in powerful vision-language models, whose representations are generalizable across a variety of downstream tasks. Recently, their generalizability has been further extended by incorporating trainable prompts, borrowed from the natural language processing literature. While such prompt learning techniques have shown impressive results, we identify that these prompts are trained based on global image features which limits itself in two aspects: First, by using global features, these prompts could be focusing less on the discriminative foreground image, resulting in poor generalization to various out-of-distribution test cases. Second, existing work weights all prompts equally whereas our intuition is that these prompts are more specific to the type of the image. We address these issues with as part of our proposed Contextual Prompt Learning (CoPL) framework, capable of aligning the prompts to the localized features of the image. Our key innovations over earlier works include using local image features as part of the prompt learning process, and more crucially, learning to weight these prompts based on local features that are appropriate for the task at hand. This gives us dynamic prompts that are both aligned to local image features as well as aware of local contextual relationships. Our extensive set of experiments on a variety of standard and few-shot datasets show that our method produces substantially improved performance when compared to the current state of the art methods. We also demonstrate both few-shot and out-of-distribution performance to establish the utility of learning dynamic prompts that are aligned to local image features.",contextual prompt learning understandingrecent advance multimodal learning ha resulted powerful model whose representation generalizable across variety downstream task recently generalizability ha extended incorporating trainable prompt borrowed natural language processing literature prompt learning technique shown impressive result identify prompt trained based global image feature limit two aspect first using global feature prompt could focusing le discriminative foreground image resulting poor generalization various test case second existing work weight prompt equally whereas intuition prompt specific type image address issue part proposed contextual prompt learning copl framework capable aligning prompt localized feature image key innovation earlier work include using local image feature part prompt learning process crucially learning weight prompt based local feature appropriate task hand give u dynamic prompt aligned local image feature well aware local contextual relationship extensive set experiment variety standard datasets show method produce substantially improved performance compared current state art method also demonstrate performance establish utility learning dynamic prompt aligned local image feature,6
66d2021641c2003d8614c898bbddb653ec349b22,Rethinking Semi-supervised Learning with Language Models,"Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of unlabelled data: Self-training (ST) and Task-adaptive pre-training (TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data, while TAPT continues pre-training on the unlabelled data before fine-tuning. To the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been systematically studied, and no previous work has directly compared TAPT and ST in terms of their ability to utilize the pool of unlabelled data. In this paper, we provide an extensive empirical study comparing five state-of-the-art ST approaches and TAPT across various NLP tasks and data sizes, including in- and out-of-domain settings. Surprisingly, we find that TAPT is a strong and more robust SSL learner, even when using just a few hundred unlabelled samples or in the presence of domain shifts, compared to more sophisticated ST approaches, and tends to bring greater improvements in SSL than in fully-supervised settings. Our further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist. We offer a fresh perspective for future SSL research, suggesting the use of unsupervised pre-training objectives over dependency on pseudo labels.",2023,"Rethinking Semi-supervised Learning with Language ModelsSemi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of unlabelled data: Self-training (ST) and Task-adaptive pre-training (TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data, while TAPT continues pre-training on the unlabelled data before fine-tuning. To the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been systematically studied, and no previous work has directly compared TAPT and ST in terms of their ability to utilize the pool of unlabelled data. In this paper, we provide an extensive empirical study comparing five state-of-the-art ST approaches and TAPT across various NLP tasks and data sizes, including in- and out-of-domain settings. Surprisingly, we find that TAPT is a strong and more robust SSL learner, even when using just a few hundred unlabelled samples or in the presence of domain shifts, compared to more sophisticated ST approaches, and tends to bring greater improvements in SSL than in fully-supervised settings. Our further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist. We offer a fresh perspective for future SSL research, suggesting the use of unsupervised pre-training objectives over dependency on pseudo labels.",rethinking learning language learning ssl popular setting aiming effectively utilize unlabelled data improve model performance downstream natural language processing nlp task currently two popular approach make use unlabelled data st tapt st us teacher model assign unlabelled data tapt continues unlabelled data best knowledge effectiveness tapt ssl task ha systematically studied previous work ha directly compared tapt st term ability utilize pool unlabelled data paper provide extensive empirical study comparing five st approach tapt across various nlp task data size including setting surprisingly find tapt strong robust ssl learner even using hundred unlabelled sample presence domain shift compared sophisticated st approach tends bring greater improvement ssl setting analysis demonstrates risk using st approach size labelled unlabelled data small domain shift exist offer fresh perspective future ssl research suggesting use unsupervised objective dependency pseudo label,2
9d920d44327e7915b01141c58993b27a991f16e8,Online ML Self-adaptation in Face of Traps,"Online machine learning (ML) is often used in self-adaptive systems to strengthen the adaptation mechanism and improve the system utility. Despite such benefits, applying online ML for self-adaptation can be challenging, and not many papers report its limitations. Recently, we experimented with applying online ML for self-adaptation of a smart farming scenario and we had faced several unexpected difficulties -- traps -- that, to our knowledge, are not discussed enough in the community. In this paper, we report our experience with these traps. Specifically, we discuss several traps that relate to the specification and online training of the ML-based estimators, their impact on self-adaptation, and the approach used to evaluate the estimators. Our overview of these traps provides a list of lessons learned, which can serve as guidance for other researchers and practitioners when applying online ML for self-adaptation.",2023,"Online ML Self-adaptation in Face of TrapsOnline machine learning (ML) is often used in self-adaptive systems to strengthen the adaptation mechanism and improve the system utility. Despite such benefits, applying online ML for self-adaptation can be challenging, and not many papers report its limitations. Recently, we experimented with applying online ML for self-adaptation of a smart farming scenario and we had faced several unexpected difficulties -- traps -- that, to our knowledge, are not discussed enough in the community. In this paper, we report our experience with these traps. Specifically, we discuss several traps that relate to the specification and online training of the ML-based estimators, their impact on self-adaptation, and the approach used to evaluate the estimators. Our overview of these traps provides a list of lessons learned, which can serve as guidance for other researchers and practitioners when applying online ML for self-adaptation.",online ml face trapsonline machine learning ml often used system strengthen adaptation mechanism improve system utility despite benefit applying online ml challenging many paper report limitation recently experimented applying online ml smart farming scenario faced several unexpected difficulty trap knowledge discussed enough community paper report experience trap specifically discus several trap relate specification online training estimator impact approach used evaluate estimator overview trap provides list lesson learned serve guidance researcher practitioner applying online ml,9
0e9b936a1e9d9ee56f386f22c50d0a710e6e21ef,Simultaneous identification of models and parameters of scientific simulators,"Many scientific models are composed of multiple discrete components, and scien tists often make heuristic decisions about which components to include. Bayesian inference provides a mathematical framework for systematically selecting model components, but defining prior distributions over model components and developing associated inference schemes has been challenging. We approach this problem in an amortized simulation-based inference framework: We define implicit model priors over a fixed set of candidate components and train neural networks to infer joint probability distributions over both, model components and associated parameters from simulations. To represent distributions over model components, we introduce a conditional mixture of multivariate binary distributions in the Grassmann formalism. Our approach can be applied to any compositional stochastic simulator without requiring access to likelihood evaluations. We first illustrate our method on a simple time series model with redundant components and show that it can retrieve joint posterior distribution over a set of symbolic expressions and their parameters while accurately capturing redundancy with strongly correlated posteriors. We then apply our approach to drift-diffusion models, a commonly used model class in cognitive neuroscience. After validating the method on synthetic data, we show that our approach explains experimental data as well as previous methods, but that our fully probabilistic approach can help to discover multiple data-consistent model configurations, as well as reveal non-identifiable model components and parameters. Our method provides a powerful tool for data-driven scientific inquiry which will allow scientists to systematically identify essential model components and make uncertainty-informed modelling decisions.",2023,"Simultaneous identification of models and parameters of scientific simulatorsMany scientific models are composed of multiple discrete components, and scien tists often make heuristic decisions about which components to include. Bayesian inference provides a mathematical framework for systematically selecting model components, but defining prior distributions over model components and developing associated inference schemes has been challenging. We approach this problem in an amortized simulation-based inference framework: We define implicit model priors over a fixed set of candidate components and train neural networks to infer joint probability distributions over both, model components and associated parameters from simulations. To represent distributions over model components, we introduce a conditional mixture of multivariate binary distributions in the Grassmann formalism. Our approach can be applied to any compositional stochastic simulator without requiring access to likelihood evaluations. We first illustrate our method on a simple time series model with redundant components and show that it can retrieve joint posterior distribution over a set of symbolic expressions and their parameters while accurately capturing redundancy with strongly correlated posteriors. We then apply our approach to drift-diffusion models, a commonly used model class in cognitive neuroscience. After validating the method on synthetic data, we show that our approach explains experimental data as well as previous methods, but that our fully probabilistic approach can help to discover multiple data-consistent model configurations, as well as reveal non-identifiable model components and parameters. Our method provides a powerful tool for data-driven scientific inquiry which will allow scientists to systematically identify essential model components and make uncertainty-informed modelling decisions.",simultaneous identification model parameter scientific simulatorsmany scientific model composed multiple discrete component scien tists often make heuristic decision component include bayesian inference provides mathematical framework systematically selecting model component defining prior distribution model component developing associated inference scheme ha challenging approach problem amortized inference framework define implicit model prior fixed set candidate component train neural network infer joint probability distribution model component associated parameter simulation represent distribution model component introduce conditional mixture multivariate binary distribution grassmann formalism approach applied compositional stochastic simulator without requiring access likelihood evaluation first illustrate method simple time series model redundant component show retrieve joint posterior distribution set symbolic expression parameter accurately capturing redundancy strongly correlated posterior apply approach model commonly used model class cognitive neuroscience validating method synthetic data show approach explains experimental data well previous method fully probabilistic approach help discover multiple model configuration well reveal model component parameter method provides powerful tool scientific inquiry allow scientist systematically identify essential model component make modelling decision,8
f7d3172010a9cb198e335c5d38da5b5acf82e0d3,QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources,"Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update scheme for the quantized weights. As a result, QFT reduces the model state memory to 21% of the standard solution while achieving comparable performance, e.g., tuning a LLaMA-7B model requires only<30GB of memory, satisfied by a single A6000 GPU.",2023,"QFT: Quantized Full-parameter Tuning of LLMs with Affordable ResourcesLarge Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update scheme for the quantized weights. As a result, QFT reduces the model state memory to 21% of the standard solution while achieving comparable performance, e.g., tuning a LLaMA-7B model requires only<30GB of memory, satisfied by a single A6000 GPU.",qft quantized tuning llm affordable resourceslarge language model llm showcased remarkable impact across wide spectrum natural language processing task model downstream datasets provides significant performance gain process ha challenging due extraordinary resource requirement end existing effort focus unfortunately fail capitalize powerful potential work propose qft novel quantized tuning framework llm enables without harming performance framework incorporates two novel idea adopt efficient lion optimizer keep track momentum ha consistent update magnitude parameter inherent advantage robust quantization ii quantize model state store integer value present gradient flow parameter update scheme quantized weight result qft reduces model state memory standard solution achieving comparable performance tuning model requires memory satisfied single gpu,0
97622dbadf8bcb03ae2dc735be995f11490b97ef,Controlling Character Motions without Observable Driving Source,"How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly.",2023,"Controlling Character Motions without Observable Driving SourceHow to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly.",controlling character motion without observable driving sourcehow generate diverse unlimited long sequence without driving source argue research problem ha unique technical challenge behind without semantic constraint driving source using standard autoregressive model generate infinitely long sequence would easily result ood issue due accumulated error insufficient diversity produce natural motion sequence undesired periodic pattern along time tackle challenge propose systematic framework marries benefit novel control policy trained reinforcement learning using carefully designed reward function prior model easily injected top generate unlimited long diverse sequence although focus driving source framework generalized controlled synthesis explicit driving source comprehensive evaluation conclude proposed framework address challenge outperform strong baseline significantly,3
2496220c2da2f57ea3084ac52e36adc6fc4eee94,"Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup","Mixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood. We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution - a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a regression toward the mean, an accidental property that we identify in several datasets. We have found a new equivalence between two successful methods: selective mixup and resampling. We identify limits of the former, confirm the effectiveness of the latter, and find better combinations of their respective benefits.",2023,"Selective Mixup Helps with Distribution Shifts, But Not (Only) because of MixupMixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood. We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution - a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a regression toward the mean, an accidental property that we identify in several datasets. We have found a new equivalence between two successful methods: selective mixup and resampling. We identify limits of the former, confirm the effectiveness of the latter, and find better combinations of their respective benefits.",selective mixup help distribution shift mixupmixup highly successful technique improve generalization neural network augmenting training data combination random pair selective mixup family method apply mixup specific pair combining example across class domain method claimed remarkable improvement benchmark distribution shift mechanism limitation remain poorly understood examine overlooked aspect selective mixup explains success completely new light find selection pair affect training distribution improve generalization mean completely unrelated mixing example binary classification mixup across class implicitly resamples data uniform class distribution classical solution label shift show empirically implicit resampling explains much improvement prior work theoretically result rely regression toward mean accidental property identify several datasets found new equivalence two successful method selective mixup resampling identify limit former confirm effectiveness latter find better combination respective benefit,2
191815e4109ee392b9120b61642c0e859fb662a1,RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding,"Knowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct extensive experiments to verify each component of RulE. Results on multiple benchmarks reveal that our model outperforms the majority of existing embedding-based and rule-based approaches.",2022,"RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule EmbeddingKnowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct extensive experiments to verify each component of RulE. Results on multiple benchmarks reveal that our model outperforms the majority of existing embedding-based and rule-based approaches.",rule knowledge graph reasoning rule embeddingknowledge graph kg reasoning important problem knowledge graph paper propose novel principled framework called rule stand rul e e mbedding effectively leverage logical rule enhance kg reasoning unlike knowledge graph embedding kge method rule learns rule embeddings existing triplet rule jointly representing entity relation logical rule unified embedding space based learned rule embeddings confidence score calculated rule reflecting consistency observed triplet allows u perform logical rule inference soft way thus alleviating brittleness logic hand rule injects prior logical rule information embedding space enriching regularizing embeddings make kge alone perform better rule conceptually simple empirically effective conduct extensive experiment verify component rule result multiple benchmark reveal model outperforms majority existing approach,5
295c940210067e83b9f91161b03873f556ad96c0,TPC: Transformation-Specific Smoothing for Point Cloud Models,"Point cloud models with neural network architectures have achieved great success and have been widely used in safety-critical applications, such as Lidar-based recognition systems in autonomous vehicles. However, such models are shown vulnerable to adversarial attacks which aim to apply stealthy semantic transformations such as rotation and tapering to mislead model predictions. In this paper, we propose a transformation-specific smoothing framework TPC, which provides tight and scalable robustness guarantees for point cloud models against semantic transformation attacks. We first categorize common 3D transformations into three categories: additive (e.g., shearing), composable (e.g., rotation), and indirectly composable (e.g., tapering), and we present generic robustness certification strategies for all categories respectively. We then specify unique certification protocols for a range of specific semantic transformations and their compositions. Extensive experiments on several common 3D transformations show that TPC significantly outperforms the state of the art. For example, our framework boosts the certified accuracy against twisting transformation along z-axis (within 20$^\circ$) from 20.3$\%$ to 83.8$\%$. Codes and models are available at https://github.com/chuwd19/Point-Cloud-Smoothing.",2022,"TPC: Transformation-Specific Smoothing for Point Cloud ModelsPoint cloud models with neural network architectures have achieved great success and have been widely used in safety-critical applications, such as Lidar-based recognition systems in autonomous vehicles. However, such models are shown vulnerable to adversarial attacks which aim to apply stealthy semantic transformations such as rotation and tapering to mislead model predictions. In this paper, we propose a transformation-specific smoothing framework TPC, which provides tight and scalable robustness guarantees for point cloud models against semantic transformation attacks. We first categorize common 3D transformations into three categories: additive (e.g., shearing), composable (e.g., rotation), and indirectly composable (e.g., tapering), and we present generic robustness certification strategies for all categories respectively. We then specify unique certification protocols for a range of specific semantic transformations and their compositions. Extensive experiments on several common 3D transformations show that TPC significantly outperforms the state of the art. For example, our framework boosts the certified accuracy against twisting transformation along z-axis (within 20$^\circ$) from 20.3$\%$ to 83.8$\%$. Codes and models are available at https://github.com/chuwd19/Point-Cloud-Smoothing.",tpc smoothing point cloud modelspoint cloud model neural network architecture achieved great success widely used application recognition system autonomous vehicle however model shown vulnerable adversarial attack aim apply stealthy semantic transformation rotation tapering mislead model prediction paper propose smoothing framework tpc provides tight scalable robustness guarantee point cloud model semantic transformation attack first categorize common transformation three category additive shearing composable rotation indirectly composable tapering present generic robustness certification strategy category respectively specify unique certification protocol range specific semantic transformation composition extensive experiment several common transformation show tpc significantly outperforms state art example framework boost certified accuracy twisting transformation along within code model available http,0
a924a2207906fb192a3ebd6ad0208a5bae684335,SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval),"We present the first Africentric SemEval Shared task, Sentiment Analysis for African Languages (AfriSenti-SemEval) - The dataset is available at https://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval is a sentiment classification challenge in 14 African languages: Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yorb (Muhammad et al., 2023), using data labeled with 3 sentiment classes. We present three subtasks: (1) Task A: monolingual classification, which received 44 submissions; (2) Task B: multilingual classification, which received 32 submissions; and (3) Task C: zero-shot classification, which received 34 submissions. The best performance for tasks A and B was achieved by NLNDE team with 71.31 and 75.06 weighted F1, respectively. UCAS-IIE-NLP achieved the best average score for task C with 58.15 weighted F1. We describe the various approaches adopted by the top 10 systems and their approaches.",2023,"SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval)We present the first Africentric SemEval Shared task, Sentiment Analysis for African Languages (AfriSenti-SemEval) - The dataset is available at https://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval is a sentiment classification challenge in 14 African languages: Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yorb (Muhammad et al., 2023), using data labeled with 3 sentiment classes. We present three subtasks: (1) Task A: monolingual classification, which received 44 submissions; (2) Task B: multilingual classification, which received 32 submissions; and (3) Task C: zero-shot classification, which received 34 submissions. The best performance for tasks A and B was achieved by NLNDE team with 71.31 and 75.06 weighted F1, respectively. UCAS-IIE-NLP achieved the best average score for task C with 58.15 weighted F1. We describe the various approaches adopted by the top 10 systems and their approaches.",task sentiment analysis african language present first africentric semeval shared task sentiment analysis african language dataset available http sentiment classification challenge african language amharic algerian arabic hausa igbo kinyarwanda moroccan arabic mozambican portuguese nigerian pidgin oromo swahili tigrinya twi xitsonga yorb muhammad et using data labeled sentiment class present three subtasks task monolingual classification received submission task b multilingual classification received submission task c classification received submission best performance task b wa achieved nlnde team weighted respectively achieved best average score task c weighted describe various approach adopted top system approach,7
12e2d23a0887edff7e487c41d2588971f7e7627a,The Point to Which Soft Actor-Critic Converges,"Soft actor-critic is a successful successor over soft Q-learning. While lived under maximum entropy framework, their relationship is still unclear. In this paper, we prove that in the limit they converge to the same solution. This is appealing since it translates the optimization from an arduous to an easier way. The same justification can also be applied to other regularizers such as KL divergence.",2023,"The Point to Which Soft Actor-Critic ConvergesSoft actor-critic is a successful successor over soft Q-learning. While lived under maximum entropy framework, their relationship is still unclear. In this paper, we prove that in the limit they converge to the same solution. This is appealing since it translates the optimization from an arduous to an easier way. The same justification can also be applied to other regularizers such as KL divergence.",point soft convergessoft successful successor soft lived maximum entropy framework relationship still unclear paper prove limit converge solution appealing since translates optimization arduous easier way justification also applied regularizers kl divergence,8
42f09ef91efe4fd1b4f94fbae3a38d79f9bb290d,Revisiting Weighted Strategy for Non-stationary Parametric Bandits,"Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a refined analysis framework, which simplifies the derivation and importantly produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\widetilde{O}(k_\mu^{\frac{5}{4}} c_\mu^{-\frac{3}{4}} d^{\frac{3}{4}} P_T^{\frac{1}{4}}T^{\frac{3}{4}})$ regret, improving the $\widetilde{O}(k_\mu^{2} c_\mu^{-1}d^{\frac{9}{10}} P_T^{\frac{1}{5}}T^{\frac{4}{5}})$ bound in prior work, where $k_\mu$ and $c_\mu$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon.",2023,"Revisiting Weighted Strategy for Non-stationary Parametric BanditsNon-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a refined analysis framework, which simplifies the derivation and importantly produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\widetilde{O}(k_\mu^{\frac{5}{4}} c_\mu^{-\frac{3}{4}} d^{\frac{3}{4}} P_T^{\frac{1}{4}}T^{\frac{3}{4}})$ regret, improving the $\widetilde{O}(k_\mu^{2} c_\mu^{-1}d^{\frac{9}{10}} P_T^{\frac{1}{5}}T^{\frac{4}{5}})$ bound in prior work, where $k_\mu$ and $c_\mu$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon.",revisiting weighted strategy parametric parametric bandit attracted much attention recently three principled way deal including weighted restart strategy many environment exhibit gradual drifting pattern weighted strategy commonly adopted application however previous theoretical study show analysis involved algorithm either computationally le efficient statistically suboptimal paper revisits weighted strategy parametric bandit linear bandit lb discover undesirable feature due inadequate regret analysis result overly complex algorithm design propose refined analysis framework simplifies derivation importantly produce simpler algorithm efficient algorithm retaining regret previous study furthermore new framework used improve regret bound parametric bandit including generalized linear bandit glb bandit scb example develop simple weighted glb algorithm regret improving bound prior work characterize reward model nonlinearity measure denote dimension time horizon,8
ee9e96e272524c00d08b59ad183f14f2dc44205b,Multi-label Node Classification On Graph-Structured Data,"Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, we define homophily and Cross-Class Neighborhood Similarity for the multi-label scenario and provide a thorough analyses of the collected $9$ multi-label datasets. Finally, we perform a large-scale comparative study with $8$ methods and $9$ datasets and analyse the performances of the methods to assess the progress made by current state of the art in the multi-label node classification scenario. We release our benchmark at https://github.com/Tianqi-py/MLGNC.",2023,"Multi-label Node Classification On Graph-Structured DataGraph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, we define homophily and Cross-Class Neighborhood Similarity for the multi-label scenario and provide a thorough analyses of the collected $9$ multi-label datasets. Finally, we perform a large-scale comparative study with $8$ methods and $9$ datasets and analyse the performances of the methods to assess the progress made by current state of the art in the multi-label node classification scenario. We release our benchmark at https://github.com/Tianqi-py/MLGNC.",node classification datagraph neural network gnns shown improvement node classification task graph improvement largely demonstrated classification scenario general realistic scenario node could multiple label ha far received little attention first challenge conducting focused study node classification limited number publicly available graph datasets therefore first contribution collect release three biological datasets develop graph generator generate datasets tunable property high label similarity high homophily usually attributed success gnns argue scenario doe follow usual semantics homophily heterophily far defined scenario second contribution define homophily neighborhood similarity scenario provide thorough analysis collected datasets finally perform comparative study method datasets analyse performance method ass progress made current state art node classification scenario release benchmark http,5
2f9f467128ffd754482b342c7196e69b9e52eac9,Modulating Pretrained Diffusion Models for Multimodal Image Synthesis,"We present multimodal conditioning modules (MCM) for enabling conditional image synthesis using pretrained diffusion models. Previous multimodal synthesis works rely on training networks from scratch or fine-tuning pretrained networks, both of which are computationally expensive for large, state-of-the-art diffusion models. Our method uses pretrained networks but does not require any updates to the diffusion network’s parameters. MCM is a small module trained to modulate the diffusion network’s predictions during sampling using 2D modalities (e.g., semantic segmentation maps, sketches) that were unseen during the original training of the diffusion model. We show that MCM enables user control over the spatial layout of the image and leads to increased control over the image generation process. Training MCM is cheap as it does not require gradients from the original diffusion net, consists of only ∼ 1% of the number of parameters of the base diffusion model, and is trained using only a limited number of training examples. We evaluate our method on unconditional and text-conditional models to demonstrate the improved control over the generated images and their alignment with respect to the conditioning inputs.",2023,"Modulating Pretrained Diffusion Models for Multimodal Image SynthesisWe present multimodal conditioning modules (MCM) for enabling conditional image synthesis using pretrained diffusion models. Previous multimodal synthesis works rely on training networks from scratch or fine-tuning pretrained networks, both of which are computationally expensive for large, state-of-the-art diffusion models. Our method uses pretrained networks but does not require any updates to the diffusion network’s parameters. MCM is a small module trained to modulate the diffusion network’s predictions during sampling using 2D modalities (e.g., semantic segmentation maps, sketches) that were unseen during the original training of the diffusion model. We show that MCM enables user control over the spatial layout of the image and leads to increased control over the image generation process. Training MCM is cheap as it does not require gradients from the original diffusion net, consists of only ∼ 1% of the number of parameters of the base diffusion model, and is trained using only a limited number of training examples. We evaluate our method on unconditional and text-conditional models to demonstrate the improved control over the generated images and their alignment with respect to the conditioning inputs.",modulating pretrained diffusion model multimodal image synthesiswe present multimodal conditioning module mcm enabling conditional image synthesis using pretrained diffusion model previous multimodal synthesis work rely training network scratch pretrained network computationally expensive large diffusion model method us pretrained network doe require update diffusion network parameter mcm small module trained modulate diffusion network prediction sampling using modality semantic segmentation map sketch unseen original training diffusion model show mcm enables user control spatial layout image lead increased control image generation process training mcm cheap doe require gradient original diffusion net consists number parameter base diffusion model trained using limited number training example evaluate method unconditional model demonstrate improved control generated image alignment respect conditioning input,3
125aeb18a0e4b6eb75b09b2cebddf7b73e062809,Semantic Specialization for Knowledge-based Word Sense Disambiguation,"A promising approach for knowledge-based Word Sense Disambiguation (WSD) is to select the sense whose contextualized embeddings computed for its definition sentence are closest to those computed for a target word in a given sentence. This approach relies on the similarity of the sense and context embeddings computed by a pre-trained language model. We propose a semantic specialization for WSD where contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The key idea is, for a given sense, to bring semantically related senses and contexts closer and send different/unrelated senses farther away. We realize this idea as the joint optimization of the Attract-Repel objective for sense pairs and the self-training objective for context-sense pairs while controlling deviations from the original embeddings. The proposed method outperformed previous studies that adapt contextualized embeddings. It achieved state-of-the-art performance on knowledge-based WSD when combined with the reranking heuristic that uses the sense inventory. We found that the similarity characteristics of specialized embeddings conform to the key idea. We also found that the (dis)similarity of embeddings between the related/different/unrelated senses correlates well with the performance of WSD.",2023,"Semantic Specialization for Knowledge-based Word Sense DisambiguationA promising approach for knowledge-based Word Sense Disambiguation (WSD) is to select the sense whose contextualized embeddings computed for its definition sentence are closest to those computed for a target word in a given sentence. This approach relies on the similarity of the sense and context embeddings computed by a pre-trained language model. We propose a semantic specialization for WSD where contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The key idea is, for a given sense, to bring semantically related senses and contexts closer and send different/unrelated senses farther away. We realize this idea as the joint optimization of the Attract-Repel objective for sense pairs and the self-training objective for context-sense pairs while controlling deviations from the original embeddings. The proposed method outperformed previous studies that adapt contextualized embeddings. It achieved state-of-the-art performance on knowledge-based WSD when combined with the reranking heuristic that uses the sense inventory. We found that the similarity characteristics of specialized embeddings conform to the key idea. We also found that the (dis)similarity of embeddings between the related/different/unrelated senses correlates well with the performance of WSD.",semantic specialization word sense disambiguationa promising approach word sense disambiguation wsd select sense whose contextualized embeddings computed definition sentence closest computed target word given sentence approach relies similarity sense context embeddings computed language model propose semantic specialization wsd contextualized embeddings adapted wsd task using solely lexical knowledge key idea given sense bring semantically related sens context closer send sens farther away realize idea joint optimization objective sense pair objective pair controlling deviation original embeddings proposed method outperformed previous study adapt contextualized embeddings achieved performance wsd combined reranking heuristic us sense inventory found similarity characteristic specialized embeddings conform key idea also found dis similarity embeddings sens correlate well performance wsd,7
4d3a08f3b53261c84101a3473a8dc237b7223184,"Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables","Question answering on tabular data (a.k.a TableQA), which aims at generating answers to questions grounded on a provided table, has gained significant attention recently. Prior work primarily produces concise factual responses through information extraction from individual or limited table cells, lacking the ability to reason across diverse table cells. Yet, the realm of free-form TableQA, which demands intricate strategies for selecting relevant table cells and the sophisticated integration and inference of discrete data fragments, remains mostly unexplored. To this end, this paper proposes a generalized three-stage approach: Table-to- Graph conversion and cell localizing, external knowledge retrieval, and the fusion of table and text (called TAG-QA), to address the challenge of inferring long free-form answers in generative TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph neural network to gather intersecting cells between relevant rows and columns, (2) leverages external knowledge from Wikipedia, and (3) generates answers by integrating both tabular data and natural linguistic information. Experiments showcase the superior capabilities of TAG-QA in generating sentences that are both faithful and coherent, particularly when compared to several state-of-the-art baselines. Notably, TAG-QA surpasses the robust pipeline-based baseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score, respectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score, respectively.",2023,"Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over TablesQuestion answering on tabular data (a.k.a TableQA), which aims at generating answers to questions grounded on a provided table, has gained significant attention recently. Prior work primarily produces concise factual responses through information extraction from individual or limited table cells, lacking the ability to reason across diverse table cells. Yet, the realm of free-form TableQA, which demands intricate strategies for selecting relevant table cells and the sophisticated integration and inference of discrete data fragments, remains mostly unexplored. To this end, this paper proposes a generalized three-stage approach: Table-to- Graph conversion and cell localizing, external knowledge retrieval, and the fusion of table and text (called TAG-QA), to address the challenge of inferring long free-form answers in generative TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph neural network to gather intersecting cells between relevant rows and columns, (2) leverages external knowledge from Wikipedia, and (3) generates answers by integrating both tabular data and natural linguistic information. Experiments showcase the superior capabilities of TAG-QA in generating sentences that are both faithful and coherent, particularly when compared to several state-of-the-art baselines. Notably, TAG-QA surpasses the robust pipeline-based baseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score, respectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score, respectively.",localize retrieve fuse generalized framework question answering tablesquestion answering tabular data tableqa aim generating answer question grounded provided table ha gained significant attention recently prior work primarily produce concise factual response information extraction individual limited table cell lacking ability reason across diverse table cell yet realm tableqa demand intricate strategy selecting relevant table cell sophisticated integration inference discrete data fragment remains mostly unexplored end paper proposes generalized approach graph conversion cell localizing external knowledge retrieval fusion table text called address challenge inferring long answer generative tableqa particular locates relevant table cell using graph neural network gather intersecting cell relevant row column leverage external knowledge wikipedia generates answer integrating tabular data natural linguistic information experiment showcase superior capability generating sentence faithful coherent particularly compared several baseline notably surpasses robust baseline tapa term parent respectively furthermore outperforms model parent respectively,7
f92bedc210c05165957683b2be400fb01ae6056d,AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation,"Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of domain shift. The proposed method, named AR-TTA, outperforms existing approaches on both synthetic and more real-world benchmarks and shows robustness across a variety of TTA scenarios.",2023,"AR-TTA: A Simple Method for Real-World Continual Test-Time AdaptationTest-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of domain shift. The proposed method, named AR-TTA, outperforms existing approaches on both synthetic and more real-world benchmarks and shows robustness across a variety of TTA scenarios.",simple method continual adaptation promising research direction allows source model adapt change data distribution without supervision yet current method usually evaluated benchmark simplification scenario hence propose validate adaptation method using recently introduced datasets autonomous driving namely shift observe current adaptation method struggle effectively handle varying degree domain shift often resulting degraded performance fall source model noticed root problem lie inability preserve knowledge source model adapt dynamically changing temporally correlated data stream therefore enhance framework incorporating small memory buffer increase model stability time perform dynamic adaptation based intensity domain shift proposed method named outperforms existing approach synthetic benchmark show robustness across variety tta scenario,2
c714f39c410eb934565b398a829edac0b6058728,"When Layers Play the Lottery, all Tickets Win at Initialization","Pruning is a standard technique for reducing the computational cost of deep networks. Many advances in pruning leverage concepts from the Lottery Ticket Hypothesis (LTH). LTH reveals that inside a trained dense network exists sparse subnetworks (tickets) able to achieve similar accuracy (i.e., win the lottery - winning tickets). Pruning at initialization focuses on finding winning tickets without training a dense network. Studies on these concepts share the trend that subnetworks come from weight or filter pruning. In this work, we investigate LTH and pruning at initialization from the lens of layer pruning. First, we confirm the existence of winning tickets when the pruning process removes layers. Leveraged by this observation, we propose to discover these winning tickets at initialization, eliminating the requirement of heavy computational resources for training the initial (over-parameterized) dense network. Extensive experiments show that our winning tickets notably speed up the training phase and reduce up to 51% of carbon emission, an important step towards democratization and green Artificial Intelligence. Beyond computational benefits, our winning tickets exhibit robustness against adversarial and out-of-distribution examples. Finally, we show that our subnetworks easily win the lottery at initialization while tickets from filter removal (the standard structured LTH) hardly become winning tickets.",2023,"When Layers Play the Lottery, all Tickets Win at InitializationPruning is a standard technique for reducing the computational cost of deep networks. Many advances in pruning leverage concepts from the Lottery Ticket Hypothesis (LTH). LTH reveals that inside a trained dense network exists sparse subnetworks (tickets) able to achieve similar accuracy (i.e., win the lottery - winning tickets). Pruning at initialization focuses on finding winning tickets without training a dense network. Studies on these concepts share the trend that subnetworks come from weight or filter pruning. In this work, we investigate LTH and pruning at initialization from the lens of layer pruning. First, we confirm the existence of winning tickets when the pruning process removes layers. Leveraged by this observation, we propose to discover these winning tickets at initialization, eliminating the requirement of heavy computational resources for training the initial (over-parameterized) dense network. Extensive experiments show that our winning tickets notably speed up the training phase and reduce up to 51% of carbon emission, an important step towards democratization and green Artificial Intelligence. Beyond computational benefits, our winning tickets exhibit robustness against adversarial and out-of-distribution examples. Finally, we show that our subnetworks easily win the lottery at initialization while tickets from filter removal (the standard structured LTH) hardly become winning tickets.",layer play lottery ticket win initializationpruning standard technique reducing computational cost deep network many advance pruning leverage concept lottery ticket hypothesis lth lth reveals inside trained dense network exists sparse subnetworks ticket able achieve similar accuracy win lottery winning ticket pruning initialization focus finding winning ticket without training dense network study concept share trend subnetworks come weight filter pruning work investigate lth pruning initialization lens layer pruning first confirm existence winning ticket pruning process remove layer leveraged observation propose discover winning ticket initialization eliminating requirement heavy computational resource training initial dense network extensive experiment show winning ticket notably speed training phase reduce carbon emission important step towards democratization green artificial intelligence beyond computational benefit winning ticket exhibit robustness adversarial example finally show subnetworks easily win lottery initialization ticket filter removal standard structured lth hardly become winning ticket,0
a3fd994c3389a2b0564424b6d8d87308223c8753,Identifying Weight-Variant Latent Causal Models,"The task of causal representation learning aims to uncover latent higher-level causal representations that affect lower-level observations. Identifying true latent causal representations from observed data, while allowing instantaneous causal relations among latent variables, remains a challenge, however. To this end, we start from the analysis of three intrinsic properties in identifying latent space from observations: transitivity, permutation indeterminacy, and scaling indeterminacy. We find that transitivity acts as a key role in impeding the identifiability of latent causal representations. To address the unidentifiable issue due to transitivity, we introduce a novel identifiability condition where the underlying latent causal model satisfies a linear-Gaussian model, in which the causal coefficients and the distribution of Gaussian noise are modulated by an additional observed variable. Under some mild assumptions, we can show that the latent causal representations can be identified up to trivial permutation and scaling. Furthermore, based on this theoretical result, we propose a novel method, termed Structural caUsAl Variational autoEncoder, which directly learns latent causal representations and causal relationships among them, together with the mapping from the latent causal variables to the observed ones. We show that the proposed method learns the true parameters asymptotically. Experimental results on synthetic and real data demonstrate the identifiability and consistency results and the efficacy of the proposed method in learning latent causal representations.",2022,"Identifying Weight-Variant Latent Causal ModelsThe task of causal representation learning aims to uncover latent higher-level causal representations that affect lower-level observations. Identifying true latent causal representations from observed data, while allowing instantaneous causal relations among latent variables, remains a challenge, however. To this end, we start from the analysis of three intrinsic properties in identifying latent space from observations: transitivity, permutation indeterminacy, and scaling indeterminacy. We find that transitivity acts as a key role in impeding the identifiability of latent causal representations. To address the unidentifiable issue due to transitivity, we introduce a novel identifiability condition where the underlying latent causal model satisfies a linear-Gaussian model, in which the causal coefficients and the distribution of Gaussian noise are modulated by an additional observed variable. Under some mild assumptions, we can show that the latent causal representations can be identified up to trivial permutation and scaling. Furthermore, based on this theoretical result, we propose a novel method, termed Structural caUsAl Variational autoEncoder, which directly learns latent causal representations and causal relationships among them, together with the mapping from the latent causal variables to the observed ones. We show that the proposed method learns the true parameters asymptotically. Experimental results on synthetic and real data demonstrate the identifiability and consistency results and the efficacy of the proposed method in learning latent causal representations.",identifying latent causal modelsthe task causal representation learning aim uncover latent causal representation affect observation identifying true latent causal representation observed data allowing instantaneous causal relation among latent variable remains challenge however end start analysis three intrinsic property identifying latent space observation transitivity permutation indeterminacy scaling indeterminacy find transitivity act key role impeding identifiability latent causal representation address unidentifiable issue due transitivity introduce novel identifiability condition underlying latent causal model satisfies model causal coefficient distribution gaussian noise modulated additional observed variable mild assumption show latent causal representation identified trivial permutation scaling furthermore based theoretical result propose novel method termed structural causal variational autoencoder directly learns latent causal representation causal relationship among together mapping latent causal variable observed one show proposed method learns true parameter asymptotically experimental result synthetic real data demonstrate identifiability consistency result efficacy proposed method learning latent causal representation,8
edae09bc0afbc863e918acced670faa505d8f52e,Subspace Adaptation Prior for Few-Shot Learning,,2023,Subspace Adaptation Prior for Few-Shot Learning,subspace adaptation prior learning,2
3007fc536ed6a2aff26588b0efedcba4ef570580,"What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue","This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.",2023,"What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging DialogueThis paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.",ground designing user conversational agent engaging dialoguethis paper present method building personalized dialogue system address wwh problem natural response generation commercial setting personalized dialogue response heavily interleaved casual response turn proposed approach involves weighted dataset blending negative persona information augmentation method design personalized conversation datasets address challenge wwh personalized dialogue system work effectively balance dialogue fluency tendency ground also introducing label improve controllability explainability grounded response combination method lead fluent conversation evidenced subjective human evaluation well objective evaluation,7
8bfef5324554090afae97327002340a6aea89270,Diffusion-based Target Sampler for Unsupervised Domain Adaptation,"Limited transferability hinders the performance of deep learning models when applied to new application scenarios. Recently, unsupervised domain adaptation (UDA) has achieved significant progress in addressing this issue via learning domain-invariant features. However, large domain shifts and the sample scarcity in the target domain make existing UDA methods achieve suboptimal performance. To alleviate these issues, we propose a plug-and-play Diffusion-based Target Sampler (DTS) to generate high fidelity and diversity pseudo target samples. By introducing class-conditional information, the labels of the generated target samples can be controlled. The generated samples can well simulate the data distribution of the target domain and help existing UDA methods transfer from the source domain to the target domain more easily, thus improving the transfer performance. Extensive experiments on various benchmarks demonstrate that the performance of existing UDA methods can be greatly improved through the proposed DTS method.",2023,"Diffusion-based Target Sampler for Unsupervised Domain AdaptationLimited transferability hinders the performance of deep learning models when applied to new application scenarios. Recently, unsupervised domain adaptation (UDA) has achieved significant progress in addressing this issue via learning domain-invariant features. However, large domain shifts and the sample scarcity in the target domain make existing UDA methods achieve suboptimal performance. To alleviate these issues, we propose a plug-and-play Diffusion-based Target Sampler (DTS) to generate high fidelity and diversity pseudo target samples. By introducing class-conditional information, the labels of the generated target samples can be controlled. The generated samples can well simulate the data distribution of the target domain and help existing UDA methods transfer from the source domain to the target domain more easily, thus improving the transfer performance. Extensive experiments on various benchmarks demonstrate that the performance of existing UDA methods can be greatly improved through the proposed DTS method.",target sampler unsupervised domain adaptationlimited transferability hinders performance deep learning model applied new application scenario recently unsupervised domain adaptation uda ha achieved significant progress addressing issue via learning feature however large domain shift sample scarcity target domain make existing uda method achieve suboptimal performance alleviate issue propose target sampler dts generate high fidelity diversity pseudo target sample introducing information label generated target sample controlled generated sample well simulate data distribution target domain help existing uda method transfer source domain target domain easily thus improving transfer performance extensive experiment various benchmark demonstrate performance existing uda method greatly improved proposed dts method,2
93975d9fdc724fd67ec677d72585b632b6b944f7,Towards a Universal Understanding of Color Harmony: Fuzzy Approach,"Harmony level prediction is receiving increasing attention nowadays. Color plays a crucial role in affecting human aesthetic responses. In this paper, we explore color harmony using a fuzzy-based color model and address the question of its universality. For our experiments, we utilize a dataset containing attractive images from five different domains: fashion, art, nature, interior design, and brand logos. We aim to identify harmony patterns and dominant color palettes within these images using a fuzzy approach. It is well-suited for this task because it can handle the inherent subjectivity and contextual variability associated with aesthetics and color harmony evaluation. Our experimental results suggest that color harmony is largely universal. Additionally, our findings reveal that color harmony is not solely influenced by hue relationships on the color wheel but also by the saturation and intensity of colors. In palettes with high harmony levels, we observed a prevalent adherence to color wheel principles while maintaining moderate levels of saturation and intensity. These findings contribute to ongoing research on color harmony and its underlying principles, offering valuable insights for designers, artists, and researchers in the field of aesthetics.",2023,"Towards a Universal Understanding of Color Harmony: Fuzzy ApproachHarmony level prediction is receiving increasing attention nowadays. Color plays a crucial role in affecting human aesthetic responses. In this paper, we explore color harmony using a fuzzy-based color model and address the question of its universality. For our experiments, we utilize a dataset containing attractive images from five different domains: fashion, art, nature, interior design, and brand logos. We aim to identify harmony patterns and dominant color palettes within these images using a fuzzy approach. It is well-suited for this task because it can handle the inherent subjectivity and contextual variability associated with aesthetics and color harmony evaluation. Our experimental results suggest that color harmony is largely universal. Additionally, our findings reveal that color harmony is not solely influenced by hue relationships on the color wheel but also by the saturation and intensity of colors. In palettes with high harmony levels, we observed a prevalent adherence to color wheel principles while maintaining moderate levels of saturation and intensity. These findings contribute to ongoing research on color harmony and its underlying principles, offering valuable insights for designers, artists, and researchers in the field of aesthetics.",towards universal understanding color harmony fuzzy approachharmony level prediction receiving increasing attention nowadays color play crucial role affecting human aesthetic response paper explore color harmony using color model address question universality experiment utilize dataset containing attractive image five different domain fashion art nature interior design brand logo aim identify harmony pattern dominant color palette within image using fuzzy approach task handle inherent subjectivity contextual variability associated aesthetic color harmony evaluation experimental result suggest color harmony largely universal additionally finding reveal color harmony solely influenced hue relationship color wheel also saturation intensity color palette high harmony level observed prevalent adherence color wheel principle maintaining moderate level saturation intensity finding contribute ongoing research color harmony underlying principle offering valuable insight designer artist researcher field aesthetic,3
52362ef4b08087313e19b7c3d550cdab38791927,Leveraging Different Learning Styles for Improved Knowledge Distillation,"Learning style refers to a type of training mechanism adopted by an individual to gain new knowledge. As suggested by the VARK model, humans have different learning preferences like visual, auditory, etc., for acquiring and effectively processing information. Inspired by this concept, our work explores the idea of mixed information sharing with model compression in the context of Knowledge Distillation (KD) and Mutual Learning (ML). Unlike conventional techniques that share the same type of knowledge with all networks, we propose to train individual networks with different forms of information to enhance the learning process. We formulate a combined KD and ML framework with one teacher and two student networks that share or exchange information in the form of predictions and feature maps. Our comprehensive experiments with benchmark classification and segmentation datasets demonstrate that with 15% compression, the ensemble performance of networks trained with diverse forms of knowledge outperforms the conventional techniques both quantitatively and qualitatively.",2022,"Leveraging Different Learning Styles for Improved Knowledge DistillationLearning style refers to a type of training mechanism adopted by an individual to gain new knowledge. As suggested by the VARK model, humans have different learning preferences like visual, auditory, etc., for acquiring and effectively processing information. Inspired by this concept, our work explores the idea of mixed information sharing with model compression in the context of Knowledge Distillation (KD) and Mutual Learning (ML). Unlike conventional techniques that share the same type of knowledge with all networks, we propose to train individual networks with different forms of information to enhance the learning process. We formulate a combined KD and ML framework with one teacher and two student networks that share or exchange information in the form of predictions and feature maps. Our comprehensive experiments with benchmark classification and segmentation datasets demonstrate that with 15% compression, the ensemble performance of networks trained with diverse forms of knowledge outperforms the conventional techniques both quantitatively and qualitatively.",leveraging different learning style improved knowledge distillationlearning style refers type training mechanism adopted individual gain new knowledge suggested vark model human different learning preference like visual auditory acquiring effectively processing information inspired concept work explores idea mixed information sharing model compression context knowledge distillation kd mutual learning ml unlike conventional technique share type knowledge network propose train individual network different form information enhance learning process formulate combined kd ml framework one teacher two student network share exchange information form prediction feature map comprehensive experiment benchmark classification segmentation datasets demonstrate compression ensemble performance network trained diverse form knowledge outperforms conventional technique quantitatively qualitatively,5
d6da236a8594d0f60b4c4f1568282de572bc09cd,Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values for Multi-classifiers,"While advanced classifiers have been increasingly used in real-world safety-critical applications, how to properly evaluate the black-box models given specific human values remains a concern in the community. Such human values include punishing error cases of different severity in varying degrees and making compromises in general performance to reduce specific dangerous cases. In this paper, we propose a novel evaluation measure named Meta Pattern Concern Score based on the abstract representation of probabilistic prediction and the adjustable threshold for the concession in prediction confidence, to introduce the human values into multi-classifiers. Technically, we learn from the advantages and disadvantages of two kinds of common metrics, namely the confusion matrix-based evaluation measures and the loss values, so that our measure is effective as them even under general tasks, and the cross entropy loss becomes a special case of our measure in the limit. Besides, our measure can also be used to refine the model training by dynamically adjusting the learning rate. The experiments on four kinds of models and six datasets confirm the effectiveness and efficiency of our measure. And a case study shows it can not only find the ideal model reducing 0.53% of dangerous cases by only sacrificing 0.04% of training accuracy, but also refine the learning rate to train a new model averagely outperforming the original one with a 1.62% lower value of itself and 0.36% fewer number of dangerous cases.",2022,"Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values for Multi-classifiersWhile advanced classifiers have been increasingly used in real-world safety-critical applications, how to properly evaluate the black-box models given specific human values remains a concern in the community. Such human values include punishing error cases of different severity in varying degrees and making compromises in general performance to reduce specific dangerous cases. In this paper, we propose a novel evaluation measure named Meta Pattern Concern Score based on the abstract representation of probabilistic prediction and the adjustable threshold for the concession in prediction confidence, to introduce the human values into multi-classifiers. Technically, we learn from the advantages and disadvantages of two kinds of common metrics, namely the confusion matrix-based evaluation measures and the loss values, so that our measure is effective as them even under general tasks, and the cross entropy loss becomes a special case of our measure in the limit. Besides, our measure can also be used to refine the model training by dynamically adjusting the learning rate. The experiments on four kinds of models and six datasets confirm the effectiveness and efficiency of our measure. And a case study shows it can not only find the ideal model reducing 0.53% of dangerous cases by only sacrificing 0.04% of training accuracy, but also refine the learning rate to train a new model averagely outperforming the original one with a 1.62% lower value of itself and 0.36% fewer number of dangerous cases.",meta pattern concern score novel evaluation measure human value advanced classifier increasingly used application properly evaluate model given specific human value remains concern community human value include punishing error case different severity varying degree making compromise general performance reduce specific dangerous case paper propose novel evaluation measure named meta pattern concern score based abstract representation probabilistic prediction adjustable threshold concession prediction confidence introduce human value technically learn advantage disadvantage two kind common metric namely confusion evaluation measure loss value measure effective even general task cross entropy loss becomes special case measure limit besides measure also used refine model training dynamically adjusting learning rate experiment four kind model six datasets confirm effectiveness efficiency measure case study show find ideal model reducing dangerous case sacrificing training accuracy also refine learning rate train new model averagely outperforming original one lower value fewer number dangerous case,2
1d2a0b982b52fcd6d14185d9c61cce8c85cb8d33,Discovering and Explaining Driver Behaviour under HoS Regulations,"World wide transport authorities are imposing complex Hours of Service regulations to drivers, which constraint the amount of working, driving and resting time when delivering a service. As a consequence, transport companies are responsible not only of scheduling driving plans aligned with laws that deﬁne the legal behaviour of a driver, but also of monitoring and identifying as soon as possible problematic patterns that can incur in costs due to sanctions. Transport experts are frequently in charge of many drivers and lack time to analyse the vast amount of data recorded by the onboard sensors, and companies have grown accustomed to pay sanctions rather than predict and forestall wrongdoings. This paper exposes an application for summarising raw driver activity logs according to these regulations and for explaining driver behaviour in a human readable format. The system employs planning, constraint, and clustering techniques to extract and describe what the driver has been doing while identifying infractions and the activities that originate them. Furthermore, it groups drivers based on similar driving patterns. An experimentation in real world data indicates that recurring driving patterns can be clustered from short basic driving sequences to whole drivers working days.",2023,"Discovering and Explaining Driver Behaviour under HoS RegulationsWorld wide transport authorities are imposing complex Hours of Service regulations to drivers, which constraint the amount of working, driving and resting time when delivering a service. As a consequence, transport companies are responsible not only of scheduling driving plans aligned with laws that deﬁne the legal behaviour of a driver, but also of monitoring and identifying as soon as possible problematic patterns that can incur in costs due to sanctions. Transport experts are frequently in charge of many drivers and lack time to analyse the vast amount of data recorded by the onboard sensors, and companies have grown accustomed to pay sanctions rather than predict and forestall wrongdoings. This paper exposes an application for summarising raw driver activity logs according to these regulations and for explaining driver behaviour in a human readable format. The system employs planning, constraint, and clustering techniques to extract and describe what the driver has been doing while identifying infractions and the activities that originate them. Furthermore, it groups drivers based on similar driving patterns. An experimentation in real world data indicates that recurring driving patterns can be clustered from short basic driving sequences to whole drivers working days.",discovering explaining driver behaviour ho regulationsworld wide transport authority imposing complex hour service regulation driver constraint amount working driving resting time delivering service consequence transport company responsible scheduling driving plan aligned law deﬁne legal behaviour driver also monitoring identifying soon possible problematic pattern incur cost due sanction transport expert frequently charge many driver lack time analyse vast amount data recorded onboard sensor company grown accustomed pay sanction rather predict forestall wrongdoing paper expose application summarising raw driver activity log according regulation explaining driver behaviour human readable format system employ planning constraint clustering technique extract describe driver ha identifying infraction activity originate furthermore group driver based similar driving pattern experimentation real world data indicates recurring driving pattern clustered short basic driving sequence whole driver working day,9
973ce9aa3251857693abc21438b35432a8d83bec,Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?,"Translation Quality Estimation (TQE) is an essential step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. This work examines whether the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using \textbf{eight language pairs} including English to Italian, German, French, Japanese, Dutch, Portuguese, Turkish, and Chinese training corpora, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. \textit{if the translation needs to be edited}. However, there is definitely much space to improve the model accuracy, e.g. they are 82.42\% and 83.69\% for English-Italian and English-German respectively using our experimental settings. English-Italiano bilingual Abstract is available in the paper.",2023,"Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?Translation Quality Estimation (TQE) is an essential step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. This work examines whether the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using \textbf{eight language pairs} including English to Italian, German, French, Japanese, Dutch, Portuguese, Turkish, and Chinese training corpora, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. \textit{if the translation needs to be edited}. However, there is definitely much space to improve the model accuracy, e.g. they are 82.42\% and 83.69\% for English-Italian and English-German respectively using our experimental settings. English-Italiano bilingual Abstract is available in the paper.",predicting perfect quality segment mt output openai llm possible capture editing distance pattern historical data translation quality estimation tqe essential step deploying output translation usage tqe also critical assessing machine translation mt human translation ht quality without seeing reference translation work examines whether large language model llm tqe task capability take chatgpt one example approach tqe binary classification task using eight language pair including english italian german french japanese dutch portuguese turkish chinese training corpus experimental result show chatgpt via api achieve relatively high score predicting translation quality translation need edited however definitely much space improve model accuracy respectively using experimental setting bilingual abstract available paper,7
e962f95e03a50ff2f3a0fe7840daebac04578c46,Structure-informed Language Models Are Protein Designers,"This paper demonstrates that language models are strong structure-based protein designers. We present LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds. We conduct a structural surgery on pLMs, where a lightweight structural adapter is implanted into pLMs and endows it with structural awareness. During inference, iterative refinement is performed to effectively optimize the generated protein sequences. Experiments show that LM-Design improves the state-of-the-art results by a large margin, leading to 4% to 12% accuracy gains in sequence recovery (e.g., 55.65%/56.63% on CATH 4.2/4.3 single-chain benchmarks, and >60% when designing protein complexes). We provide extensive and in-depth analyses, which verify that LM-Design can (1) indeed leverage both structural and sequential knowledge to accurately handle structurally non-deterministic regions, (2) benefit from scaling data and model size, and (3) generalize to other proteins (e.g., antibodies and de novo proteins).",2023,"Structure-informed Language Models Are Protein DesignersThis paper demonstrates that language models are strong structure-based protein designers. We present LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds. We conduct a structural surgery on pLMs, where a lightweight structural adapter is implanted into pLMs and endows it with structural awareness. During inference, iterative refinement is performed to effectively optimize the generated protein sequences. Experiments show that LM-Design improves the state-of-the-art results by a large margin, leading to 4% to 12% accuracy gains in sequence recovery (e.g., 55.65%/56.63% on CATH 4.2/4.3 single-chain benchmarks, and >60% when designing protein complexes). We provide extensive and in-depth analyses, which verify that LM-Design can (1) indeed leverage both structural and sequential knowledge to accurately handle structurally non-deterministic regions, (2) benefit from scaling data and model size, and (3) generalize to other proteins (e.g., antibodies and de novo proteins).",language model protein designersthis paper demonstrates language model strong protein designer present generic approach reprogramming protein language model plms learned massive sequential evolutionary knowledge universe natural protein sequence acquire immediate capability design preferable protein sequence given fold conduct structural surgery plms lightweight structural adapter implanted plms endows structural awareness inference iterative refinement performed effectively optimize generated protein sequence experiment show improves result large margin leading accuracy gain sequence recovery cath benchmark designing protein complex provide extensive analysis verify indeed leverage structural sequential knowledge accurately handle structurally region benefit scaling data model size generalize protein antibody de novo protein,5
e468ed6b824e60f45ba9a20b034e4090c6630751,Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources,"We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",2023,"Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous SourcesWe present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",grounding large language model via dynamic knowledge adapting heterogeneous sourceswe present cok novel framework augments large language model llm dynamically incorporating grounding information heterogeneous source result factual rationale reduced hallucination generation specifically cok consists three stage reasoning preparation dynamic knowledge adapting answer consolidation given question cok first prepares several preliminary rationale answer identifying relevant knowledge domain majority consensus among answer sample cok corrects rationale step step adapting knowledge identified domain corrected rationale plausibly serve better foundation final answer consolidation unlike prior study primarily use unstructured data cok also leverage structured knowledge source wikidata table provide reliable factual information access unstructured structured knowledge source dynamic knowledge adapting stage propose adaptive query generator allows generation query various type query language including sparql sql natural sentence moreover minimize error propagation rationale cok corrects rationale progressively using preceding corrected rationale generate correct subsequent rationale extensive experiment show cok consistently improves performance llm task across different domain,7
72aaac76776bb678634422a45862d2c224215b18,An Embarrassingly Simple Model for Dialogue Relation Extraction,"Dialogue relation extraction (RE) is to predict the relation type of two entities mentioned in a dialogue. In this paper, we propose a simple yet effective model named SimpleRE for the RE task. SimpleRE captures the interrelations among multiple relations in a dialogue through a novel input format named BERT Relation Token Sequence (BRS). In BRS, multiple [CLS] tokens are used to capture possible relations between different pairs of entities mentioned in the dialogue. A Relation Refinement Gate (RRG) is then designed to extract relation-specific semantic representation in an adaptive manner. Experiments on the DialogRE dataset show that SimpleRE achieves the best performance, with much shorter training time. Further, SimpleRE outperforms all direct baselines on sentence-level RE without using external resources.",2020,"An Embarrassingly Simple Model for Dialogue Relation ExtractionDialogue relation extraction (RE) is to predict the relation type of two entities mentioned in a dialogue. In this paper, we propose a simple yet effective model named SimpleRE for the RE task. SimpleRE captures the interrelations among multiple relations in a dialogue through a novel input format named BERT Relation Token Sequence (BRS). In BRS, multiple [CLS] tokens are used to capture possible relations between different pairs of entities mentioned in the dialogue. A Relation Refinement Gate (RRG) is then designed to extract relation-specific semantic representation in an adaptive manner. Experiments on the DialogRE dataset show that SimpleRE achieves the best performance, with much shorter training time. Further, SimpleRE outperforms all direct baselines on sentence-level RE without using external resources.",embarrassingly simple model dialogue relation extractiondialogue relation extraction predict relation type two entity mentioned dialogue paper propose simple yet effective model named simplere task simplere capture interrelation among multiple relation dialogue novel input format named bert relation token sequence br br multiple cl token used capture possible relation different pair entity mentioned dialogue relation refinement gate rrg designed extract semantic representation adaptive manner experiment dialogre dataset show simplere achieves best performance much shorter training time simplere outperforms direct baseline without using external resource,7
edccb296307f1ea187c403072159fc00b96cb888,λ-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces,"The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decision-Aware Actor-Critic framework ($\lambda$-AC) for decision-aware model-based reinforcement learning in continuous state-spaces and highlight important design choices in different environments.",2023,"λ-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spacesThe idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decision-Aware Actor-Critic framework ($\lambda$-AC) for decision-aware model-based reinforcement learning in continuous state-spaces and highlight important design choices in different environments.",learning latent model reinforcement learning continuous idea model learning model accurate matter ha gained prominence reinforcement learning promising theoretical result established empirical performance algorithm leveraging loss ha lacking especially continuous control problem paper present study necessary component reinforcement learning model showcase design choice enable algorithm end provide theoretical empirical investigation prominent algorithmic idea field highlight empirical design decision established muzero line work vital achieving good performance related algorithm showcase difference behavior different instantiation algorithm stochastic environment using insight propose latent framework reinforcement learning continuous highlight important design choice different environment,8
ba3e9e45f91257c9af6dfe74ed91a03c53086f07,Parallel Sampling of Diffusion Models,"Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we improve sampling speed by 2-4x across a range of robotics and image generation models, giving state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable degradation of task reward, FID score, or CLIP score.",2023,"Parallel Sampling of Diffusion ModelsDiffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we improve sampling speed by 2-4x across a range of robotics and image generation models, giving state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable degradation of task reward, FID score, or CLIP score.",parallel sampling diffusion modelsdiffusion model powerful generative model suffer slow sampling often taking sequential denoising step one sample result considerable effort directed toward reducing number denoising step method hurt sample quality instead reducing number denoising step trading quality speed paper explore orthogonal approach run denoising step parallel trading compute speed spite sequential nature denoising step show surprisingly possible parallelize sampling via picard iteration guessing solution future denoising step iteratively refining convergence insight present paradigm novel method accelerate sampling pretrained diffusion model denoising multiple step parallel paradigm first diffusion sampling method enables trading compute speed even compatible existing fast sampling technique ddim dpmsolver using paradigm improve sampling speed across range robotics image generation model giving sampling speed diffusionpolicy measurable degradation task reward fid score clip score,3
4c5b4a8e31d3119c1e3b5753693ff283c9717218,DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation,"We propose DISC-MedLLM, a comprehensive solution that leverages Large Language Models (LLMs) to provide accurate and truthful medical response in end-to-end conversational healthcare services. To construct high-quality Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing medical knowledge-graphs, reconstructing real-world dialogues, and incorporating human-guided preference rephrasing. These datasets are instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both single-turn and multi-turn consultation scenarios. Extensive experimental results demonstrate the effectiveness of the proposed model in bridging the gap between general language models and real-world medical consultation. Additionally, we release the constructed dataset and model weights to further contribute to research and development. Further details and resources can be found at https://github.com/FudanDISC/DISC-MedLLM",2023,"DISC-MedLLM: Bridging General Large Language Models and Real-World Medical ConsultationWe propose DISC-MedLLM, a comprehensive solution that leverages Large Language Models (LLMs) to provide accurate and truthful medical response in end-to-end conversational healthcare services. To construct high-quality Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing medical knowledge-graphs, reconstructing real-world dialogues, and incorporating human-guided preference rephrasing. These datasets are instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both single-turn and multi-turn consultation scenarios. Extensive experimental results demonstrate the effectiveness of the proposed model in bridging the gap between general language models and real-world medical consultation. Additionally, we release the constructed dataset and model weights to further contribute to research and development. Further details and resources can be found at https://github.com/FudanDISC/DISC-MedLLM",bridging general large language model medical consultationwe propose comprehensive solution leverage large language model llm provide accurate truthful medical response conversational healthcare service construct supervised sft datasets employ three strategy utilizing medical reconstructing dialogue incorporating preference rephrasing datasets instrumental training surpassing existing medical llm consultation scenario extensive experimental result demonstrate effectiveness proposed model bridging gap general language model medical consultation additionally release constructed dataset model weight contribute research development detail resource found http,7
7280df1bbbcaf743cb793d5bd654b4cf12b575f4,A comparative analysis of gradient boosting algorithms,,2019,A comparative analysis of gradient boosting algorithms,comparative analysis gradient boosting algorithm,4
22d34b881d64523da54f13d01fc3c6d93a8412e3,Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence,"AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D&D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict the state of the game given the dialogue history. We create a gameplay dataset consisting of nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns, 500,000 dice rolls, and 58 million words. We automatically annotate the data with partial state information about the game play. We train a large language model (LM) to generate the next game turn, conditioning it on different information. The LM can respond as a particular character or as the player who runs the game—i.e., the Dungeon Master (DM). It is trained to produce dialogue that is either in-character (roleplaying in the fictional world) or out-of-character (discussing rules or strategy). We perform a human evaluation to determine what factors make the generated output plausible and interesting. We further perform an automatic evaluation to determine how well the model can predict the game state given the history and examine how well tracking the game state improves its ability to produce plausible conversational output.",2022,"Dungeons and Dragons as a Dialog Challenge for Artificial IntelligenceAI researchers have posited Dungeons and Dragons (D&D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D&D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict the state of the game given the dialogue history. We create a gameplay dataset consisting of nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns, 500,000 dice rolls, and 58 million words. We automatically annotate the data with partial state information about the game play. We train a large language model (LM) to generate the next game turn, conditioning it on different information. The LM can respond as a particular character or as the player who runs the game—i.e., the Dungeon Master (DM). It is trained to produce dialogue that is either in-character (roleplaying in the fictional world) or out-of-character (discussing rules or strategy). We perform a human evaluation to determine what factors make the generated output plausible and interesting. We further perform an automatic evaluation to determine how well the model can predict the game state given the history and examine how well tracking the game state improves its ability to produce plausible conversational output.",dungeon dragon dialog challenge artificial intelligenceai researcher posited dungeon dragon challenge problem test system various capability paper frame specifically dialogue system challenge task generate next conversational turn game predict state game given dialogue history create gameplay dataset consisting nearly game total player dialogue turn dice roll million word automatically annotate data partial state information game play train large language model lm generate next game turn conditioning different information lm respond particular character player run dungeon master dm trained produce dialogue either roleplaying fictional world discussing rule strategy perform human evaluation determine factor make generated output plausible interesting perform automatic evaluation determine well model predict game state given history examine well tracking game state improves ability produce plausible conversational output,7
8f6c68009799b56ef3a1dc014ce59a58c471d97f,Adaptive Window Pruning for Efficient Local Motion Deblurring,"Local motion blur commonly occurs in real-world photography due to the mixing between moving objects and stationary backgrounds during exposure. Existing image deblurring methods predominantly focus on global deblurring, inadvertently affecting the sharpness of backgrounds in locally blurred images and wasting unnecessary computation on sharp pixels, especially for high-resolution images. This paper aims to adaptively and efficiently restore high-resolution locally blurred images. We propose a local motion deblurring vision Transformer (LMD-ViT) built on adaptive window pruning Transformer blocks (AdaWPT). To focus deblurring on local regions and reduce computation, AdaWPT prunes unnecessary windows, only allowing the active windows to be involved in the deblurring processes. The pruning operation relies on the blurriness confidence predicted by a confidence predictor that is trained end-to-end using a reconstruction loss with Gumbel-Softmax re-parameterization and a pruning loss guided by annotated blur masks. Our method removes local motion blur effectively without distorting sharp regions, demonstrated by its exceptional perceptual and quantitative improvements compared to state-of-the-art methods. In addition, our approach substantially reduces FLOPs by 66% and achieves more than a twofold increase in inference speed compared to Transformer-based deblurring methods. We will make our code and annotated blur masks publicly available.",2023,"Adaptive Window Pruning for Efficient Local Motion DeblurringLocal motion blur commonly occurs in real-world photography due to the mixing between moving objects and stationary backgrounds during exposure. Existing image deblurring methods predominantly focus on global deblurring, inadvertently affecting the sharpness of backgrounds in locally blurred images and wasting unnecessary computation on sharp pixels, especially for high-resolution images. This paper aims to adaptively and efficiently restore high-resolution locally blurred images. We propose a local motion deblurring vision Transformer (LMD-ViT) built on adaptive window pruning Transformer blocks (AdaWPT). To focus deblurring on local regions and reduce computation, AdaWPT prunes unnecessary windows, only allowing the active windows to be involved in the deblurring processes. The pruning operation relies on the blurriness confidence predicted by a confidence predictor that is trained end-to-end using a reconstruction loss with Gumbel-Softmax re-parameterization and a pruning loss guided by annotated blur masks. Our method removes local motion blur effectively without distorting sharp regions, demonstrated by its exceptional perceptual and quantitative improvements compared to state-of-the-art methods. In addition, our approach substantially reduces FLOPs by 66% and achieves more than a twofold increase in inference speed compared to Transformer-based deblurring methods. We will make our code and annotated blur masks publicly available.",adaptive window pruning efficient local motion deblurringlocal motion blur commonly occurs photography due mixing moving object stationary background exposure existing image deblurring method predominantly focus global deblurring inadvertently affecting sharpness background locally blurred image wasting unnecessary computation sharp pixel especially image paper aim adaptively efficiently restore locally blurred image propose local motion deblurring vision transformer built adaptive window pruning transformer block adawpt focus deblurring local region reduce computation adawpt prune unnecessary window allowing active window involved deblurring process pruning operation relies blurriness confidence predicted confidence predictor trained using reconstruction loss pruning loss guided annotated blur mask method remove local motion blur effectively without distorting sharp region demonstrated exceptional perceptual quantitative improvement compared method addition approach substantially reduces flop achieves twofold increase inference speed compared deblurring method make code annotated blur mask publicly available,1
09a85806442373f167e45eaf662a7914df048b10,Neural Machine Translation Models Can Learn to be Few-shot Learners,"The emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant few-shot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.",2023,"Neural Machine Translation Models Can Learn to be Few-shot LearnersThe emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant few-shot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.",neural machine translation model learn learnersthe emergent ability large language model use small number example learn perform novel domain task also called learning icl work show much smaller model trained perform icl towards specialized training objective exemplified task domain adaptation neural machine translation capacity icl model take advantage relevant example adapt output towards domain compare quality domain adaptation traditional supervised technique icl large language model approach allows efficient batch inference mix domain outperforms baseline term translation quality immediate adaptation rate ability reproduce specific term shown single example,7
7fff5c07e630c756f11517f81662cd272b6048f1,Position Paper on Dataset Engineering to Accelerate Science,"Data is a critical element in any discovery process. In the last decades, we observed exponential growth in the volume of available data and the technology to manipulate it. However, data is only practical when one can structure it for a well-defined task. For instance, we need a corpus of text broken into sentences to train a natural language machine-learning model. In this work, we will use the token \textit{dataset} to designate a structured set of data built to perform a well-defined task. Moreover, the dataset will be used in most cases as a blueprint of an entity that at any moment can be stored as a table. Specifically, in science, each area has unique forms to organize, gather and handle its datasets. We believe that datasets must be a first-class entity in any knowledge-intensive process, and all workflows should have exceptional attention to datasets' lifecycle, from their gathering to uses and evolution. We advocate that science and engineering discovery processes are extreme instances of the need for such organization on datasets, claiming for new approaches and tooling. Furthermore, these requirements are more evident when the discovery workflow uses artificial intelligence methods to empower the subject-matter expert. In this work, we discuss an approach to bringing datasets as a critical entity in the discovery process in science. We illustrate some concepts using material discovery as a use case. We chose this domain because it leverages many significant problems that can be generalized to other science fields.",2023,"Position Paper on Dataset Engineering to Accelerate ScienceData is a critical element in any discovery process. In the last decades, we observed exponential growth in the volume of available data and the technology to manipulate it. However, data is only practical when one can structure it for a well-defined task. For instance, we need a corpus of text broken into sentences to train a natural language machine-learning model. In this work, we will use the token \textit{dataset} to designate a structured set of data built to perform a well-defined task. Moreover, the dataset will be used in most cases as a blueprint of an entity that at any moment can be stored as a table. Specifically, in science, each area has unique forms to organize, gather and handle its datasets. We believe that datasets must be a first-class entity in any knowledge-intensive process, and all workflows should have exceptional attention to datasets' lifecycle, from their gathering to uses and evolution. We advocate that science and engineering discovery processes are extreme instances of the need for such organization on datasets, claiming for new approaches and tooling. Furthermore, these requirements are more evident when the discovery workflow uses artificial intelligence methods to empower the subject-matter expert. In this work, we discuss an approach to bringing datasets as a critical entity in the discovery process in science. We illustrate some concepts using material discovery as a use case. We chose this domain because it leverages many significant problems that can be generalized to other science fields.",position paper dataset engineering accelerate sciencedata critical element discovery process last decade observed exponential growth volume available data technology manipulate however data practical one structure task instance need corpus text broken sentence train natural language model work use token dataset designate structured set data built perform task moreover dataset used case blueprint entity moment stored table specifically science area ha unique form organize gather handle datasets believe datasets must entity process workflow exceptional attention datasets lifecycle gathering us evolution advocate science engineering discovery process extreme instance need organization datasets claiming new approach tooling furthermore requirement evident discovery workflow us artificial intelligence method empower expert work discus approach bringing datasets critical entity discovery process science illustrate concept using material discovery use case chose domain leverage many significant problem generalized science field,9
ac074b68a1ed8b50f5c242c66d6d4a3c40aef165,Fuzzy Expert Systems for Prediction of ICU Admission in Patients with COVID-19,"The pandemic COVID-19 disease has had a dramatic impact on almost all countries around the world so that many hospitals have been overwhelmed with Covid-19 cases. As medical resources are limited, deciding on the proper allocation of these resources is a very crucial issue. Besides, uncertainty is a major factor that can affect decisions, especially in medical fields. To cope with this issue, we use fuzzy logic (FL) as one of the most suitable methods in modeling systems with high uncertainty and complexity. We intend to make use of the advantages of FL in decisions on cases that need to treat in ICU. In this study, an interval type-2 fuzzy expert system is proposed for prediction of ICU admission in COVID-19 patients. For this prediction task, we also developed an adaptive neuro-fuzzy inference system (ANFIS). Finally, the results of these fuzzy systems are compared to some well-known classification methods such as Naive Bayes (NB), Case-Based Reasoning (CBR), Decision Tree (DT), and K Nearest Neighbor (KNN). The results show that the type-2 fuzzy expert system and ANFIS models perform competitively in terms of accuracy and F-measure compared to the other system modeling techniques.",2021,"Fuzzy Expert Systems for Prediction of ICU Admission in Patients with COVID-19The pandemic COVID-19 disease has had a dramatic impact on almost all countries around the world so that many hospitals have been overwhelmed with Covid-19 cases. As medical resources are limited, deciding on the proper allocation of these resources is a very crucial issue. Besides, uncertainty is a major factor that can affect decisions, especially in medical fields. To cope with this issue, we use fuzzy logic (FL) as one of the most suitable methods in modeling systems with high uncertainty and complexity. We intend to make use of the advantages of FL in decisions on cases that need to treat in ICU. In this study, an interval type-2 fuzzy expert system is proposed for prediction of ICU admission in COVID-19 patients. For this prediction task, we also developed an adaptive neuro-fuzzy inference system (ANFIS). Finally, the results of these fuzzy systems are compared to some well-known classification methods such as Naive Bayes (NB), Case-Based Reasoning (CBR), Decision Tree (DT), and K Nearest Neighbor (KNN). The results show that the type-2 fuzzy expert system and ANFIS models perform competitively in terms of accuracy and F-measure compared to the other system modeling techniques.",fuzzy expert system prediction icu admission patient pandemic disease ha dramatic impact almost country around world many hospital overwhelmed case medical resource limited deciding proper allocation resource crucial issue besides uncertainty major factor affect decision especially medical field cope issue use fuzzy logic fl one suitable method modeling system high uncertainty complexity intend make use advantage fl decision case need treat icu study interval fuzzy expert system proposed prediction icu admission patient prediction task also developed adaptive inference system anfis finally result fuzzy system compared classification method naive bayes nb reasoning cbr decision tree dt k nearest neighbor knn result show fuzzy expert system anfis model perform competitively term accuracy compared system modeling technique,4
30ea884a5b8e4cffe18ebfef6c033e5c37b79eeb,World Models for Math Story Problems,"Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several existing datasets and annotate a corpus of 1,019 problems and 3,204 logical forms with MathWorld. Using this data, we demonstrate the following use cases of MathWorld: (1) prompting language models with synthetically generated question-answer pairs to probe their reasoning and world modeling abilities, and (2) generating new problems by using the world models as a design space.",2023,"World Models for Math Story ProblemsSolving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several existing datasets and annotate a corpus of 1,019 problems and 3,204 logical forms with MathWorld. Using this data, we demonstrate the following use cases of MathWorld: (1) prompting language models with synthetically generated question-answer pairs to probe their reasoning and world modeling abilities, and (2) generating new problems by using the world models as a design space.",world model math story problemssolving math story problem complex task student nlp model alike requiring understand world described story reason compute answer recent year seen impressive performance automatically solving problem large language model innovative technique prompt however remains unclear model posse accurate representation mathematical concept lead lack interpretability trustworthiness impedes usefulness various application paper consolidate previous work categorizing representing math story problem develop mathworld semantic formalism specific domain math story problem mathworld assign world model math story problem represent situation action introduced text mathematical relationship combine math story problem several existing datasets annotate corpus problem logical form mathworld using data demonstrate following use case mathworld prompting language model synthetically generated pair probe reasoning world modeling ability generating new problem using world model design space,7
e36396c0dd9f386d6b8a329b151e8826a138c8e3,Hierarchical Side-Tuning for Vision Transformers,"Fine-tuning pre-trained Vision Transformers (ViT) has consistently demonstrated promising performance in the realm of visual recognition. However, adapting large pre-trained models to various tasks poses a significant challenge. This challenge arises from the need for each model to undergo an independent and comprehensive fine-tuning process, leading to substantial computational and memory demands. While recent advancements in Parameter-efficient Transfer Learning (PETL) have demonstrated their ability to achieve superior performance compared to full fine-tuning with a smaller subset of parameter updates, they tend to overlook dense prediction tasks such as object detection and segmentation. In this paper, we introduce Hierarchical Side-Tuning (HST), a novel PETL approach that enables ViT transfer to various downstream tasks effectively. Diverging from existing methods that exclusively fine-tune parameters within input spaces or certain modules connected to the backbone, we tune a lightweight and hierarchical side network (HSN) that leverages intermediate activations extracted from the backbone and generates multi-scale features to make predictions. To validate HST, we conducted extensive experiments encompassing diverse visual tasks, including classification, object detection, instance segmentation, and semantic segmentation. Notably, our method achieves state-of-the-art average Top-1 accuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters. When applied to object detection tasks on COCO testdev benchmark, HST even surpasses full fine-tuning and obtains better performance with 49.7 box AP and 43.2 mask AP using Cascade Mask R-CNN.",2023,"Hierarchical Side-Tuning for Vision TransformersFine-tuning pre-trained Vision Transformers (ViT) has consistently demonstrated promising performance in the realm of visual recognition. However, adapting large pre-trained models to various tasks poses a significant challenge. This challenge arises from the need for each model to undergo an independent and comprehensive fine-tuning process, leading to substantial computational and memory demands. While recent advancements in Parameter-efficient Transfer Learning (PETL) have demonstrated their ability to achieve superior performance compared to full fine-tuning with a smaller subset of parameter updates, they tend to overlook dense prediction tasks such as object detection and segmentation. In this paper, we introduce Hierarchical Side-Tuning (HST), a novel PETL approach that enables ViT transfer to various downstream tasks effectively. Diverging from existing methods that exclusively fine-tune parameters within input spaces or certain modules connected to the backbone, we tune a lightweight and hierarchical side network (HSN) that leverages intermediate activations extracted from the backbone and generates multi-scale features to make predictions. To validate HST, we conducted extensive experiments encompassing diverse visual tasks, including classification, object detection, instance segmentation, and semantic segmentation. Notably, our method achieves state-of-the-art average Top-1 accuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters. When applied to object detection tasks on COCO testdev benchmark, HST even surpasses full fine-tuning and obtains better performance with 49.7 box AP and 43.2 mask AP using Cascade Mask R-CNN.",hierarchical vision vision transformer vit ha consistently demonstrated promising performance realm visual recognition however adapting large model various task pose significant challenge challenge arises need model undergo independent comprehensive process leading substantial computational memory demand recent advancement transfer learning petl demonstrated ability achieve superior performance compared full smaller subset parameter update tend overlook dense prediction task object detection segmentation paper introduce hierarchical hst novel petl approach enables vit transfer various downstream task effectively diverging existing method exclusively parameter within input space certain module connected backbone tune lightweight hierarchical side network hsn leverage intermediate activation extracted backbone generates feature make prediction validate hst conducted extensive experiment encompassing diverse visual task including classification object detection instance segmentation semantic segmentation notably method achieves average accuracy mere parameter applied object detection task coco testdev benchmark hst even surpasses full obtains better performance box ap mask ap using cascade mask,6
bf415e4575f3b51ac6cd97b6df81bcbeadb2e422,DeepTriNet: A Tri-Level Attention Based DeepLabv3+ Architecture for Semantic Segmentation of Satellite Images,"The segmentation of satellite images is crucial in remote sensing applications. Existing methods face challenges in recognizing small-scale objects in satellite images for semantic segmentation primarily due to ignoring the low-level characteristics of the underlying network and due to containing distinct amounts of information by different feature maps. Thus, in this research, a tri-level attention-based DeepLabv3+ architecture (DeepTriNet) is proposed for the semantic segmentation of satellite images. The proposed hybrid method combines squeeze-and-excitation networks (SENets) and tri-level attention units (TAUs) with the vanilla DeepLabv3+ architecture, where the TAUs are used to bridge the semantic feature gap among encoders output and the SENets used to put more weight on relevant features. The proposed DeepTriNet finds which features are the more relevant and more generalized way by its self-supervision rather we annotate them. The study showed that the proposed DeepTriNet performs better than many conventional techniques with an accuracy of 98% and 77%, IoU 80% and 58%, precision 88% and 68%, and recall of 79% and 55% on the 4-class Land-Cover.ai dataset and the 15-class GID-2 dataset respectively. The proposed method will greatly contribute to natural resource management and change detection in rural and urban regions through efficient and semantic satellite image segmentation",2023,"DeepTriNet: A Tri-Level Attention Based DeepLabv3+ Architecture for Semantic Segmentation of Satellite ImagesThe segmentation of satellite images is crucial in remote sensing applications. Existing methods face challenges in recognizing small-scale objects in satellite images for semantic segmentation primarily due to ignoring the low-level characteristics of the underlying network and due to containing distinct amounts of information by different feature maps. Thus, in this research, a tri-level attention-based DeepLabv3+ architecture (DeepTriNet) is proposed for the semantic segmentation of satellite images. The proposed hybrid method combines squeeze-and-excitation networks (SENets) and tri-level attention units (TAUs) with the vanilla DeepLabv3+ architecture, where the TAUs are used to bridge the semantic feature gap among encoders output and the SENets used to put more weight on relevant features. The proposed DeepTriNet finds which features are the more relevant and more generalized way by its self-supervision rather we annotate them. The study showed that the proposed DeepTriNet performs better than many conventional techniques with an accuracy of 98% and 77%, IoU 80% and 58%, precision 88% and 68%, and recall of 79% and 55% on the 4-class Land-Cover.ai dataset and the 15-class GID-2 dataset respectively. The proposed method will greatly contribute to natural resource management and change detection in rural and urban regions through efficient and semantic satellite image segmentation",deeptrinet attention based architecture semantic segmentation satellite imagesthe segmentation satellite image crucial remote sensing application existing method face challenge recognizing object satellite image semantic segmentation primarily due ignoring characteristic underlying network due containing distinct amount information different feature map thus research architecture deeptrinet proposed semantic segmentation satellite image proposed hybrid method combine network senets attention unit tau vanilla architecture tau used bridge semantic feature gap among encoders output senets used put weight relevant feature proposed deeptrinet find feature relevant generalized way rather annotate study showed proposed deeptrinet performs better many conventional technique accuracy iou precision recall dataset dataset respectively proposed method greatly contribute natural resource management change detection rural urban region efficient semantic satellite image segmentation,1
6a3eb598ffed0ba91c2c912f6050abc153a4b2b3,Graph Mixer Networks,"In recent years, the attention mechanism has demonstrated superior performance in various tasks, leading to the emergence of GAT and Graph Transformer models that utilize this mechanism to extract relational information from graph-structured data. However, the high computational cost associated with the Transformer block, as seen in Vision Transformers, has motivated the development of alternative architectures such as MLP-Mixers, which have been shown to improve performance in image tasks while reducing the computational cost. Despite the effectiveness of Transformers in graph-based tasks, their computational efficiency remains a concern. The logic behind MLP-Mixers, which addresses this issue in image tasks, has the potential to be applied to graph-structured data as well. In this paper, we propose the Graph Mixer Network (GMN), also referred to as Graph Nasreddin Nets (GNasNets), a framework that incorporates the principles of MLP-Mixers for graph-structured data. Using a PNA model with multiple aggregators as the foundation, our proposed GMN has demonstrated improved performance compared to Graph Transformers. The source code is available publicly at https://github.com/asarigun/GraphMixerNetworks.",2023,"Graph Mixer NetworksIn recent years, the attention mechanism has demonstrated superior performance in various tasks, leading to the emergence of GAT and Graph Transformer models that utilize this mechanism to extract relational information from graph-structured data. However, the high computational cost associated with the Transformer block, as seen in Vision Transformers, has motivated the development of alternative architectures such as MLP-Mixers, which have been shown to improve performance in image tasks while reducing the computational cost. Despite the effectiveness of Transformers in graph-based tasks, their computational efficiency remains a concern. The logic behind MLP-Mixers, which addresses this issue in image tasks, has the potential to be applied to graph-structured data as well. In this paper, we propose the Graph Mixer Network (GMN), also referred to as Graph Nasreddin Nets (GNasNets), a framework that incorporates the principles of MLP-Mixers for graph-structured data. Using a PNA model with multiple aggregators as the foundation, our proposed GMN has demonstrated improved performance compared to Graph Transformers. The source code is available publicly at https://github.com/asarigun/GraphMixerNetworks.",graph mixer networksin recent year attention mechanism ha demonstrated superior performance various task leading emergence gat graph transformer model utilize mechanism extract relational information data however high computational cost associated transformer block seen vision transformer ha motivated development alternative architecture shown improve performance image task reducing computational cost despite effectiveness transformer task computational efficiency remains concern logic behind address issue image task ha potential applied data well paper propose graph mixer network gmn also referred graph nasreddin net gnasnets framework incorporates principle data using pna model multiple aggregator foundation proposed gmn ha demonstrated improved performance compared graph transformer source code available publicly http,6
17a48ebfef2ed820f3529f11b9a5acf48a9a0fe5,A new perspective on building efficient and expressive 3D equivariant graph neural networks,"Geometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these networks through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (LSE) and frame transition encoding (FTE). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out the design space for future developments of equivariant graph neural networks. Our codes are available at \url{https://github.com/yuanqidu/LeftNet}.",2023,"A new perspective on building efficient and expressive 3D equivariant graph neural networksGeometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these networks through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (LSE) and frame transition encoding (FTE). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out the design space for future developments of equivariant graph neural networks. Our codes are available at \url{https://github.com/yuanqidu/LeftNet}.",new perspective building efficient expressive equivariant graph neural networksgeometric deep learning enables encoding physical symmetry modeling object despite rapid progress encoding symmetry graph neural network gnns comprehensive evaluation expressiveness network analysis lack today paper propose local hierarchy isomorphism evaluate expressive power equivariant gnns investigate process representing global geometric information local patch work lead two crucial module designing expressive efficient geometric gnns namely local substructure encoding lse frame transition encoding fte demonstrate applicability theory propose leftnet effectively implement module achieves performance molecular property prediction task point design space future development equivariant graph neural network code available http,5
c88d5b5671d86e4e6b641916ba2faccc0dd2c1a9,Layered State Discovery for Incremental Autonomous Exploration,"We study the autonomous exploration (AX) problem proposed by Lim&Auer (2012). In this setting, the objective is to discover a set of $\epsilon$-optimal policies reaching a set $\mathcal{S}_L^{\rightarrow}$ of incrementally $L$-controllable states. We introduce a novel layered decomposition of the set of incrementally $L$-controllable states that is based on the iterative application of a state-expansion operator. We leverage these results to design Layered Autonomous Exploration (LAE), a novel algorithm for AX that attains a sample complexity of $\tilde{\mathcal{O}}(LS^{\rightarrow}_{L(1+\epsilon)}\Gamma_{L(1+\epsilon)} A \ln^{12}(S^{\rightarrow}_{L(1+\epsilon)})/\epsilon^2)$, where $S^{\rightarrow}_{L(1+\epsilon)}$ is the number of states that are incrementally $L(1+\epsilon)$-controllable, $A$ is the number of actions, and $\Gamma_{L(1+\epsilon)}$ is the branching factor of the transitions over such states. LAE improves over the algorithm of Tarbouriech et al. (2020a) by a factor of $L^2$ and it is the first algorithm for AX that works in a countably-infinite state space. Moreover, we show that, under a certain identifiability assumption, LAE achieves minimax-optimal sample complexity of $\tilde{\mathcal{O}}(LS^{\rightarrow}_{L}A\ln^{12}(S^{\rightarrow}_{L})/\epsilon^2)$, outperforming existing algorithms and matching for the first time the lower bound proved by Cai et al. (2022) up to logarithmic factors.",2023,"Layered State Discovery for Incremental Autonomous ExplorationWe study the autonomous exploration (AX) problem proposed by Lim&Auer (2012). In this setting, the objective is to discover a set of $\epsilon$-optimal policies reaching a set $\mathcal{S}_L^{\rightarrow}$ of incrementally $L$-controllable states. We introduce a novel layered decomposition of the set of incrementally $L$-controllable states that is based on the iterative application of a state-expansion operator. We leverage these results to design Layered Autonomous Exploration (LAE), a novel algorithm for AX that attains a sample complexity of $\tilde{\mathcal{O}}(LS^{\rightarrow}_{L(1+\epsilon)}\Gamma_{L(1+\epsilon)} A \ln^{12}(S^{\rightarrow}_{L(1+\epsilon)})/\epsilon^2)$, where $S^{\rightarrow}_{L(1+\epsilon)}$ is the number of states that are incrementally $L(1+\epsilon)$-controllable, $A$ is the number of actions, and $\Gamma_{L(1+\epsilon)}$ is the branching factor of the transitions over such states. LAE improves over the algorithm of Tarbouriech et al. (2020a) by a factor of $L^2$ and it is the first algorithm for AX that works in a countably-infinite state space. Moreover, we show that, under a certain identifiability assumption, LAE achieves minimax-optimal sample complexity of $\tilde{\mathcal{O}}(LS^{\rightarrow}_{L}A\ln^{12}(S^{\rightarrow}_{L})/\epsilon^2)$, outperforming existing algorithms and matching for the first time the lower bound proved by Cai et al. (2022) up to logarithmic factors.",layered state discovery incremental autonomous explorationwe study autonomous exploration ax problem proposed lim auer setting objective discover set policy reaching set incrementally l state introduce novel layered decomposition set incrementally l state based iterative application operator leverage result design layered autonomous exploration lae novel algorithm ax attains sample complexity l l l l number state incrementally l number action l branching factor transition state lae improves algorithm tarbouriech et al factor first algorithm ax work state space moreover show certain identifiability assumption lae achieves sample complexity l l outperforming existing algorithm matching first time lower bound proved cai et al logarithmic factor,8
5134eb2119463f9e0f0eb289f7ce61bc1bfe58f0,Root Pose Decomposition Towards Generic Non-rigid 3D Reconstruction with Monocular Videos,"This work focuses on the 3D reconstruction of non-rigid objects based on monocular RGB video sequences. Concretely, we aim at building high-fidelity models for generic object categories and casually captured scenes. To this end, we do not assume known root poses of objects, and do not utilize category-specific templates or dense pose priors. The key idea of our method, Root Pose Decomposition (RPD), is to maintain a per-frame root pose transformation, meanwhile building a dense field with local transformations to rectify the root pose. The optimization of local transformations is performed by point registration to the canonical space. We also adapt RPD to multi-object scenarios with object occlusions and individual differences. As a result, RPD allows non-rigid 3D reconstruction for complicated scenarios containing objects with large deformations, complex motion patterns, occlusions, and scale diversities of different individuals. Such a pipeline potentially scales to diverse sets of objects in the wild. We experimentally show that RPD surpasses state-of-the-art methods on the challenging DAVIS, OVIS, and AMA datasets.",2023,"Root Pose Decomposition Towards Generic Non-rigid 3D Reconstruction with Monocular VideosThis work focuses on the 3D reconstruction of non-rigid objects based on monocular RGB video sequences. Concretely, we aim at building high-fidelity models for generic object categories and casually captured scenes. To this end, we do not assume known root poses of objects, and do not utilize category-specific templates or dense pose priors. The key idea of our method, Root Pose Decomposition (RPD), is to maintain a per-frame root pose transformation, meanwhile building a dense field with local transformations to rectify the root pose. The optimization of local transformations is performed by point registration to the canonical space. We also adapt RPD to multi-object scenarios with object occlusions and individual differences. As a result, RPD allows non-rigid 3D reconstruction for complicated scenarios containing objects with large deformations, complex motion patterns, occlusions, and scale diversities of different individuals. Such a pipeline potentially scales to diverse sets of objects in the wild. We experimentally show that RPD surpasses state-of-the-art methods on the challenging DAVIS, OVIS, and AMA datasets.",root pose decomposition towards generic reconstruction monocular videosthis work focus reconstruction object based monocular rgb video sequence concretely aim building model generic object category casually captured scene end assume known root pose object utilize template dense pose prior key idea method root pose decomposition rpd maintain root pose transformation meanwhile building dense field local transformation rectify root pose optimization local transformation performed point registration canonical space also adapt rpd scenario object occlusion individual difference result rpd allows reconstruction complicated scenario containing object large deformation complex motion pattern occlusion scale diversity different individual pipeline potentially scale diverse set object wild experimentally show rpd surpasses method challenging davis ovis ama datasets,1
bd5a854fe7f97a373a15ebf31264bb795adae877,Uncertainty and Structure in Neural Ordinary Differential Equations,"Neural ordinary differential equations (ODEs) are an emerging class of deep learning models for dynamical systems. They are particularly useful for learning an ODE vector field from observed trajectories (i.e., inverse problems). We here consider aspects of these models relevant for their application in science and engineering. Scientific predictions generally require structured uncertainty estimates. As a first contribution, we show that basic and lightweight Bayesian deep learning techniques like the Laplace approximation can be applied to neural ODEs to yield structured and meaningful uncertainty quantification. But, in the scientific domain, available information often goes beyond raw trajectories, and also includes mechanistic knowledge, e.g., in the form of conservation laws. We explore how mechanistic knowledge and uncertainty quantification interact on two recently proposed neural ODE frameworks - symplectic neural ODEs and physical models augmented with neural ODEs. In particular, uncertainty reflects the effect of mechanistic information more directly than the predictive power of the trained model could. And vice versa, structure can improve the extrapolation abilities of neural ODEs, a fact that can be best assessed in practice through uncertainty estimates. Our experimental analysis demonstrates the effectiveness of the Laplace approach on both low dimensional ODE problems and a high dimensional partial differential equation.",2023,"Uncertainty and Structure in Neural Ordinary Differential EquationsNeural ordinary differential equations (ODEs) are an emerging class of deep learning models for dynamical systems. They are particularly useful for learning an ODE vector field from observed trajectories (i.e., inverse problems). We here consider aspects of these models relevant for their application in science and engineering. Scientific predictions generally require structured uncertainty estimates. As a first contribution, we show that basic and lightweight Bayesian deep learning techniques like the Laplace approximation can be applied to neural ODEs to yield structured and meaningful uncertainty quantification. But, in the scientific domain, available information often goes beyond raw trajectories, and also includes mechanistic knowledge, e.g., in the form of conservation laws. We explore how mechanistic knowledge and uncertainty quantification interact on two recently proposed neural ODE frameworks - symplectic neural ODEs and physical models augmented with neural ODEs. In particular, uncertainty reflects the effect of mechanistic information more directly than the predictive power of the trained model could. And vice versa, structure can improve the extrapolation abilities of neural ODEs, a fact that can be best assessed in practice through uncertainty estimates. Our experimental analysis demonstrates the effectiveness of the Laplace approach on both low dimensional ODE problems and a high dimensional partial differential equation.",uncertainty structure neural ordinary differential equationsneural ordinary differential equation ode emerging class deep learning model dynamical system particularly useful learning ode vector field observed trajectory inverse problem consider aspect model relevant application science engineering scientific prediction generally require structured uncertainty estimate first contribution show basic lightweight bayesian deep learning technique like laplace approximation applied neural ode yield structured meaningful uncertainty quantification scientific domain available information often go beyond raw trajectory also includes mechanistic knowledge form conservation law explore mechanistic knowledge uncertainty quantification interact two recently proposed neural ode framework symplectic neural ode physical model augmented neural ode particular uncertainty reflects effect mechanistic information directly predictive power trained model could vice versa structure improve extrapolation ability neural ode fact best assessed practice uncertainty estimate experimental analysis demonstrates effectiveness laplace approach low dimensional ode problem high dimensional partial differential equation,8
dcdcfb7678502a9093b1c2510abc6708d02b48d6,RT-MonoDepth: Real-time Monocular Depth Estimation on Embedded Systems,"Depth sensing is a crucial function of unmanned aerial vehicles and autonomous vehicles. Due to the small size and simple structure of monocular cameras, there has been a growing interest in depth estimation from a single RGB image. However, state-of-the-art monocular CNN-based depth estimation methods using fairly complex deep neural networks are too slow for real-time inference on embedded platforms. This paper addresses the problem of real-time depth estimation on embedded systems. We propose two efficient and lightweight encoder-decoder network architectures, RT-MonoDepth and RT-MonoDepth-S, to reduce computational complexity and latency. Our methodologies demonstrate that it is possible to achieve similar accuracy as prior state-of-the-art works on depth estimation at a faster inference speed. Our proposed networks, RT-MonoDepth and RT-MonoDepth-S, runs at 18.4\&30.5 FPS on NVIDIA Jetson Nano and 253.0\&364.1 FPS on NVIDIA Jetson AGX Orin on a single RGB image of resolution 640$\times$192, and achieve relative state-of-the-art accuracy on the KITTI dataset. To the best of the authors' knowledge, this paper achieves the best accuracy and fastest inference speed compared with existing fast monocular depth estimation methods.",2023,"RT-MonoDepth: Real-time Monocular Depth Estimation on Embedded SystemsDepth sensing is a crucial function of unmanned aerial vehicles and autonomous vehicles. Due to the small size and simple structure of monocular cameras, there has been a growing interest in depth estimation from a single RGB image. However, state-of-the-art monocular CNN-based depth estimation methods using fairly complex deep neural networks are too slow for real-time inference on embedded platforms. This paper addresses the problem of real-time depth estimation on embedded systems. We propose two efficient and lightweight encoder-decoder network architectures, RT-MonoDepth and RT-MonoDepth-S, to reduce computational complexity and latency. Our methodologies demonstrate that it is possible to achieve similar accuracy as prior state-of-the-art works on depth estimation at a faster inference speed. Our proposed networks, RT-MonoDepth and RT-MonoDepth-S, runs at 18.4\&30.5 FPS on NVIDIA Jetson Nano and 253.0\&364.1 FPS on NVIDIA Jetson AGX Orin on a single RGB image of resolution 640$\times$192, and achieve relative state-of-the-art accuracy on the KITTI dataset. To the best of the authors' knowledge, this paper achieves the best accuracy and fastest inference speed compared with existing fast monocular depth estimation methods.",monocular depth estimation embedded systemsdepth sensing crucial function unmanned aerial vehicle autonomous vehicle due small size simple structure monocular camera ha growing interest depth estimation single rgb image however monocular depth estimation method using fairly complex deep neural network slow inference embedded platform paper address problem depth estimation embedded system propose two efficient lightweight network architecture reduce computational complexity latency methodology demonstrate possible achieve similar accuracy prior work depth estimation faster inference speed proposed network run fps nvidia jetson nano fps nvidia jetson agx orin single rgb image resolution achieve relative accuracy kitti dataset best author knowledge paper achieves best accuracy fastest inference speed compared existing fast monocular depth estimation method,1
5bac7d00035bc1e246a34f9ee3152b290f97bb92,Supervised Pretraining Can Learn In-Context Reinforcement Learning,"Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",2023,"Supervised Pretraining Can Learn In-Context Reinforcement LearningLarge transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",supervised pretraining learn reinforcement learninglarge transformer model trained diverse datasets shown remarkable ability learn achieving high performance task explicitly trained solve paper study learning capability transformer problem reinforcement learning rl bandit markov decision process introduce study transformer dpt supervised pretraining method transformer predicts optimal action given query state dataset interaction across diverse set task procedure simple produce model several surprising capability find pretrained transformer used solve range rl problem exhibiting exploration online conservatism offline despite explicitly trained model also generalizes beyond pretraining distribution new task automatically adapts strategy unknown structure theoretically show dpt viewed efficient implementation bayesian posterior sampling provably rl algorithm leverage connection provide guarantee regret algorithm yielded dpt prove learn faster algorithm used generate pretraining data result suggest promising yet simple path towards instilling strong ability transformer,8
d2149b7f99bdba7e1dcc7911a8c839f09c7767aa,Federated Learning of Models Pre-Trained on Different Features with Consensus Graphs,"Learning an effective global model on private and decentralized datasets has become an increasingly important challenge of machine learning when applied in practice. Existing distributed learning paradigms, such as Federated Learning, enable this via model aggregation which enforces a strong form of modeling homogeneity and synchronicity across clients. This is however not suitable to many practical scenarios. For example, in distributed sensing, heterogeneous sensors reading data from different views of the same phenomenon would need to use different models for different data modalities. Local learning therefore happens in isolation but inference requires merging the local models to achieve consensus. To enable consensus among local models, we propose a feature fusion approach that extracts local representations from local models and incorporates them into a global representation that improves the prediction performance. Achieving this requires addressing two non-trivial problems. First, we need to learn an alignment between similar feature components which are arbitrarily arranged across clients to enable representation aggregation. Second, we need to learn a consensus graph that captures the high-order interactions between local feature spaces and how to combine them to achieve a better prediction. This paper presents solutions to these problems and demonstrates them in real-world applications on time series data such as power grids and traffic networks.",2023,"Federated Learning of Models Pre-Trained on Different Features with Consensus GraphsLearning an effective global model on private and decentralized datasets has become an increasingly important challenge of machine learning when applied in practice. Existing distributed learning paradigms, such as Federated Learning, enable this via model aggregation which enforces a strong form of modeling homogeneity and synchronicity across clients. This is however not suitable to many practical scenarios. For example, in distributed sensing, heterogeneous sensors reading data from different views of the same phenomenon would need to use different models for different data modalities. Local learning therefore happens in isolation but inference requires merging the local models to achieve consensus. To enable consensus among local models, we propose a feature fusion approach that extracts local representations from local models and incorporates them into a global representation that improves the prediction performance. Achieving this requires addressing two non-trivial problems. First, we need to learn an alignment between similar feature components which are arbitrarily arranged across clients to enable representation aggregation. Second, we need to learn a consensus graph that captures the high-order interactions between local feature spaces and how to combine them to achieve a better prediction. This paper presents solutions to these problems and demonstrates them in real-world applications on time series data such as power grids and traffic networks.",federated learning model different feature consensus graphslearning effective global model private decentralized datasets ha become increasingly important challenge machine learning applied practice existing distributed learning paradigm federated learning enable via model aggregation enforces strong form modeling homogeneity synchronicity across client however suitable many practical scenario example distributed sensing heterogeneous sensor reading data different view phenomenon would need use different model different data modality local learning therefore happens isolation inference requires merging local model achieve consensus enable consensus among local model propose feature fusion approach extract local representation local model incorporates global representation improves prediction performance achieving requires addressing two problem first need learn alignment similar feature component arbitrarily arranged across client enable representation aggregation second need learn consensus graph capture interaction local feature space combine achieve better prediction paper present solution problem demonstrates application time series data power grid traffic network,5
265421df7e15e4f688d80f37855f836afc6139d1,BSGAN: A Novel Oversampling Technique for Imbalanced Pattern Recognitions,"Class imbalanced problems (CIP) are one of the potential challenges in developing unbiased Machine Learning (ML) models for predictions. CIP occurs when data samples are not equally distributed between the two or multiple classes. Borderline-Synthetic Minority Oversampling Techniques (SMOTE) is one of the approaches that has been used to balance the imbalance data by oversampling the minor (limited) samples. One of the potential drawbacks of existing Borderline-SMOTE is that it focuses on the data samples that lay at the border point and gives more attention to the extreme observations, ultimately limiting the creation of more diverse data after oversampling, and that is the almost scenario for the most of the borderline-SMOTE based oversampling strategies. As an effect, marginalization occurs after oversampling. To address these issues, in this work, we propose a hybrid oversampling technique by combining the power of borderline SMOTE and Generative Adversarial Network to generate more diverse data that follow Gaussian distributions. We named it BSGAN and tested it on four highly imbalanced datasets: Ecoli, Wine quality, Yeast, and Abalone. Our preliminary computational results reveal that BSGAN outperformed existing borderline SMOTE and GAN-based oversampling techniques and created a more diverse dataset that follows normal distribution after oversampling effect.",2023,"BSGAN: A Novel Oversampling Technique for Imbalanced Pattern RecognitionsClass imbalanced problems (CIP) are one of the potential challenges in developing unbiased Machine Learning (ML) models for predictions. CIP occurs when data samples are not equally distributed between the two or multiple classes. Borderline-Synthetic Minority Oversampling Techniques (SMOTE) is one of the approaches that has been used to balance the imbalance data by oversampling the minor (limited) samples. One of the potential drawbacks of existing Borderline-SMOTE is that it focuses on the data samples that lay at the border point and gives more attention to the extreme observations, ultimately limiting the creation of more diverse data after oversampling, and that is the almost scenario for the most of the borderline-SMOTE based oversampling strategies. As an effect, marginalization occurs after oversampling. To address these issues, in this work, we propose a hybrid oversampling technique by combining the power of borderline SMOTE and Generative Adversarial Network to generate more diverse data that follow Gaussian distributions. We named it BSGAN and tested it on four highly imbalanced datasets: Ecoli, Wine quality, Yeast, and Abalone. Our preliminary computational results reveal that BSGAN outperformed existing borderline SMOTE and GAN-based oversampling techniques and created a more diverse dataset that follows normal distribution after oversampling effect.",bsgan novel oversampling technique imbalanced pattern recognitionsclass imbalanced problem cip one potential challenge developing unbiased machine learning ml model prediction cip occurs data sample equally distributed two multiple class minority oversampling technique smote one approach ha used balance imbalance data oversampling minor limited sample one potential drawback existing focus data sample lay border point give attention extreme observation ultimately limiting creation diverse data oversampling almost scenario based oversampling strategy effect marginalization occurs oversampling address issue work propose hybrid oversampling technique combining power borderline smote generative adversarial network generate diverse data follow gaussian distribution named bsgan tested four highly imbalanced datasets ecoli wine quality yeast abalone preliminary computational result reveal bsgan outperformed existing borderline smote oversampling technique created diverse dataset follows normal distribution oversampling effect,2
0e7208e3e760947d5c7d5af27a9355887dd001d6,"Training, Architecture, and Prior for Deterministic Uncertainty Methods","Accurate and efficient uncertainty estimation is crucial to build reliable Machine Learning (ML) models capable to provide calibrated uncertainty estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this end, Deterministic Uncertainty Methods (DUMs) is a promising model family capable to perform uncertainty estimation in a single forward pass. This work investigates important design choices in DUMs: (1) we show that training schemes decoupling the core architecture and the uncertainty head schemes can significantly improve uncertainty performances. (2) we demonstrate that the core architecture expressiveness is crucial for uncertainty performance and that additional architecture constraints to avoid feature collapse can deteriorate the trade-off between OOD generalization and detection. (3) Contrary to other Bayesian models, we show that the prior defined by DUMs do not have a strong effect on the final performances.",2023,"Training, Architecture, and Prior for Deterministic Uncertainty MethodsAccurate and efficient uncertainty estimation is crucial to build reliable Machine Learning (ML) models capable to provide calibrated uncertainty estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this end, Deterministic Uncertainty Methods (DUMs) is a promising model family capable to perform uncertainty estimation in a single forward pass. This work investigates important design choices in DUMs: (1) we show that training schemes decoupling the core architecture and the uncertainty head schemes can significantly improve uncertainty performances. (2) we demonstrate that the core architecture expressiveness is crucial for uncertainty performance and that additional architecture constraints to avoid feature collapse can deteriorate the trade-off between OOD generalization and detection. (3) Contrary to other Bayesian models, we show that the prior defined by DUMs do not have a strong effect on the final performances.",training architecture prior deterministic uncertainty methodsaccurate efficient uncertainty estimation crucial build reliable machine learning ml model capable provide calibrated uncertainty estimate generalize detect ood datasets end deterministic uncertainty method dums promising model family capable perform uncertainty estimation single forward pas work investigates important design choice dums show training scheme decoupling core architecture uncertainty head scheme significantly improve uncertainty performance demonstrate core architecture expressiveness crucial uncertainty performance additional architecture constraint avoid feature collapse deteriorate ood generalization detection contrary bayesian model show prior defined dums strong effect final performance,8
9541cf136f442e992f10021c53081f33c73a2ed0,Recognize Anything: A Strong Image Tagging Model,"We present the Recognize Anything Model (RAM): a strong foundation model for image tagging. RAM makes a substantial step for large models in computer vision, demonstrating the zero-shot ability to recognize any common category with high accuracy. RAM introduces a new paradigm for image tagging, leveraging large-scale image-text pairs for training instead of manual annotations. The development of RAM comprises four key steps. Firstly, annotation-free image tags are obtained at scale through automatic text semantic parsing. Subsequently, a preliminary model is trained for automatic annotation by unifying the caption and tagging tasks, supervised by the original texts and parsed tags, respectively. Thirdly, a data engine is employed to generate additional annotations and clean incorrect ones. Lastly, the model is retrained with the processed data and fine-tuned using a smaller but higher-quality dataset. We evaluate the tagging capabilities of RAM on numerous benchmarks and observe impressive zero-shot performance, significantly outperforming CLIP and BLIP. Remarkably, RAM even surpasses the fully supervised manners and exhibits competitive performance with the Google tagging API. We are releasing the RAM at \url{https://recognize-anything.github.io/} to foster the advancements of large models in computer vision.",2023,"Recognize Anything: A Strong Image Tagging ModelWe present the Recognize Anything Model (RAM): a strong foundation model for image tagging. RAM makes a substantial step for large models in computer vision, demonstrating the zero-shot ability to recognize any common category with high accuracy. RAM introduces a new paradigm for image tagging, leveraging large-scale image-text pairs for training instead of manual annotations. The development of RAM comprises four key steps. Firstly, annotation-free image tags are obtained at scale through automatic text semantic parsing. Subsequently, a preliminary model is trained for automatic annotation by unifying the caption and tagging tasks, supervised by the original texts and parsed tags, respectively. Thirdly, a data engine is employed to generate additional annotations and clean incorrect ones. Lastly, the model is retrained with the processed data and fine-tuned using a smaller but higher-quality dataset. We evaluate the tagging capabilities of RAM on numerous benchmarks and observe impressive zero-shot performance, significantly outperforming CLIP and BLIP. Remarkably, RAM even surpasses the fully supervised manners and exhibits competitive performance with the Google tagging API. We are releasing the RAM at \url{https://recognize-anything.github.io/} to foster the advancements of large models in computer vision.",recognize anything strong image tagging modelwe present recognize anything model ram strong foundation model image tagging ram make substantial step large model computer vision demonstrating ability recognize common category high accuracy ram introduces new paradigm image tagging leveraging pair training instead manual annotation development ram comprises four key step firstly image tag obtained scale automatic text semantic parsing subsequently preliminary model trained automatic annotation unifying caption tagging task supervised original text parsed tag respectively thirdly data engine employed generate additional annotation clean incorrect one lastly model retrained processed data using smaller dataset evaluate tagging capability ram numerous benchmark observe impressive performance significantly outperforming clip blip remarkably ram even surpasses fully supervised manner exhibit competitive performance google tagging api releasing ram http foster advancement large model computer vision,7
9ed40ac0a7b69c0e007508aa2bf73b9deed99adc,Self-distillation for surgical action recognition,"Surgical scene understanding is a key prerequisite for contextaware decision support in the operating room. While deep learning-based approaches have already reached or even surpassed human performance in various fields, the task of surgical action recognition remains a major challenge. With this contribution, we are the first to investigate the concept of self-distillation as a means of addressing class imbalance and potential label ambiguity in surgical video analysis. Our proposed method is a heterogeneous ensemble of three models that use Swin Transfomers as backbone and the concepts of self-distillation and multi-task learning as core design choices. According to ablation studies performed with the CholecT45 challenge data via cross-validation, the biggest performance boost is achieved by the usage of soft labels obtained by self-distillation. External validation of our method on an independent test set was achieved by providing a Docker container of our inference model to the challenge organizers. According to their analysis, our method outperforms all other solutions submitted to the latest challenge in the field. Our approach thus shows the potential of self-distillation for becoming an important tool in medical image analysis applications.",2023,"Self-distillation for surgical action recognitionSurgical scene understanding is a key prerequisite for contextaware decision support in the operating room. While deep learning-based approaches have already reached or even surpassed human performance in various fields, the task of surgical action recognition remains a major challenge. With this contribution, we are the first to investigate the concept of self-distillation as a means of addressing class imbalance and potential label ambiguity in surgical video analysis. Our proposed method is a heterogeneous ensemble of three models that use Swin Transfomers as backbone and the concepts of self-distillation and multi-task learning as core design choices. According to ablation studies performed with the CholecT45 challenge data via cross-validation, the biggest performance boost is achieved by the usage of soft labels obtained by self-distillation. External validation of our method on an independent test set was achieved by providing a Docker container of our inference model to the challenge organizers. According to their analysis, our method outperforms all other solutions submitted to the latest challenge in the field. Our approach thus shows the potential of self-distillation for becoming an important tool in medical image analysis applications.",surgical action recognitionsurgical scene understanding key prerequisite contextaware decision support operating room deep approach already reached even surpassed human performance various field task surgical action recognition remains major challenge contribution first investigate concept mean addressing class imbalance potential label ambiguity surgical video analysis proposed method heterogeneous ensemble three model use swin transfomers backbone concept learning core design choice according ablation study performed challenge data via biggest performance boost achieved usage soft label obtained external validation method independent test set wa achieved providing docker container inference model challenge organizer according analysis method outperforms solution submitted latest challenge field approach thus show potential becoming important tool medical image analysis application,4
659ce1e0e2a32c4f1c4980be1cf1412b696bd06b,Algorithmic progress in computer vision,"We investigate algorithmic progress in image classification on ImageNet, perhaps the most well-known test bed for computer vision. We estimate a model, informed by work on neural scaling laws, and infer a decomposition of progress into the scaling of compute, data, and algorithms. Using Shapley values to attribute performance improvements, we find that algorithmic improvements have been roughly as important as the scaling of compute for progress computer vision. Our estimates indicate that algorithmic innovations mostly take the form of compute-augmenting algorithmic advances (which enable researchers to get better performance from less compute), not data-augmenting algorithmic advances. We find that compute-augmenting algorithmic advances are made at a pace more than twice as fast as the rate usually associated with Moore's law. In particular, we estimate that compute-augmenting innovations halve compute requirements every nine months (95\% confidence interval: 4 to 25 months).",2022,"Algorithmic progress in computer visionWe investigate algorithmic progress in image classification on ImageNet, perhaps the most well-known test bed for computer vision. We estimate a model, informed by work on neural scaling laws, and infer a decomposition of progress into the scaling of compute, data, and algorithms. Using Shapley values to attribute performance improvements, we find that algorithmic improvements have been roughly as important as the scaling of compute for progress computer vision. Our estimates indicate that algorithmic innovations mostly take the form of compute-augmenting algorithmic advances (which enable researchers to get better performance from less compute), not data-augmenting algorithmic advances. We find that compute-augmenting algorithmic advances are made at a pace more than twice as fast as the rate usually associated with Moore's law. In particular, we estimate that compute-augmenting innovations halve compute requirements every nine months (95\% confidence interval: 4 to 25 months).",algorithmic progress computer visionwe investigate algorithmic progress image classification imagenet perhaps test bed computer vision estimate model informed work neural scaling law infer decomposition progress scaling compute data algorithm using shapley value attribute performance improvement find algorithmic improvement roughly important scaling compute progress computer vision estimate indicate algorithmic innovation mostly take form algorithmic advance enable researcher get better performance le compute algorithmic advance find algorithmic advance made pace twice fast rate usually associated moore law particular estimate innovation halve compute requirement every nine month confidence interval month,9
b3571e31437497d3fd05211f18f58bc03e6d7304,MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale,"Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability when applied to the largest publicly-available node classification benchmark exceeding 1TB in size.",2023,"MuseGNN: Interpretable and Convergent Graph Neural Network Layers at ScaleAmong the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability when applied to the largest publicly-available node classification benchmark exceeding 1TB in size.",musegnn interpretable convergent graph neural network layer scaleamong many variant graph neural network gnn architecture capable modeling data relation important subclass involves layer designed forward pas iteratively reduces energy function interest way node embeddings produced output layer dually serve predictive feature solving downstream task node classification energy function minimizers inherit desirable inductive bias interpretability however scaling gnn architecture constructed way remains challenging part convergence forward pas may involve model considerable depth tackle limitation propose energy function scalable gnn layer iteratively reduce guided convergence guarantee certain setting also instantiate full gnn architecture based design model achieves competitive accuracy scalability applied largest node classification benchmark exceeding size,0
8fb6db8c71ccdafff4546cb3ef9dfa9ea6089cf1,Latent Diffusion Counterfactual Explanations,"Counterfactual explanations have emerged as a promising method for elucidating the behavior of opaque black-box models. Recently, several works leveraged pixel-space diffusion models for counterfactual generation. To handle noisy, adversarial gradients during counterfactual generation -- causing unrealistic artifacts or mere adversarial perturbations -- they required either auxiliary adversarially robust models or computationally intensive guidance schemes. However, such requirements limit their applicability, e.g., in scenarios with restricted access to the model's training data. To address these limitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE). LDCE harnesses the capabilities of recent class- or text-conditional foundation latent diffusion models to expedite counterfactual generation and focus on the important, semantic parts of the data. Furthermore, we propose a novel consensus guidance mechanism to filter out noisy, adversarial gradients that are misaligned with the diffusion model's implicit classifier. We demonstrate the versatility of LDCE across a wide spectrum of models trained on diverse datasets with different learning paradigms. Finally, we showcase how LDCE can provide insights into model errors, enhancing our understanding of black-box model behavior.",2023,"Latent Diffusion Counterfactual ExplanationsCounterfactual explanations have emerged as a promising method for elucidating the behavior of opaque black-box models. Recently, several works leveraged pixel-space diffusion models for counterfactual generation. To handle noisy, adversarial gradients during counterfactual generation -- causing unrealistic artifacts or mere adversarial perturbations -- they required either auxiliary adversarially robust models or computationally intensive guidance schemes. However, such requirements limit their applicability, e.g., in scenarios with restricted access to the model's training data. To address these limitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE). LDCE harnesses the capabilities of recent class- or text-conditional foundation latent diffusion models to expedite counterfactual generation and focus on the important, semantic parts of the data. Furthermore, we propose a novel consensus guidance mechanism to filter out noisy, adversarial gradients that are misaligned with the diffusion model's implicit classifier. We demonstrate the versatility of LDCE across a wide spectrum of models trained on diverse datasets with different learning paradigms. Finally, we showcase how LDCE can provide insights into model errors, enhancing our understanding of black-box model behavior.",latent diffusion counterfactual explanationscounterfactual explanation emerged promising method elucidating behavior opaque model recently several work leveraged diffusion model counterfactual generation handle noisy adversarial gradient counterfactual generation causing unrealistic artifact mere adversarial perturbation required either auxiliary adversarially robust model computationally intensive guidance scheme however requirement limit applicability scenario restricted access model training data address limitation introduce latent diffusion counterfactual explanation ldce ldce harness capability recent foundation latent diffusion model expedite counterfactual generation focus important semantic part data furthermore propose novel consensus guidance mechanism filter noisy adversarial gradient misaligned diffusion model implicit classifier demonstrate versatility ldce across wide spectrum model trained diverse datasets different learning paradigm finally showcase ldce provide insight model error enhancing understanding model behavior,3
3a8ade79b61b5ebe86ee9a0f5f16fc094a76ebb8,Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in Autonomous Vehicle Safety,"Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving. While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems. Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios. To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator. This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame. Furthermore, an end-to-end short-term trajectory prediction model using convolutional neural networks (CNN) and long short-term memory (LSTM) networks has also been developed. This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment. In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community. Our datasets are publicly available on https://github.com/sharmasushil/Navigating-Uncertainty-Trajectory-Prediction .",2023,"Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in Autonomous Vehicle SafetyAutonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving. While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems. Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios. To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator. This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame. Furthermore, an end-to-end short-term trajectory prediction model using convolutional neural networks (CNN) and long short-term memory (LSTM) networks has also been developed. This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment. In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community. Our datasets are publicly available on https://github.com/sharmasushil/Navigating-Uncertainty-Trajectory-Prediction .",navigating uncertainty role trajectory prediction autonomous vehicle safetyautonomous vehicle require accurate reliable trajectory prediction safe efficient driving commercial automated vehicle currently use state algorithm trajectory forecasting recent effort focused system often design model limited availability datasets typically restricted generic scenario address limitation developed synthetic dataset trajectory prediction task using carla simulator dataset extensive incorporates considered complex scenario pedestrian crossing road vehicle overtaking comprises perspective view image corresponding imu odometry information frame furthermore trajectory prediction model using convolutional neural network cnn long memory lstm network ha also developed model handle corner case slowing near zebra crossing stopping pedestrian cross road without need explicit encoding surrounding environment effort accelerate research assist others releasing dataset model research community datasets publicly available http,1
74b91baef6641bc80f7daa5df4507717bf6e77cd,Long-Range Transformer Architectures for Document Understanding,,2023,Long-Range Transformer Architectures for Document Understanding,transformer architecture document understanding,6
549f026dfe0dcd86ff8f081584f3385e5696fd8e,When Source-Free Domain Adaptation Meets Learning with Noisy Labels,"Recent state-of-the-art source-free domain adaptation (SFDA) methods have focused on learning meaningful cluster structures in the feature space, which have succeeded in adapting the knowledge from source domain to unlabeled target domain without accessing the private source data. However, existing methods rely on the pseudo-labels generated by source models that can be noisy due to domain shift. In this paper, we study SFDA from the perspective of learning with label noise (LLN). Unlike the label noise in the conventional LLN scenario, we prove that the label noise in SFDA follows a different distribution assumption. We also prove that such a difference makes existing LLN methods that rely on their distribution assumptions unable to address the label noise in SFDA. Empirical evidence suggests that only marginal improvements are achieved when applying the existing LLN methods to solve the SFDA problem. On the other hand, although there exists a fundamental difference between the label noise in the two scenarios, we demonstrate theoretically that the early-time training phenomenon (ETP), which has been previously observed in conventional label noise settings, can also be observed in the SFDA problem. Extensive experiments demonstrate significant improvements to existing SFDA algorithms by leveraging ETP to address the label noise in SFDA.",2023,"When Source-Free Domain Adaptation Meets Learning with Noisy LabelsRecent state-of-the-art source-free domain adaptation (SFDA) methods have focused on learning meaningful cluster structures in the feature space, which have succeeded in adapting the knowledge from source domain to unlabeled target domain without accessing the private source data. However, existing methods rely on the pseudo-labels generated by source models that can be noisy due to domain shift. In this paper, we study SFDA from the perspective of learning with label noise (LLN). Unlike the label noise in the conventional LLN scenario, we prove that the label noise in SFDA follows a different distribution assumption. We also prove that such a difference makes existing LLN methods that rely on their distribution assumptions unable to address the label noise in SFDA. Empirical evidence suggests that only marginal improvements are achieved when applying the existing LLN methods to solve the SFDA problem. On the other hand, although there exists a fundamental difference between the label noise in the two scenarios, we demonstrate theoretically that the early-time training phenomenon (ETP), which has been previously observed in conventional label noise settings, can also be observed in the SFDA problem. Extensive experiments demonstrate significant improvements to existing SFDA algorithms by leveraging ETP to address the label noise in SFDA.",domain adaptation meet learning noisy labelsrecent domain adaptation sfda method focused learning meaningful cluster structure feature space succeeded adapting knowledge source domain unlabeled target domain without accessing private source data however existing method rely generated source model noisy due domain shift paper study sfda perspective learning label noise lln unlike label noise conventional lln scenario prove label noise sfda follows different distribution assumption also prove difference make existing lln method rely distribution assumption unable address label noise sfda empirical evidence suggests marginal improvement achieved applying existing lln method solve sfda problem hand although exists fundamental difference label noise two scenario demonstrate theoretically training phenomenon etp ha previously observed conventional label noise setting also observed sfda problem extensive experiment demonstrate significant improvement existing sfda algorithm leveraging etp address label noise sfda,2
6634b7cd186f36bfb151883e9542c462427107cf,Probabilistic Linguistic Knowledge and Token-level Text Augmentation,"This paper investigates the effectiveness of token-level text augmentation and the role of probabilistic linguistic knowledge within a linguistically-motivated evaluation context. Two text augmentation programs, REDA and REDA$_{NG}$, were developed, both implementing five token-level text editing operations: Synonym Replacement (SR), Random Swap (RS), Random Insertion (RI), Random Deletion (RD), and Random Mix (RM). REDA$_{NG}$ leverages pretrained $n$-gram language models to select the most likely augmented texts from REDA's output. Comprehensive and fine-grained experiments were conducted on a binary question matching classification task in both Chinese and English. The results strongly refute the general effectiveness of the five token-level text augmentation techniques under investigation, whether applied together or separately, and irrespective of various common classification model types used, including transformers. Furthermore, the role of probabilistic linguistic knowledge is found to be minimal.",2023,"Probabilistic Linguistic Knowledge and Token-level Text AugmentationThis paper investigates the effectiveness of token-level text augmentation and the role of probabilistic linguistic knowledge within a linguistically-motivated evaluation context. Two text augmentation programs, REDA and REDA$_{NG}$, were developed, both implementing five token-level text editing operations: Synonym Replacement (SR), Random Swap (RS), Random Insertion (RI), Random Deletion (RD), and Random Mix (RM). REDA$_{NG}$ leverages pretrained $n$-gram language models to select the most likely augmented texts from REDA's output. Comprehensive and fine-grained experiments were conducted on a binary question matching classification task in both Chinese and English. The results strongly refute the general effectiveness of the five token-level text augmentation techniques under investigation, whether applied together or separately, and irrespective of various common classification model types used, including transformers. Furthermore, the role of probabilistic linguistic knowledge is found to be minimal.",probabilistic linguistic knowledge text augmentationthis paper investigates effectiveness text augmentation role probabilistic linguistic knowledge within evaluation context two text augmentation program reda reda ng developed implementing five text editing operation synonym replacement sr random swap r random insertion ri random deletion rd random mix rm reda ng leverage pretrained n language model select likely augmented text reda output comprehensive experiment conducted binary question matching classification task chinese english result strongly refute general effectiveness five text augmentation technique investigation whether applied together separately irrespective various common classification model type used including transformer furthermore role probabilistic linguistic knowledge found minimal,7
5521e81a4858ce54c18f5a01965a73e7006835f2,IPDreamer: Appearance-Controllable 3D Object Generation with Image Prompts,"Recent advances in text-to-3D generation have been remarkable, with methods such as DreamFusion leveraging large-scale text-to-image diffusion-based models to supervise 3D generation. These methods, including the variational score distillation proposed by ProlificDreamer, enable the synthesis of detailed and photorealistic textured meshes. However, the appearance of 3D objects generated by these methods is often random and uncontrollable, posing a challenge in achieving appearance-controllable 3D objects. To address this challenge, we introduce IPDreamer, a novel approach that incorporates image prompts to provide specific and comprehensive appearance information for 3D object generation. Our results demonstrate that IPDreamer effectively generates high-quality 3D objects that are consistent with both the provided text and image prompts, demonstrating its promising capability in appearance-controllable 3D object generation.",2023,"IPDreamer: Appearance-Controllable 3D Object Generation with Image PromptsRecent advances in text-to-3D generation have been remarkable, with methods such as DreamFusion leveraging large-scale text-to-image diffusion-based models to supervise 3D generation. These methods, including the variational score distillation proposed by ProlificDreamer, enable the synthesis of detailed and photorealistic textured meshes. However, the appearance of 3D objects generated by these methods is often random and uncontrollable, posing a challenge in achieving appearance-controllable 3D objects. To address this challenge, we introduce IPDreamer, a novel approach that incorporates image prompts to provide specific and comprehensive appearance information for 3D object generation. Our results demonstrate that IPDreamer effectively generates high-quality 3D objects that are consistent with both the provided text and image prompts, demonstrating its promising capability in appearance-controllable 3D object generation.",ipdreamer object generation image promptsrecent advance generation remarkable method dreamfusion leveraging model supervise generation method including variational score distillation proposed prolificdreamer enable synthesis detailed photorealistic textured mesh however appearance object generated method often random uncontrollable posing challenge achieving object address challenge introduce ipdreamer novel approach incorporates image prompt provide specific comprehensive appearance information object generation result demonstrate ipdreamer effectively generates object consistent provided text image prompt demonstrating promising capability object generation,3
d21e52c6362ad9eac1d16c2850960410cc31eb02,Sound-Print: Generalised Face Presentation Attack Detection using Deep Representation of Sound Echoes,"Facial biometrics are widely deployed in smartphone-based applications because of their usability and increased verification accuracy in unconstrained scenarios. The evolving applications of smartphone-based facial recognition have also increased Presentation Attacks (PAs), where an attacker can present a Presentation Attack Instrument (PAI) to maliciously gain access to the application. Because the materials used to generate PAI are not deterministic, the detection of unknown presentation attacks is challenging. In this paper, we present an acoustic echo-based face Presentation Attack Detection (PAD) on a smartphone in which the PAs are detected based on the reflection profiles of the transmitted signal. We propose a novel transmission signal based on the wide pulse that allows us to model the background noise before transmitting the signal and increase the Signal-to-Noise Ratio (SNR). The received signal reflections were processed to remove background noise and accurately represent reflection characteristics. The reflection profiles of the bona fide and PAs are different owing to the different reflection characteristics of the human skin and artefact materials. Extensive experiments are presented using the newly collected Acoustic Sound Echo Dataset (ASED) with 4807 samples captured from bona fide and four different types of PAIs, including print (two types), display, and silicone face-mask attacks. The obtained results indicate the robustness of the proposed method for detecting unknown face presentation attacks.",2023,"Sound-Print: Generalised Face Presentation Attack Detection using Deep Representation of Sound EchoesFacial biometrics are widely deployed in smartphone-based applications because of their usability and increased verification accuracy in unconstrained scenarios. The evolving applications of smartphone-based facial recognition have also increased Presentation Attacks (PAs), where an attacker can present a Presentation Attack Instrument (PAI) to maliciously gain access to the application. Because the materials used to generate PAI are not deterministic, the detection of unknown presentation attacks is challenging. In this paper, we present an acoustic echo-based face Presentation Attack Detection (PAD) on a smartphone in which the PAs are detected based on the reflection profiles of the transmitted signal. We propose a novel transmission signal based on the wide pulse that allows us to model the background noise before transmitting the signal and increase the Signal-to-Noise Ratio (SNR). The received signal reflections were processed to remove background noise and accurately represent reflection characteristics. The reflection profiles of the bona fide and PAs are different owing to the different reflection characteristics of the human skin and artefact materials. Extensive experiments are presented using the newly collected Acoustic Sound Echo Dataset (ASED) with 4807 samples captured from bona fide and four different types of PAIs, including print (two types), display, and silicone face-mask attacks. The obtained results indicate the robustness of the proposed method for detecting unknown face presentation attacks.",generalised face presentation attack detection using deep representation sound echoesfacial biometrics widely deployed application usability increased verification accuracy unconstrained scenario evolving application facial recognition also increased presentation attack pa attacker present presentation attack instrument pai maliciously gain access application material used generate pai deterministic detection unknown presentation attack challenging paper present acoustic face presentation attack detection pad smartphone pa detected based reflection profile transmitted signal propose novel transmission signal based wide pulse allows u model background noise transmitting signal increase ratio snr received signal reflection processed remove background noise accurately represent reflection characteristic reflection profile bona fide pa different owing different reflection characteristic human skin artefact material extensive experiment presented using newly collected acoustic sound echo dataset ased sample captured bona fide four different type pais including print two type display silicone attack obtained result indicate robustness proposed method detecting unknown face presentation attack,4
3586d39566b9fd77e7c20345f1244b63e97115f5,Fusion of Satellite Images and Weather Data With Transformer Networks for Downy Mildew Disease Detection,"Crop diseases significantly affect the quantity and quality of agricultural production. In a context where the goal of precision agriculture is to minimize or even avoid the use of pesticides, weather and remote sensing data with deep learning can play a pivotal role in detecting crop diseases, allowing localized treatment of crops. However, combining heterogeneous data such as weather and images remains a hot topic and challenging task. Recent developments in transformer architectures have shown the possibility of fusion of data from different domains, such as text-image. The current trend is to custom only one transformer to create a multimodal fusion model. Conversely, we propose a new approach to realize data fusion using three transformers. In this paper, we first solved the missing satellite images problem, by interpolating them with a ConvLSTM model. Then, we proposed a multimodal fusion architecture that jointly learns to process visual and weather information. The architecture is built from three main components, a Vision Transformer and two transformer-encoders, allowing to fuse both image and weather modalities. The results of the proposed method are promising achieving an overall accuracy of 97%.",2022,"Fusion of Satellite Images and Weather Data With Transformer Networks for Downy Mildew Disease DetectionCrop diseases significantly affect the quantity and quality of agricultural production. In a context where the goal of precision agriculture is to minimize or even avoid the use of pesticides, weather and remote sensing data with deep learning can play a pivotal role in detecting crop diseases, allowing localized treatment of crops. However, combining heterogeneous data such as weather and images remains a hot topic and challenging task. Recent developments in transformer architectures have shown the possibility of fusion of data from different domains, such as text-image. The current trend is to custom only one transformer to create a multimodal fusion model. Conversely, we propose a new approach to realize data fusion using three transformers. In this paper, we first solved the missing satellite images problem, by interpolating them with a ConvLSTM model. Then, we proposed a multimodal fusion architecture that jointly learns to process visual and weather information. The architecture is built from three main components, a Vision Transformer and two transformer-encoders, allowing to fuse both image and weather modalities. The results of the proposed method are promising achieving an overall accuracy of 97%.",fusion satellite image weather data transformer network downy mildew disease detectioncrop disease significantly affect quantity quality agricultural production context goal precision agriculture minimize even avoid use pesticide weather remote sensing data deep learning play pivotal role detecting crop disease allowing localized treatment crop however combining heterogeneous data weather image remains hot topic challenging task recent development transformer architecture shown possibility fusion data different domain current trend custom one transformer create multimodal fusion model conversely propose new approach realize data fusion using three transformer paper first solved missing satellite image problem interpolating convlstm model proposed multimodal fusion architecture jointly learns process visual weather information architecture built three main component vision transformer two allowing fuse image weather modality result proposed method promising achieving overall accuracy,6
174a9b78350e9561555052bc6901cc44782f4c62,Estimating Numbers without Regression,"Despite recent successes in language models, their ability to represent numbers is insufficient. Humans conceptualize numbers based on their magnitudes, effectively projecting them on a number line; whereas subword tokenization fails to explicitly capture magnitude by splitting numbers into arbitrary chunks. To alleviate this shortcoming, alternative approaches have been proposed that modify numbers at various stages of the language modeling pipeline. These methods change either the (1) notation in which numbers are written (\eg scientific vs decimal), the (2) vocabulary used to represent numbers or the entire (3) architecture of the underlying language model, to directly regress to a desired number. Previous work suggests that architectural change helps achieve state-of-the-art on number estimation but we find an insightful ablation: changing the model's vocabulary instead (\eg introduce a new token for numbers in range 10-100) is a far better trade-off. In the context of masked number prediction, a carefully designed tokenization scheme is both the simplest to implement and sufficient, \ie with similar performance to the state-of-the-art approach that requires making significant architectural changes. Finally, we report similar trends on the downstream task of numerical fact estimation (for Fermi Problems) and discuss reasons behind our findings.",2023,"Estimating Numbers without RegressionDespite recent successes in language models, their ability to represent numbers is insufficient. Humans conceptualize numbers based on their magnitudes, effectively projecting them on a number line; whereas subword tokenization fails to explicitly capture magnitude by splitting numbers into arbitrary chunks. To alleviate this shortcoming, alternative approaches have been proposed that modify numbers at various stages of the language modeling pipeline. These methods change either the (1) notation in which numbers are written (\eg scientific vs decimal), the (2) vocabulary used to represent numbers or the entire (3) architecture of the underlying language model, to directly regress to a desired number. Previous work suggests that architectural change helps achieve state-of-the-art on number estimation but we find an insightful ablation: changing the model's vocabulary instead (\eg introduce a new token for numbers in range 10-100) is a far better trade-off. In the context of masked number prediction, a carefully designed tokenization scheme is both the simplest to implement and sufficient, \ie with similar performance to the state-of-the-art approach that requires making significant architectural changes. Finally, we report similar trends on the downstream task of numerical fact estimation (for Fermi Problems) and discuss reasons behind our findings.",estimating number without regressiondespite recent success language model ability represent number insufficient human conceptualize number based magnitude effectively projecting number line whereas subword tokenization fails explicitly capture magnitude splitting number arbitrary chunk alleviate shortcoming alternative approach proposed modify number various stage language modeling pipeline method change either notation number written scientific v decimal vocabulary used represent number entire architecture underlying language model directly regress desired number previous work suggests architectural change help achieve number estimation find insightful ablation changing model vocabulary instead introduce new token number range far better context masked number prediction carefully designed tokenization scheme simplest implement sufficient similar performance approach requires making significant architectural change finally report similar trend downstream task numerical fact estimation fermi problem discus reason behind finding,8
57707fc1f0d918663eced33f1fb9bf8ed10227f2,Maximal Objectives in the Multi-armed Bandit with Applications,"In several applications of the stochastic multi-armed bandit problem, the traditional objective of maximizing the expected total reward can be inappropriate. In this paper, motivated by certain operational concerns in online platforms, we consider a new objective in the classical setup. Given $K$ arms, instead of maximizing the expected total reward from $T$ pulls (the traditional""sum""objective), we consider the vector of total rewards earned from each of the $K$ arms at the end of $T$ pulls and aim to maximize the expected highest total reward across arms (the""max""objective). For this objective, we show that any policy must incur an instance-dependent asymptotic regret of $\Omega(\log T)$ (with a higher instance-dependent constant compared to the traditional objective) and a worst-case regret of $\Omega(K^{1/3}T^{2/3})$. We then design an adaptive explore-then-commit policy featuring exploration based on appropriately tuned confidence bounds on the mean reward and an adaptive stopping criterion, which adapts to the problem difficulty and achieves these bounds (up to logarithmic factors). We then generalize our algorithmic insights to the problem of maximizing the expected value of the average total reward of the top $m$ arms with the highest total rewards. Our numerical experiments demonstrate the efficacy of our policies compared to several natural alternatives in practical parameter regimes. We discuss applications of these new objectives to the problem of grooming an adequate supply of value-providing market participants (workers/sellers/service providers) in online platforms.",2020,"Maximal Objectives in the Multi-armed Bandit with ApplicationsIn several applications of the stochastic multi-armed bandit problem, the traditional objective of maximizing the expected total reward can be inappropriate. In this paper, motivated by certain operational concerns in online platforms, we consider a new objective in the classical setup. Given $K$ arms, instead of maximizing the expected total reward from $T$ pulls (the traditional""sum""objective), we consider the vector of total rewards earned from each of the $K$ arms at the end of $T$ pulls and aim to maximize the expected highest total reward across arms (the""max""objective). For this objective, we show that any policy must incur an instance-dependent asymptotic regret of $\Omega(\log T)$ (with a higher instance-dependent constant compared to the traditional objective) and a worst-case regret of $\Omega(K^{1/3}T^{2/3})$. We then design an adaptive explore-then-commit policy featuring exploration based on appropriately tuned confidence bounds on the mean reward and an adaptive stopping criterion, which adapts to the problem difficulty and achieves these bounds (up to logarithmic factors). We then generalize our algorithmic insights to the problem of maximizing the expected value of the average total reward of the top $m$ arms with the highest total rewards. Our numerical experiments demonstrate the efficacy of our policies compared to several natural alternatives in practical parameter regimes. We discuss applications of these new objectives to the problem of grooming an adequate supply of value-providing market participants (workers/sellers/service providers) in online platforms.",maximal objective bandit applicationsin several application stochastic bandit problem traditional objective maximizing expected total reward inappropriate paper motivated certain operational concern online platform consider new objective classical setup given k arm instead maximizing expected total reward pull traditional sum objective consider vector total reward earned k arm end pull aim maximize expected highest total reward across arm max objective objective show policy must incur asymptotic regret higher constant compared traditional objective regret design adaptive policy featuring exploration based appropriately tuned confidence bound mean reward adaptive stopping criterion adapts problem difficulty achieves bound logarithmic factor generalize algorithmic insight problem maximizing expected value average total reward top arm highest total reward numerical experiment demonstrate efficacy policy compared several natural alternative practical parameter regime discus application new objective problem grooming adequate supply market participant provider online platform,8
6e393b84a336e4fe6034f91489d7fbd2b1b6319e,GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data,"Vertical federated learning (VFL) is a distributed learning paradigm, where computing clients collectively train a model based on the partial features of the same set of samples they possess. Current research on VFL focuses on the case when samples are independent, but it rarely addresses an emerging scenario when samples are interrelated through a graph. For graph-structured data, graph neural networks (GNNs) are competitive machine learning models, but a naive implementation in the VFL setting causes a significant communication overhead. Moreover, the analysis of the training is faced with a challenge caused by the biased stochastic gradients. In this paper, we propose a model splitting method that splits a backbone GNN across the clients and the server and a communication-efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and stale updates to skip aggregation when evaluating the model and skip feature exchanges during training, greatly reducing communication. We offer a theoretical analysis and conduct extensive numerical experiments on real-world datasets, showing that the proposed algorithm effectively trains a GNN model, whose performance matches that of the backbone GNN when trained in a centralized manner.",2023,"GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph DataVertical federated learning (VFL) is a distributed learning paradigm, where computing clients collectively train a model based on the partial features of the same set of samples they possess. Current research on VFL focuses on the case when samples are independent, but it rarely addresses an emerging scenario when samples are interrelated through a graph. For graph-structured data, graph neural networks (GNNs) are competitive machine learning models, but a naive implementation in the VFL setting causes a significant communication overhead. Moreover, the analysis of the training is faced with a challenge caused by the biased stochastic gradients. In this paper, we propose a model splitting method that splits a backbone GNN across the clients and the server and a communication-efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and stale updates to skip aggregation when evaluating the model and skip feature exchanges during training, greatly reducing communication. We offer a theoretical analysis and conduct extensive numerical experiments on real-world datasets, showing that the proposed algorithm effectively trains a GNN model, whose performance matches that of the backbone GNN when trained in a centralized manner.",glasu algorithm federated learning vertically distributed graph datavertical federated learning vfl distributed learning paradigm computing client collectively train model based partial feature set sample posse current research vfl focus case sample independent rarely address emerging scenario sample interrelated graph data graph neural network gnns competitive machine learning model naive implementation vfl setting cause significant communication overhead moreover analysis training faced challenge caused biased stochastic gradient paper propose model splitting method split backbone gnn across client server algorithm glasu train model glasu adopts lazy aggregation stale update skip aggregation evaluating model skip feature exchange training greatly reducing communication offer theoretical analysis conduct extensive numerical experiment datasets showing proposed algorithm effectively train gnn model whose performance match backbone gnn trained centralized manner,5
a7a74a093c9da205685f2cc2d692dfc6d77caccb,Unleashing the Potential of Spiking Neural Networks by Dynamic Confidence,"This paper presents a new methodology to alleviate the fundamental trade-off between accuracy and latency in spiking neural networks (SNNs). The approach involves decoding confidence information over time from the SNN outputs and using it to develop a decision-making agent that can dynamically determine when to terminate each inference. The proposed method, Dynamic Confidence, provides several significant benefits to SNNs. 1. It can effectively optimize latency dynamically at runtime, setting it apart from many existing low-latency SNN algorithms. Our experiments on CIFAR-10 and ImageNet datasets have demonstrated an average 40% speedup across eight different settings after applying Dynamic Confidence. 2. The decision-making agent in Dynamic Confidence is straightforward to construct and highly robust in parameter space, making it extremely easy to implement. 3. The proposed method enables visualizing the potential of any given SNN, which sets a target for current SNNs to approach. For instance, if an SNN can terminate at the most appropriate time point for each input sample, a ResNet-50 SNN can achieve an accuracy as high as 82.47% on ImageNet within just 4.71 time steps on average. Unlocking the potential of SNNs needs a highly-reliable decision-making agent to be constructed and fed with a high-quality estimation of ground truth. In this regard, Dynamic Confidence represents a meaningful step toward realizing the potential of SNNs.",2023,"Unleashing the Potential of Spiking Neural Networks by Dynamic ConfidenceThis paper presents a new methodology to alleviate the fundamental trade-off between accuracy and latency in spiking neural networks (SNNs). The approach involves decoding confidence information over time from the SNN outputs and using it to develop a decision-making agent that can dynamically determine when to terminate each inference. The proposed method, Dynamic Confidence, provides several significant benefits to SNNs. 1. It can effectively optimize latency dynamically at runtime, setting it apart from many existing low-latency SNN algorithms. Our experiments on CIFAR-10 and ImageNet datasets have demonstrated an average 40% speedup across eight different settings after applying Dynamic Confidence. 2. The decision-making agent in Dynamic Confidence is straightforward to construct and highly robust in parameter space, making it extremely easy to implement. 3. The proposed method enables visualizing the potential of any given SNN, which sets a target for current SNNs to approach. For instance, if an SNN can terminate at the most appropriate time point for each input sample, a ResNet-50 SNN can achieve an accuracy as high as 82.47% on ImageNet within just 4.71 time steps on average. Unlocking the potential of SNNs needs a highly-reliable decision-making agent to be constructed and fed with a high-quality estimation of ground truth. In this regard, Dynamic Confidence represents a meaningful step toward realizing the potential of SNNs.",unleashing potential spiking neural network dynamic confidencethis paper present new methodology alleviate fundamental accuracy latency spiking neural network snns approach involves decoding confidence information time snn output using develop agent dynamically determine terminate inference proposed method dynamic confidence provides several significant benefit snns effectively optimize latency dynamically runtime setting apart many existing snn algorithm experiment imagenet datasets demonstrated average speedup across eight different setting applying dynamic confidence agent dynamic confidence straightforward construct highly robust parameter space making extremely easy implement proposed method enables visualizing potential given snn set target current snns approach instance snn terminate appropriate time point input sample snn achieve accuracy high imagenet within time step average unlocking potential snns need agent constructed fed estimation ground truth regard dynamic confidence represents meaningful step toward realizing potential snns,0
bc99c855d52ba3d432c428fb4096b3a22c04f8bf,MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer,"The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for ViTs. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8\% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks. The code is available at https://github.com/fistyee/MixPro.",2023,"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision TransformerThe recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for ViTs. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8\% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks. The code is available at https://github.com/fistyee/MixPro.",mixpro data augmentation maskmix progressive attention labeling vision transformerthe recently proposed data augmentation transmix employ attention label help visual transformer vit achieve better robustness performance however transmix deficient two aspect image cropping method transmix may suitable vits early stage training model produce unreliable attention map transmix us unreliable attention map compute mixed attention label affect model address aforementioned issue propose maskmix progressive attention labeling pal image label space respectively detail perspective image space design maskmix mix two image based grid mask particular size mask patch adjustable multiple image patch size ensures image patch come one image contains global content perspective label space design pal utilizes progressive factor dynamically attention weight mixed attention label finally combine maskmix progressive attention labeling new data augmentation method named mixpro experimental result show method improve various model scale imagenet classification accuracy based epoch mixpro imagenet model also demonstrate better transferability semantic segmentation object detection instance segmentation furthermore compared transmix mixpro also show stronger robustness several benchmark code available http,6
d1c319386655d8ff873da1f8e76eda5c28dde0b3,Some Options for Instantiation of Bipolar Argument Graphs with Deductive Arguments,"Argument graphs provide an abstract representation of an argumentative situation. A bipolar argument graph is a directed graph where each node denotes an argument, and each arc denotes the influence of one argument on another. Here we assume that the influence is supporting, attacking, or ambiguous. In a bipolar argument graph, each argument is atomic and so it has no internal structure. Yet to better understand the nature of the individual arguments, and how they interact, it is important to consider their internal structure. To address this need, this paper presents a framework based on the use of logical arguments to instantiate bipolar argument graphs, and a set of possible constraints on instantiating arguments that take into account the internal structure of the arguments, and the types of relationship between arguments.",2023,"Some Options for Instantiation of Bipolar Argument Graphs with Deductive ArgumentsArgument graphs provide an abstract representation of an argumentative situation. A bipolar argument graph is a directed graph where each node denotes an argument, and each arc denotes the influence of one argument on another. Here we assume that the influence is supporting, attacking, or ambiguous. In a bipolar argument graph, each argument is atomic and so it has no internal structure. Yet to better understand the nature of the individual arguments, and how they interact, it is important to consider their internal structure. To address this need, this paper presents a framework based on the use of logical arguments to instantiate bipolar argument graphs, and a set of possible constraints on instantiating arguments that take into account the internal structure of the arguments, and the types of relationship between arguments.",option instantiation bipolar argument graph deductive argumentsargument graph provide abstract representation argumentative situation bipolar argument graph directed graph node denotes argument arc denotes influence one argument another assume influence supporting attacking ambiguous bipolar argument graph argument atomic ha internal structure yet better understand nature individual argument interact important consider internal structure address need paper present framework based use logical argument instantiate bipolar argument graph set possible constraint instantiating argument take account internal structure argument type relationship argument,7
